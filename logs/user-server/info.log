13:08:51.285 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:08:51.314 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:08:51.353 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:08:51.354 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:08:51.354 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:08:51.354 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:08:51.363 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:08:51.368 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:08:51.369 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:08:51.393 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:08:51.393 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:08:51.393 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:08:51.393 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:08:51.394 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:08:51.503 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49506.
13:08:51.518 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:08:51.537 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:08:51.545 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:08:51.545 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:08:51.547 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:08:51.558 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-da02d7e4-9e5e-4952-86a7-6b350ad129a6
13:08:51.579 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:08:51.585 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:08:51.602 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @6021ms to org.sparkproject.jetty.util.log.Slf4jLog
13:08:51.646 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:08:51.653 [main] INFO  org.sparkproject.jetty.server.Server - Started @6072ms
13:08:51.667 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@4d4960c8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:08:51.667 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:08:51.675 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bcbf05b{/,null,AVAILABLE,@Spark}
13:08:51.705 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:08:51.708 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:08:51.718 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49507.
13:08:51.718 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49507
13:08:51.719 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:08:51.721 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49507, None)
13:08:51.723 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49507 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49507, None)
13:08:51.725 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49507, None)
13:08:51.725 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49507, None)
13:08:51.827 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6bcbf05b{/,null,STOPPED,@Spark}
13:08:51.828 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@342726f1{/jobs,null,AVAILABLE,@Spark}
13:08:51.828 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77134e08{/jobs/json,null,AVAILABLE,@Spark}
13:08:51.829 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/jobs/job,null,AVAILABLE,@Spark}
13:08:51.830 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/jobs/job/json,null,AVAILABLE,@Spark}
13:08:51.831 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/stages,null,AVAILABLE,@Spark}
13:08:51.831 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/stages/json,null,AVAILABLE,@Spark}
13:08:51.832 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/stages/stage,null,AVAILABLE,@Spark}
13:08:51.832 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/stages/stage/json,null,AVAILABLE,@Spark}
13:08:51.832 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/stages/pool,null,AVAILABLE,@Spark}
13:08:51.833 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/stages/pool/json,null,AVAILABLE,@Spark}
13:08:51.833 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/storage,null,AVAILABLE,@Spark}
13:08:51.833 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/storage/json,null,AVAILABLE,@Spark}
13:08:51.833 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/storage/rdd,null,AVAILABLE,@Spark}
13:08:51.834 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30d25c03{/storage/rdd/json,null,AVAILABLE,@Spark}
13:08:51.834 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@116a2108{/environment,null,AVAILABLE,@Spark}
13:08:51.834 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c1c5936{/environment/json,null,AVAILABLE,@Spark}
13:08:51.835 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8cf9b{/executors,null,AVAILABLE,@Spark}
13:08:51.835 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b15f922{/executors/json,null,AVAILABLE,@Spark}
13:08:51.835 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b869331{/executors/threadDump,null,AVAILABLE,@Spark}
13:08:51.836 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ce9b9a9{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:08:51.840 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3533df16{/static,null,AVAILABLE,@Spark}
13:08:51.840 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@778db7c5{/,null,AVAILABLE,@Spark}
13:08:51.841 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2875b016{/api,null,AVAILABLE,@Spark}
13:08:51.841 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e8a9c30{/jobs/job/kill,null,AVAILABLE,@Spark}
13:08:51.842 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70211e49{/stages/stage/kill,null,AVAILABLE,@Spark}
13:08:51.844 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e1e2e5e{/metrics/json,null,AVAILABLE,@Spark}
13:08:52.025 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:08:52.026 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@5ebf43b7
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:08:52.027 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@e6e35c0
13:08:52.052 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760504934000
13:08:52.052 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760504934000 ms
13:08:52.053 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:08:52.055 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b16e202{/streaming,null,AVAILABLE,@Spark}
13:08:52.055 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd5122d{/streaming/json,null,AVAILABLE,@Spark}
13:08:52.055 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edef76c{/streaming/batch,null,AVAILABLE,@Spark}
13:08:52.056 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70c53dbe{/streaming/batch/json,null,AVAILABLE,@Spark}
13:08:52.056 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:08:52.056 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a331b46{/static/streaming,null,AVAILABLE,@Spark}
13:08:52.056 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:08:52.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:35) with 1 output partitions
13:08:52.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:35)
13:08:52.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:08:52.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:08:52.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:08:52.114 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:08:52.431 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:08:52.432 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49507 (size: 35.5 KiB, free: 2004.6 MiB)
13:08:52.434 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:08:52.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:08:52.440 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:08:52.459 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:08:52.465 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:08:52.594 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760504932600
13:08:52.595 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:08:52.595 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:08:52.598 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:08:52.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:08:52.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:08:52.600 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:08:52.600 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:08:52.600 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:08:52.603 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:08:52.603 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:08:52.603 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:08:52.604 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:08:52.606 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:08:54.033 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504934000 ms
13:08:54.034 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504934000 ms.0 from job set of time 1760504934000 ms
13:08:54.037 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504934000 ms.0 from job set of time 1760504934000 ms
13:08:54.038 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.037 s for time 1760504934000 ms (execution: 0.003 s)
13:08:54.040 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:08:54.041 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:08:54.612 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:08:54.618 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:08:54.619 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:08:54.619 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:08:54.620 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:08:54.621 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:08:54.620 [receiver-supervisor-future-1] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:08:54.622 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:08:54.623 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:08:54.623 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:08:54.623 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:08:54.625 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:08:56.630 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:08:56.632 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:08:56.633 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:08:56.633 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:08:56.634 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:08:56.634 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:08:56.634 [receiver-supervisor-future-2] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:08:56.636 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:08:56.636 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:08:56.636 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:08:56.636 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:08:56.637 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:08:57.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504937000 ms
13:08:57.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504937000 ms.0 from job set of time 1760504937000 ms
13:08:57.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504937000 ms.0 from job set of time 1760504937000 ms
13:08:57.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760504937000 ms (execution: 0.000 s)
13:08:57.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:08:57.010 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:08:57.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504937000 ms
13:08:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:08:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:08:58.639 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:08:58.641 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:08:58.642 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:08:58.642 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:08:58.643 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:08:58.644 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:08:58.644 [receiver-supervisor-future-3] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:08:58.645 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:08:58.645 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:08:58.645 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:08:58.646 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:08:58.646 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:00.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504940000 ms
13:09:00.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504940000 ms.0 from job set of time 1760504940000 ms
13:09:00.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504940000 ms.0 from job set of time 1760504940000 ms
13:09:00.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760504940000 ms (execution: 0.000 s)
13:09:00.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 2 from persistence list
13:09:00.007 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:09:00.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[2] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504940000 ms
13:09:00.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504934000 ms
13:09:00.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504934000 ms
13:09:00.651 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:00.653 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:00.654 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:00.654 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:00.655 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:00.655 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:00.655 [receiver-supervisor-future-4] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:00.656 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:00.657 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:00.657 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:00.657 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:00.657 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:02.658 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:02.659 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:02.659 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:02.659 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:02.659 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:02.659 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:02.659 [receiver-supervisor-future-5] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:02.660 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:02.660 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:02.660 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:02.660 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:02.660 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:03.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504943000 ms
13:09:03.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504943000 ms.0 from job set of time 1760504943000 ms
13:09:03.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504943000 ms.0 from job set of time 1760504943000 ms
13:09:03.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760504943000 ms (execution: 0.001 s)
13:09:03.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 3 from persistence list
13:09:03.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[3] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504943000 ms
13:09:03.005 [block-manager-storage-async-thread-pool-6] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:09:03.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504937000 ms
13:09:03.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504937000 ms
13:09:04.665 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:04.666 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:04.666 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:04.666 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:04.667 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:04.667 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:04.667 [receiver-supervisor-future-6] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:04.668 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:04.668 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:04.668 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:04.669 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:04.669 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:06.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504946000 ms
13:09:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504946000 ms.0 from job set of time 1760504946000 ms
13:09:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504946000 ms.0 from job set of time 1760504946000 ms
13:09:06.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504946000 ms (execution: 0.000 s)
13:09:06.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 4 from persistence list
13:09:06.009 [block-manager-storage-async-thread-pool-9] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:09:06.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[4] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504946000 ms
13:09:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504940000 ms
13:09:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504940000 ms
13:09:06.672 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:06.673 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:06.673 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:06.673 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:06.674 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:06.674 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:06.674 [receiver-supervisor-future-7] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:06.675 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:06.675 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:06.675 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:06.675 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:06.675 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:08.680 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:08.682 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:08.682 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:08.683 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:08.684 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:08.684 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:08.684 [receiver-supervisor-future-8] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:08.685 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:08.685 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:08.686 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:08.686 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:08.686 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:09.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504949000 ms
13:09:09.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504949000 ms.0 from job set of time 1760504949000 ms
13:09:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504949000 ms.0 from job set of time 1760504949000 ms
13:09:09.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504949000 ms (execution: 0.002 s)
13:09:09.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 5 from persistence list
13:09:09.011 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 5
13:09:09.012 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[5] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504949000 ms
13:09:09.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504943000 ms
13:09:09.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504943000 ms
13:09:10.688 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:10.689 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:10.690 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:10.690 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:10.691 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:10.691 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:10.691 [receiver-supervisor-future-9] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:10.692 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:10.692 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:10.692 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:10.692 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:10.692 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:12.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504952000 ms
13:09:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504952000 ms.0 from job set of time 1760504952000 ms
13:09:12.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504952000 ms.0 from job set of time 1760504952000 ms
13:09:12.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504952000 ms (execution: 0.001 s)
13:09:12.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 6 from persistence list
13:09:12.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[6] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504952000 ms
13:09:12.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504946000 ms
13:09:12.010 [block-manager-storage-async-thread-pool-15] INFO  org.apache.spark.storage.BlockManager - Removing RDD 6
13:09:12.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504946000 ms
13:09:12.698 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:12.699 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:12.699 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:12.699 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:12.700 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:12.700 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:12.700 [receiver-supervisor-future-10] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:12.702 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:12.702 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:12.702 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:12.702 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:12.703 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:14.704 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:14.706 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:14.706 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:14.706 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:14.707 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:14.707 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:14.707 [receiver-supervisor-future-11] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:14.709 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:14.709 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:14.709 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:14.709 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:14.710 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:15.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504955000 ms
13:09:15.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504955000 ms.0 from job set of time 1760504955000 ms
13:09:15.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504955000 ms.0 from job set of time 1760504955000 ms
13:09:15.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504955000 ms (execution: 0.001 s)
13:09:15.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 7 from persistence list
13:09:15.008 [block-manager-storage-async-thread-pool-18] INFO  org.apache.spark.storage.BlockManager - Removing RDD 7
13:09:15.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[7] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504955000 ms
13:09:15.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504949000 ms
13:09:15.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504949000 ms
13:09:16.712 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:16.714 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:16.714 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:16.714 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:16.716 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:16.716 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:16.716 [receiver-supervisor-future-12] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:16.717 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:16.717 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:16.717 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:16.717 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:16.718 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:18.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504958000 ms
13:09:18.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504958000 ms.0 from job set of time 1760504958000 ms
13:09:18.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504958000 ms.0 from job set of time 1760504958000 ms
13:09:18.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504958000 ms (execution: 0.001 s)
13:09:18.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:09:18.009 [block-manager-storage-async-thread-pool-21] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:09:18.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504958000 ms
13:09:18.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504952000 ms
13:09:18.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504952000 ms
13:09:18.723 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:18.724 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:18.724 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:18.724 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:18.725 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:18.725 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:18.725 [receiver-supervisor-future-13] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:18.726 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:18.726 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:18.726 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:18.726 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:18.727 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:20.728 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:20.729 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:20.729 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:20.729 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:20.730 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:20.730 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:20.730 [receiver-supervisor-future-14] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:20.731 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:20.731 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:20.731 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:20.731 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:20.732 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:21.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504961000 ms
13:09:21.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504961000 ms.0 from job set of time 1760504961000 ms
13:09:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504961000 ms.0 from job set of time 1760504961000 ms
13:09:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760504961000 ms (execution: 0.001 s)
13:09:21.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 9 from persistence list
13:09:21.009 [block-manager-storage-async-thread-pool-24] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:09:21.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[9] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504961000 ms
13:09:21.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504955000 ms
13:09:21.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504955000 ms
13:09:22.737 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:22.738 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:22.738 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:22.738 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:22.739 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:22.740 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:22.740 [receiver-supervisor-future-15] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:22.741 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:22.741 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:22.741 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:22.741 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:22.742 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:24.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504964000 ms
13:09:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504964000 ms.0 from job set of time 1760504964000 ms
13:09:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504964000 ms.0 from job set of time 1760504964000 ms
13:09:24.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760504964000 ms (execution: 0.001 s)
13:09:24.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 10 from persistence list
13:09:24.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[10] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504964000 ms
13:09:24.006 [block-manager-storage-async-thread-pool-27] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:09:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504958000 ms
13:09:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504958000 ms
13:09:24.746 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:24.748 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:24.748 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:24.748 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:24.749 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:24.750 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:24.750 [receiver-supervisor-future-16] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:24.751 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:24.751 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:24.751 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:24.751 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:24.751 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:26.752 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:26.753 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:26.754 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:26.754 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:26.755 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:26.755 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:26.755 [receiver-supervisor-future-17] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:26.756 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:26.756 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:26.756 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:26.757 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:26.757 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:27.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504967000 ms
13:09:27.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504967000 ms.0 from job set of time 1760504967000 ms
13:09:27.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504967000 ms.0 from job set of time 1760504967000 ms
13:09:27.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760504967000 ms (execution: 0.000 s)
13:09:27.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 11 from persistence list
13:09:27.005 [block-manager-storage-async-thread-pool-30] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:09:27.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[11] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504967000 ms
13:09:27.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504961000 ms
13:09:27.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504961000 ms
13:09:28.760 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:28.761 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:28.762 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:28.763 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:28.764 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:28.764 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:28.764 [receiver-supervisor-future-18] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:28.765 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:28.765 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:28.765 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:28.765 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:28.766 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:30.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504970000 ms
13:09:30.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504970000 ms.0 from job set of time 1760504970000 ms
13:09:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504970000 ms.0 from job set of time 1760504970000 ms
13:09:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760504970000 ms (execution: 0.001 s)
13:09:30.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 12 from persistence list
13:09:30.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[12] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504970000 ms
13:09:30.009 [block-manager-storage-async-thread-pool-33] INFO  org.apache.spark.storage.BlockManager - Removing RDD 12
13:09:30.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504964000 ms
13:09:30.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504964000 ms
13:09:30.771 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:30.772 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:30.772 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:30.772 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:30.773 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:30.773 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:30.773 [receiver-supervisor-future-19] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:30.775 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:30.775 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:30.775 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:30.775 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:30.775 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:32.776 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:32.777 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:32.777 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:32.777 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:32.778 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:32.778 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:32.778 [receiver-supervisor-future-20] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:32.779 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:32.779 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:32.779 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:32.780 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:32.780 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:33.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504973000 ms
13:09:33.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504973000 ms.0 from job set of time 1760504973000 ms
13:09:33.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504973000 ms.0 from job set of time 1760504973000 ms
13:09:33.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760504973000 ms (execution: 0.000 s)
13:09:33.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 13 from persistence list
13:09:33.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[13] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504973000 ms
13:09:33.006 [block-manager-storage-async-thread-pool-36] INFO  org.apache.spark.storage.BlockManager - Removing RDD 13
13:09:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504967000 ms
13:09:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504967000 ms
13:09:34.783 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:34.785 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:34.785 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:34.785 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:34.786 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:34.786 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:34.786 [receiver-supervisor-future-21] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:34.788 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:34.788 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:34.788 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:34.788 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:34.789 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:36.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504976000 ms
13:09:36.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504976000 ms.0 from job set of time 1760504976000 ms
13:09:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504976000 ms.0 from job set of time 1760504976000 ms
13:09:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760504976000 ms (execution: 0.000 s)
13:09:36.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 14 from persistence list
13:09:36.007 [block-manager-storage-async-thread-pool-39] INFO  org.apache.spark.storage.BlockManager - Removing RDD 14
13:09:36.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[14] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504976000 ms
13:09:36.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504970000 ms
13:09:36.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504970000 ms
13:09:36.792 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:36.793 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:36.793 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:36.793 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:36.794 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:36.794 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:36.794 [receiver-supervisor-future-22] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:36.796 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:36.796 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:36.796 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:36.796 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:36.797 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:38.799 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:38.800 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:38.800 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:38.800 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:38.801 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:38.801 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:38.801 [receiver-supervisor-future-23] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:38.801 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:38.801 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:38.801 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:38.802 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:38.802 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:39.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504979000 ms
13:09:39.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504979000 ms.0 from job set of time 1760504979000 ms
13:09:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504979000 ms.0 from job set of time 1760504979000 ms
13:09:39.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760504979000 ms (execution: 0.001 s)
13:09:39.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:09:39.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504979000 ms
13:09:39.010 [block-manager-storage-async-thread-pool-42] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:09:39.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504973000 ms
13:09:39.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504973000 ms
13:09:40.806 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:40.806 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:40.806 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:40.806 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:40.807 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:40.807 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:40.807 [receiver-supervisor-future-24] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:40.808 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:40.808 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:40.808 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:40.808 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:40.808 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:42.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504982000 ms
13:09:42.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504982000 ms.0 from job set of time 1760504982000 ms
13:09:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504982000 ms.0 from job set of time 1760504982000 ms
13:09:42.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760504982000 ms (execution: 0.001 s)
13:09:42.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 16 from persistence list
13:09:42.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[16] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504982000 ms
13:09:42.009 [block-manager-storage-async-thread-pool-45] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:09:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504976000 ms
13:09:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504976000 ms
13:09:42.813 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:42.813 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:42.813 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:42.814 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:42.814 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:42.814 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:42.814 [receiver-supervisor-future-25] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:42.814 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:42.814 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:42.814 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:42.815 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:42.815 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:44.820 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:44.821 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:44.821 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:44.821 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:44.822 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:44.822 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:44.822 [receiver-supervisor-future-26] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:44.823 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:44.823 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:44.823 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:44.824 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:44.824 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:45.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504985000 ms
13:09:45.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504985000 ms.0 from job set of time 1760504985000 ms
13:09:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504985000 ms.0 from job set of time 1760504985000 ms
13:09:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760504985000 ms (execution: 0.001 s)
13:09:45.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 17 from persistence list
13:09:45.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[17] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504985000 ms
13:09:45.010 [block-manager-storage-async-thread-pool-48] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:09:45.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504979000 ms
13:09:45.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504979000 ms
13:09:46.829 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:46.830 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:46.831 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:46.831 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:46.833 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:46.833 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:46.833 [receiver-supervisor-future-27] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:46.835 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:46.835 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:46.835 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:46.835 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:46.836 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:48.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504988000 ms
13:09:48.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504988000 ms.0 from job set of time 1760504988000 ms
13:09:48.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504988000 ms.0 from job set of time 1760504988000 ms
13:09:48.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760504988000 ms (execution: 0.001 s)
13:09:48.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 18 from persistence list
13:09:48.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[18] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504988000 ms
13:09:48.006 [block-manager-storage-async-thread-pool-51] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:09:48.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504982000 ms
13:09:48.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504982000 ms
13:09:48.840 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:48.841 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:48.842 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:48.842 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:48.843 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:48.843 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:48.843 [receiver-supervisor-future-28] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:48.845 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:48.845 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:48.845 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:48.845 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:48.846 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:50.849 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:50.850 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:50.851 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:50.851 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:50.852 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:50.852 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:50.852 [receiver-supervisor-future-29] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:50.853 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:50.853 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:50.853 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:50.854 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:50.854 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:51.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504991000 ms
13:09:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504991000 ms.0 from job set of time 1760504991000 ms
13:09:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504991000 ms.0 from job set of time 1760504991000 ms
13:09:51.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760504991000 ms (execution: 0.001 s)
13:09:51.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 19 from persistence list
13:09:51.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[19] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504991000 ms
13:09:51.010 [block-manager-storage-async-thread-pool-54] INFO  org.apache.spark.storage.BlockManager - Removing RDD 19
13:09:51.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504985000 ms
13:09:51.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504985000 ms
13:09:52.855 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:52.856 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:52.857 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:52.857 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:52.858 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:52.858 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:52.858 [receiver-supervisor-future-30] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:52.859 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:52.859 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:52.859 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:52.860 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:52.861 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:54.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504994000 ms
13:09:54.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504994000 ms.0 from job set of time 1760504994000 ms
13:09:54.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504994000 ms.0 from job set of time 1760504994000 ms
13:09:54.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760504994000 ms (execution: 0.000 s)
13:09:54.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 20 from persistence list
13:09:54.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[20] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504994000 ms
13:09:54.006 [block-manager-storage-async-thread-pool-57] INFO  org.apache.spark.storage.BlockManager - Removing RDD 20
13:09:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504988000 ms
13:09:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504988000 ms
13:09:54.861 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:54.862 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:54.862 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:54.862 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:54.863 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:54.863 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:54.863 [receiver-supervisor-future-31] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:54.865 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:54.865 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:54.865 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:54.866 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:54.866 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:56.870 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:56.871 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:56.871 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:56.871 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:56.872 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:56.872 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:56.872 [receiver-supervisor-future-32] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:56.872 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:56.872 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:56.872 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:56.873 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:56.874 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:09:57.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760504997000 ms
13:09:57.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760504997000 ms.0 from job set of time 1760504997000 ms
13:09:57.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760504997000 ms.0 from job set of time 1760504997000 ms
13:09:57.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760504997000 ms (execution: 0.001 s)
13:09:57.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 21 from persistence list
13:09:57.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[21] at socketTextStream at sparkStreamingSocket.java:31 of time 1760504997000 ms
13:09:57.007 [block-manager-storage-async-thread-pool-60] INFO  org.apache.spark.storage.BlockManager - Removing RDD 21
13:09:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504991000 ms
13:09:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504991000 ms
13:09:58.875 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:09:58.876 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:09:58.876 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:09:58.876 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:09:58.876 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:09:58.876 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:09:58.876 [receiver-supervisor-future-33] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:09:58.877 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:09:58.877 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:09:58.877 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:09:58.877 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:09:58.878 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:00.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505000000 ms
13:10:00.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505000000 ms.0 from job set of time 1760505000000 ms
13:10:00.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505000000 ms.0 from job set of time 1760505000000 ms
13:10:00.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505000000 ms (execution: 0.001 s)
13:10:00.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 22 from persistence list
13:10:00.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[22] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505000000 ms
13:10:00.009 [block-manager-storage-async-thread-pool-63] INFO  org.apache.spark.storage.BlockManager - Removing RDD 22
13:10:00.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504994000 ms
13:10:00.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504994000 ms
13:10:00.881 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:00.883 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:00.883 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:00.883 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:00.884 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:00.884 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:00.884 [receiver-supervisor-future-34] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:00.885 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:00.885 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:00.885 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:00.886 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:00.886 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:02.888 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:02.889 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:02.889 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:02.889 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:02.890 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:02.890 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:02.891 [receiver-supervisor-future-35] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:02.892 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:02.892 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:02.892 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:02.893 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:02.893 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:03.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505003000 ms
13:10:03.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505003000 ms.0 from job set of time 1760505003000 ms
13:10:03.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505003000 ms.0 from job set of time 1760505003000 ms
13:10:03.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505003000 ms (execution: 0.001 s)
13:10:03.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 23 from persistence list
13:10:03.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[23] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505003000 ms
13:10:03.006 [block-manager-storage-async-thread-pool-66] INFO  org.apache.spark.storage.BlockManager - Removing RDD 23
13:10:03.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760504997000 ms
13:10:03.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760504997000 ms
13:10:04.898 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:04.899 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:04.899 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:04.899 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:04.900 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:04.900 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:04.900 [receiver-supervisor-future-36] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:04.902 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:04.902 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:04.902 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:04.902 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:04.902 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:06.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505006000 ms
13:10:06.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505006000 ms.0 from job set of time 1760505006000 ms
13:10:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505006000 ms.0 from job set of time 1760505006000 ms
13:10:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505006000 ms (execution: 0.001 s)
13:10:06.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 24 from persistence list
13:10:06.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[24] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505006000 ms
13:10:06.008 [block-manager-storage-async-thread-pool-69] INFO  org.apache.spark.storage.BlockManager - Removing RDD 24
13:10:06.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505000000 ms
13:10:06.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505000000 ms
13:10:06.907 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:06.909 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:06.910 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:06.910 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:06.911 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:06.911 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:06.911 [receiver-supervisor-future-37] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:06.913 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:06.913 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:06.913 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:06.913 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:06.914 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:08.918 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:08.919 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:08.920 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:08.920 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:08.921 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:08.921 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:08.921 [receiver-supervisor-future-38] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:08.923 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:08.923 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:08.923 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:08.924 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:08.924 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:09.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505009000 ms
13:10:09.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505009000 ms.0 from job set of time 1760505009000 ms
13:10:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505009000 ms.0 from job set of time 1760505009000 ms
13:10:09.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505009000 ms (execution: 0.001 s)
13:10:09.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 25 from persistence list
13:10:09.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[25] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505009000 ms
13:10:09.009 [block-manager-storage-async-thread-pool-72] INFO  org.apache.spark.storage.BlockManager - Removing RDD 25
13:10:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505003000 ms
13:10:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505003000 ms
13:10:10.929 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:10.930 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:10.931 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:10.931 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:10.932 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:10.932 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:10.932 [receiver-supervisor-future-39] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:10.933 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:10.933 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:10.933 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:10.934 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:10.934 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:12.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505012000 ms
13:10:12.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505012000 ms.0 from job set of time 1760505012000 ms
13:10:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505012000 ms.0 from job set of time 1760505012000 ms
13:10:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505012000 ms (execution: 0.001 s)
13:10:12.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 26 from persistence list
13:10:12.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[26] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505012000 ms
13:10:12.007 [block-manager-storage-async-thread-pool-75] INFO  org.apache.spark.storage.BlockManager - Removing RDD 26
13:10:12.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505006000 ms
13:10:12.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505006000 ms
13:10:12.939 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:12.940 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:12.940 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:12.940 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:12.941 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:12.942 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:12.955 [receiver-supervisor-future-40] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:12.957 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:12.957 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:12.957 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:12.958 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:12.958 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:14.959 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:14.960 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:14.961 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:14.961 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:14.962 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:14.962 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:14.962 [receiver-supervisor-future-41] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:14.964 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:14.964 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:14.964 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:14.964 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:14.965 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:15.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505015000 ms
13:10:15.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505015000 ms.0 from job set of time 1760505015000 ms
13:10:15.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505015000 ms.0 from job set of time 1760505015000 ms
13:10:15.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505015000 ms (execution: 0.001 s)
13:10:15.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 27 from persistence list
13:10:15.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[27] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505015000 ms
13:10:15.009 [block-manager-storage-async-thread-pool-78] INFO  org.apache.spark.storage.BlockManager - Removing RDD 27
13:10:15.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505009000 ms
13:10:15.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505009000 ms
13:10:16.970 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:16.972 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:16.972 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:16.972 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:16.973 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:16.974 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:16.974 [receiver-supervisor-future-42] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:16.975 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:16.976 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:16.976 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:16.976 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:16.976 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:18.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505018000 ms
13:10:18.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505018000 ms.0 from job set of time 1760505018000 ms
13:10:18.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505018000 ms.0 from job set of time 1760505018000 ms
13:10:18.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505018000 ms (execution: 0.001 s)
13:10:18.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 28 from persistence list
13:10:18.009 [block-manager-storage-async-thread-pool-81] INFO  org.apache.spark.storage.BlockManager - Removing RDD 28
13:10:18.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[28] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505018000 ms
13:10:18.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505012000 ms
13:10:18.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505012000 ms
13:10:18.980 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:18.982 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:18.982 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:18.982 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:18.983 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:18.983 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:18.983 [receiver-supervisor-future-43] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:18.985 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:18.985 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:18.985 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:18.985 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:18.985 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:20.988 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:20.989 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:20.989 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:20.989 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:20.990 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:20.990 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:20.990 [receiver-supervisor-future-44] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:20.992 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:20.992 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:20.992 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:20.992 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:20.992 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:21.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505021000 ms
13:10:21.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505021000 ms.0 from job set of time 1760505021000 ms
13:10:21.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505021000 ms.0 from job set of time 1760505021000 ms
13:10:21.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505021000 ms (execution: 0.001 s)
13:10:21.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 29 from persistence list
13:10:21.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[29] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505021000 ms
13:10:21.007 [block-manager-storage-async-thread-pool-84] INFO  org.apache.spark.storage.BlockManager - Removing RDD 29
13:10:21.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505015000 ms
13:10:21.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505015000 ms
13:10:22.996 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:22.997 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:22.997 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:22.998 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:22.999 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:22.999 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:22.999 [receiver-supervisor-future-45] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:23.001 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:23.001 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:23.001 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:23.001 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:23.002 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:24.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505024000 ms
13:10:24.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505024000 ms.0 from job set of time 1760505024000 ms
13:10:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505024000 ms.0 from job set of time 1760505024000 ms
13:10:24.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505024000 ms (execution: 0.001 s)
13:10:24.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 30 from persistence list
13:10:24.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[30] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505024000 ms
13:10:24.006 [block-manager-storage-async-thread-pool-87] INFO  org.apache.spark.storage.BlockManager - Removing RDD 30
13:10:24.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505018000 ms
13:10:24.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505018000 ms
13:10:25.006 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:25.007 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:25.007 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:25.007 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:25.009 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:25.009 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:25.009 [receiver-supervisor-future-46] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:25.011 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:25.011 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:25.011 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:25.011 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:25.011 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505027000 ms
13:10:27.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505027000 ms.0 from job set of time 1760505027000 ms
13:10:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505027000 ms.0 from job set of time 1760505027000 ms
13:10:27.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505027000 ms (execution: 0.002 s)
13:10:27.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 31 from persistence list
13:10:27.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[31] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505027000 ms
13:10:27.011 [block-manager-storage-async-thread-pool-90] INFO  org.apache.spark.storage.BlockManager - Removing RDD 31
13:10:27.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505021000 ms
13:10:27.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505021000 ms
13:10:27.016 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:27.019 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:27.020 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:27.020 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:27.029 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:27.030 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:27.030 [receiver-supervisor-future-47] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:27.037 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:27.037 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:27.037 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:27.037 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:27.038 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:29.042 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:29.043 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:29.043 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:29.043 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:29.044 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:29.045 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:29.045 [receiver-supervisor-future-48] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:29.046 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:29.046 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:29.046 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:29.047 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:29.047 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:30.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505030000 ms
13:10:30.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505030000 ms.0 from job set of time 1760505030000 ms
13:10:30.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505030000 ms.0 from job set of time 1760505030000 ms
13:10:30.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505030000 ms (execution: 0.001 s)
13:10:30.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 32 from persistence list
13:10:30.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[32] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505030000 ms
13:10:30.007 [block-manager-storage-async-thread-pool-93] INFO  org.apache.spark.storage.BlockManager - Removing RDD 32
13:10:30.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505024000 ms
13:10:30.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505024000 ms
13:10:31.051 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:31.052 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:31.052 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:31.052 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:31.053 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:31.053 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:31.053 [receiver-supervisor-future-49] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:31.054 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:31.054 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:31.054 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:31.054 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:31.054 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:33.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505033000 ms
13:10:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505033000 ms.0 from job set of time 1760505033000 ms
13:10:33.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505033000 ms.0 from job set of time 1760505033000 ms
13:10:33.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505033000 ms (execution: 0.001 s)
13:10:33.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 33 from persistence list
13:10:33.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[33] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505033000 ms
13:10:33.010 [block-manager-storage-async-thread-pool-96] INFO  org.apache.spark.storage.BlockManager - Removing RDD 33
13:10:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505027000 ms
13:10:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505027000 ms
13:10:33.059 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:33.060 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:33.061 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:33.061 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:33.062 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:33.062 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:33.062 [receiver-supervisor-future-50] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:33.064 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:33.064 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:33.064 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:33.064 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:33.065 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:35.069 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:35.071 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:35.071 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:35.071 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:35.072 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:35.072 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:35.072 [receiver-supervisor-future-51] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:35.075 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:35.075 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:35.075 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:35.075 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:35.076 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:36.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505036000 ms
13:10:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505036000 ms.0 from job set of time 1760505036000 ms
13:10:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505036000 ms.0 from job set of time 1760505036000 ms
13:10:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505036000 ms (execution: 0.001 s)
13:10:36.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 34 from persistence list
13:10:36.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[34] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505036000 ms
13:10:36.010 [block-manager-storage-async-thread-pool-99] INFO  org.apache.spark.storage.BlockManager - Removing RDD 34
13:10:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505030000 ms
13:10:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505030000 ms
13:10:37.081 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:37.083 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:37.083 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:37.083 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:37.084 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:37.084 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:37.084 [receiver-supervisor-future-52] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:37.086 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:37.086 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:37.086 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:37.086 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:37.087 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:39.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505039000 ms
13:10:39.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505039000 ms.0 from job set of time 1760505039000 ms
13:10:39.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505039000 ms.0 from job set of time 1760505039000 ms
13:10:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505039000 ms (execution: 0.001 s)
13:10:39.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 35 from persistence list
13:10:39.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[35] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505039000 ms
13:10:39.009 [block-manager-storage-async-thread-pool-102] INFO  org.apache.spark.storage.BlockManager - Removing RDD 35
13:10:39.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505033000 ms
13:10:39.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505033000 ms
13:10:39.092 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:39.093 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:39.093 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:39.093 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:39.094 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:39.094 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:39.094 [receiver-supervisor-future-53] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:39.094 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:39.094 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:39.094 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:39.095 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:39.095 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:41.100 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:41.101 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:41.101 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:41.101 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:41.102 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:41.102 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:41.102 [receiver-supervisor-future-54] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:41.104 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:41.104 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:41.104 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:41.104 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:41.104 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:42.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505042000 ms
13:10:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505042000 ms.0 from job set of time 1760505042000 ms
13:10:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505042000 ms.0 from job set of time 1760505042000 ms
13:10:42.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505042000 ms (execution: 0.000 s)
13:10:42.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 36 from persistence list
13:10:42.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[36] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505042000 ms
13:10:42.009 [block-manager-storage-async-thread-pool-105] INFO  org.apache.spark.storage.BlockManager - Removing RDD 36
13:10:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505036000 ms
13:10:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505036000 ms
13:10:43.107 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:43.107 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:43.107 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:43.108 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:43.108 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:43.108 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:43.109 [receiver-supervisor-future-55] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:43.110 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:43.110 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:43.110 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:43.110 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:43.111 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:45.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505045000 ms
13:10:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505045000 ms.0 from job set of time 1760505045000 ms
13:10:45.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505045000 ms.0 from job set of time 1760505045000 ms
13:10:45.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505045000 ms (execution: 0.001 s)
13:10:45.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 37 from persistence list
13:10:45.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[37] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505045000 ms
13:10:45.008 [block-manager-storage-async-thread-pool-108] INFO  org.apache.spark.storage.BlockManager - Removing RDD 37
13:10:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505039000 ms
13:10:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505039000 ms
13:10:45.114 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:45.116 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:45.116 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:45.116 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:45.117 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:45.117 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:45.117 [receiver-supervisor-future-56] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:45.120 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:45.120 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:45.120 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:45.120 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:45.120 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:47.125 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:47.126 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:47.126 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:47.126 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:47.127 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:47.128 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:47.128 [receiver-supervisor-future-57] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:47.130 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:47.130 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:47.130 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:47.130 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:47.130 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:48.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505048000 ms
13:10:48.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505048000 ms.0 from job set of time 1760505048000 ms
13:10:48.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505048000 ms.0 from job set of time 1760505048000 ms
13:10:48.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505048000 ms (execution: 0.001 s)
13:10:48.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 38 from persistence list
13:10:48.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[38] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505048000 ms
13:10:48.006 [block-manager-storage-async-thread-pool-111] INFO  org.apache.spark.storage.BlockManager - Removing RDD 38
13:10:48.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505042000 ms
13:10:48.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505042000 ms
13:10:49.133 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:49.134 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:49.134 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:49.134 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:49.135 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:49.135 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:49.135 [receiver-supervisor-future-58] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:49.136 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:49.137 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:49.137 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:49.137 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:49.137 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:51.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505051000 ms
13:10:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505051000 ms.0 from job set of time 1760505051000 ms
13:10:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505051000 ms.0 from job set of time 1760505051000 ms
13:10:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505051000 ms (execution: 0.001 s)
13:10:51.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 39 from persistence list
13:10:51.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[39] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505051000 ms
13:10:51.009 [block-manager-storage-async-thread-pool-114] INFO  org.apache.spark.storage.BlockManager - Removing RDD 39
13:10:51.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505045000 ms
13:10:51.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505045000 ms
13:10:51.142 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:51.143 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:51.143 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:51.143 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:51.145 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:51.145 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:51.145 [receiver-supervisor-future-59] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:51.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:51.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:51.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:51.147 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:51.147 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:53.149 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:53.150 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:53.150 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:53.150 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:53.151 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:53.151 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:53.151 [receiver-supervisor-future-60] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:53.153 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:53.153 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:53.153 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:53.153 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:53.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:54.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505054000 ms
13:10:54.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505054000 ms.0 from job set of time 1760505054000 ms
13:10:54.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505054000 ms.0 from job set of time 1760505054000 ms
13:10:54.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505054000 ms (execution: 0.001 s)
13:10:54.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 40 from persistence list
13:10:54.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[40] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505054000 ms
13:10:54.009 [block-manager-storage-async-thread-pool-117] INFO  org.apache.spark.storage.BlockManager - Removing RDD 40
13:10:54.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505048000 ms
13:10:54.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505048000 ms
13:10:55.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:55.155 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:55.156 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:55.156 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:55.157 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:55.157 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:55.157 [receiver-supervisor-future-61] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:55.158 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:55.158 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:55.158 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:55.158 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:55.159 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505057000 ms
13:10:57.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505057000 ms.0 from job set of time 1760505057000 ms
13:10:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505057000 ms.0 from job set of time 1760505057000 ms
13:10:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505057000 ms (execution: 0.001 s)
13:10:57.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 41 from persistence list
13:10:57.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[41] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505057000 ms
13:10:57.009 [block-manager-storage-async-thread-pool-120] INFO  org.apache.spark.storage.BlockManager - Removing RDD 41
13:10:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505051000 ms
13:10:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505051000 ms
13:10:57.159 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:57.160 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:57.160 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:57.160 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:57.161 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:57.162 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:57.162 [receiver-supervisor-future-62] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:57.163 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:57.163 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:57.163 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:57.163 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:57.164 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:10:59.166 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:10:59.167 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:10:59.167 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:10:59.167 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:10:59.168 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:10:59.168 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:10:59.168 [receiver-supervisor-future-63] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:10:59.170 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:10:59.170 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:10:59.170 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:10:59.170 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:10:59.170 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:11:00.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505060000 ms
13:11:00.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505060000 ms.0 from job set of time 1760505060000 ms
13:11:00.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505060000 ms.0 from job set of time 1760505060000 ms
13:11:00.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505060000 ms (execution: 0.001 s)
13:11:00.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 42 from persistence list
13:11:00.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[42] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505060000 ms
13:11:00.006 [block-manager-storage-async-thread-pool-123] INFO  org.apache.spark.storage.BlockManager - Removing RDD 42
13:11:00.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505054000 ms
13:11:00.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505054000 ms
13:11:01.173 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:11:01.174 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49506
13:11:01.174 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:11:01.174 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:11:01.175 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:11:01.175 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:11:01.176 [receiver-supervisor-future-64] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:11:01.177 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:11:01.177 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:11:01.177 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:11:01.177 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:11:01.177 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:11:01.251 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:11:01.253 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:11:01.253 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:11:01.253 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:11:01.254 [dispatcher-event-loop-3] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:11:01.254 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:17:22.373 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:17:22.401 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:17:22.439 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:17:22.439 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:17:22.439 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:17:22.439 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:17:22.448 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:17:22.453 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:17:22.454 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:17:22.479 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:17:22.479 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:17:22.479 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:17:22.479 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:17:22.479 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:17:22.596 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49655.
13:17:22.609 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:17:22.625 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:17:22.633 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:17:22.633 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:17:22.635 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:17:22.647 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-9d790ae2-94f4-4039-a69e-12fd6fc060ea
13:17:22.667 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:17:22.673 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:17:22.691 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @6226ms to org.sparkproject.jetty.util.log.Slf4jLog
13:17:22.736 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:17:22.743 [main] INFO  org.sparkproject.jetty.server.Server - Started @6278ms
13:17:22.758 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@2c779e5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:17:22.758 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:17:22.765 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cac930{/,null,AVAILABLE,@Spark}
13:17:22.798 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:17:22.801 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:17:22.812 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49656.
13:17:22.812 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49656
13:17:22.813 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:17:22.815 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49656, None)
13:17:22.817 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49656 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49656, None)
13:17:22.818 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49656, None)
13:17:22.818 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49656, None)
13:17:22.914 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@69cac930{/,null,STOPPED,@Spark}
13:17:22.915 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62566842{/jobs,null,AVAILABLE,@Spark}
13:17:22.916 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37191ef0{/jobs/json,null,AVAILABLE,@Spark}
13:17:22.916 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4dbad37{/jobs/job,null,AVAILABLE,@Spark}
13:17:22.917 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26a262d6{/jobs/job/json,null,AVAILABLE,@Spark}
13:17:22.917 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f07f02{/stages,null,AVAILABLE,@Spark}
13:17:22.918 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40f8f5a8{/stages/json,null,AVAILABLE,@Spark}
13:17:22.918 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a55f148{/stages/stage,null,AVAILABLE,@Spark}
13:17:22.919 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae2ed38{/stages/stage/json,null,AVAILABLE,@Spark}
13:17:22.919 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/stages/pool,null,AVAILABLE,@Spark}
13:17:22.919 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12b5454f{/stages/pool/json,null,AVAILABLE,@Spark}
13:17:22.920 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1431267b{/storage,null,AVAILABLE,@Spark}
13:17:22.920 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/storage/json,null,AVAILABLE,@Spark}
13:17:22.920 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/storage/rdd,null,AVAILABLE,@Spark}
13:17:22.921 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/storage/rdd/json,null,AVAILABLE,@Spark}
13:17:22.921 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/environment,null,AVAILABLE,@Spark}
13:17:22.921 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/environment/json,null,AVAILABLE,@Spark}
13:17:22.921 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/executors,null,AVAILABLE,@Spark}
13:17:22.922 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/executors/json,null,AVAILABLE,@Spark}
13:17:22.922 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/executors/threadDump,null,AVAILABLE,@Spark}
13:17:22.923 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:17:22.927 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/static,null,AVAILABLE,@Spark}
13:17:22.927 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@563a89b5{/,null,AVAILABLE,@Spark}
13:17:22.928 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@270b6b5e{/api,null,AVAILABLE,@Spark}
13:17:22.929 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a93b263{/jobs/job/kill,null,AVAILABLE,@Spark}
13:17:22.929 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bd1111a{/stages/stage/kill,null,AVAILABLE,@Spark}
13:17:22.931 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beeb0e{/metrics/json,null,AVAILABLE,@Spark}
13:17:23.106 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:17:23.107 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@775acd88
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:17:23.108 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:17:23.109 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@49d46119
13:17:23.132 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760505444000
13:17:23.132 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760505444000 ms
13:17:23.133 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:17:23.134 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba5aa7a{/streaming,null,AVAILABLE,@Spark}
13:17:23.135 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e5f96e{/streaming/json,null,AVAILABLE,@Spark}
13:17:23.135 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46690{/streaming/batch,null,AVAILABLE,@Spark}
13:17:23.135 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d50b{/streaming/batch/json,null,AVAILABLE,@Spark}
13:17:23.136 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd4a4d7{/static/streaming,null,AVAILABLE,@Spark}
13:17:23.136 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:17:23.155 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:17:23.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:35) with 1 output partitions
13:17:23.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:35)
13:17:23.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:17:23.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:17:23.159 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:17:23.214 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:17:23.522 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:17:23.523 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49656 (size: 35.5 KiB, free: 2004.6 MiB)
13:17:23.524 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:17:23.530 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:17:23.531 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:17:23.553 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:17:23.560 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:17:23.697 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760505443800
13:17:23.698 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:17:23.698 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:17:23.700 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:23.701 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:23.701 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:23.703 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:23.703 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:17:23.703 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:23.706 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:23.706 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:23.706 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:23.707 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:23.708 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:24.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505444000 ms
13:17:24.014 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505444000 ms.0 from job set of time 1760505444000 ms
13:17:24.016 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505444000 ms.0 from job set of time 1760505444000 ms
13:17:24.016 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.015 s for time 1760505444000 ms (execution: 0.001 s)
13:17:24.017 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:17:24.018 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:17:25.714 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:25.718 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:25.718 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:25.719 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:25.720 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:25.720 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:25.720 [receiver-supervisor-future-1] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:25.721 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:25.722 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:25.722 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:25.722 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:25.723 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:27.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505447000 ms
13:17:27.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505447000 ms.0 from job set of time 1760505447000 ms
13:17:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505447000 ms.0 from job set of time 1760505447000 ms
13:17:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505447000 ms (execution: 0.001 s)
13:17:27.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:17:27.023 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:17:27.024 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505447000 ms
13:17:27.024 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:17:27.024 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:17:27.725 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:27.726 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:27.727 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:27.727 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:27.728 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:27.729 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:27.729 [receiver-supervisor-future-2] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:27.730 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:27.731 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:27.731 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:27.731 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:27.732 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:29.732 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:29.734 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:29.734 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:29.735 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:29.736 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:29.736 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:29.736 [receiver-supervisor-future-3] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:29.738 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:29.738 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:29.739 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:29.739 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:29.740 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:30.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505450000 ms
13:17:30.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505450000 ms.0 from job set of time 1760505450000 ms
13:17:30.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505450000 ms.0 from job set of time 1760505450000 ms
13:17:30.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505450000 ms (execution: 0.000 s)
13:17:30.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 2 from persistence list
13:17:30.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[2] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505450000 ms
13:17:30.008 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:17:30.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505444000 ms
13:17:30.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505444000 ms
13:17:31.745 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:31.747 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:31.747 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:31.747 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:31.749 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:31.749 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:31.749 [receiver-supervisor-future-4] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:31.750 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:31.751 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:31.751 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:31.751 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:31.752 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:33.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505453000 ms
13:17:33.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505453000 ms.0 from job set of time 1760505453000 ms
13:17:33.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505453000 ms.0 from job set of time 1760505453000 ms
13:17:33.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.011 s for time 1760505453000 ms (execution: 0.002 s)
13:17:33.012 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 3 from persistence list
13:17:33.013 [block-manager-storage-async-thread-pool-6] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:17:33.014 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[3] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505453000 ms
13:17:33.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505447000 ms
13:17:33.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505447000 ms
13:17:33.757 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:33.759 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:33.760 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:33.760 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:33.761 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:33.762 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:33.761 [receiver-supervisor-future-5] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:33.763 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:33.763 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:33.763 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:33.764 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:33.765 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:35.767 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:35.768 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:35.768 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:35.768 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:35.769 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:35.769 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:35.769 [receiver-supervisor-future-6] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:35.771 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:35.771 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:35.771 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:35.771 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:35.771 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:36.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505456000 ms
13:17:36.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505456000 ms.0 from job set of time 1760505456000 ms
13:17:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505456000 ms.0 from job set of time 1760505456000 ms
13:17:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505456000 ms (execution: 0.001 s)
13:17:36.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 4 from persistence list
13:17:36.008 [block-manager-storage-async-thread-pool-9] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:17:36.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[4] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505456000 ms
13:17:36.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505450000 ms
13:17:36.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505450000 ms
13:17:37.776 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:37.778 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:37.778 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:37.778 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:37.780 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:37.780 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:37.780 [receiver-supervisor-future-7] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:37.782 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:37.782 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:37.782 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:37.782 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:37.783 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:39.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505459000 ms
13:17:39.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505459000 ms.0 from job set of time 1760505459000 ms
13:17:39.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505459000 ms.0 from job set of time 1760505459000 ms
13:17:39.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505459000 ms (execution: 0.001 s)
13:17:39.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 5 from persistence list
13:17:39.007 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 5
13:17:39.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[5] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505459000 ms
13:17:39.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505453000 ms
13:17:39.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505453000 ms
13:17:39.788 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:39.790 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:39.791 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:39.791 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:39.792 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:39.792 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:39.792 [receiver-supervisor-future-8] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:39.794 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:39.794 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:39.794 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:39.795 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:39.795 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:41.801 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:41.803 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:41.803 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:41.803 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:41.805 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:41.805 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:41.805 [receiver-supervisor-future-9] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:41.806 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:41.806 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:41.806 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:41.807 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:41.807 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:42.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505462000 ms
13:17:42.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505462000 ms.0 from job set of time 1760505462000 ms
13:17:42.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505462000 ms.0 from job set of time 1760505462000 ms
13:17:42.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505462000 ms (execution: 0.002 s)
13:17:42.011 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 6 from persistence list
13:17:42.012 [block-manager-storage-async-thread-pool-15] INFO  org.apache.spark.storage.BlockManager - Removing RDD 6
13:17:42.012 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[6] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505462000 ms
13:17:42.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505456000 ms
13:17:42.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505456000 ms
13:17:43.812 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:43.814 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:43.814 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:43.815 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:43.816 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:43.816 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:43.816 [receiver-supervisor-future-10] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:43.817 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:43.817 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:43.817 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:43.818 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:43.818 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:45.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505465000 ms
13:17:45.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505465000 ms.0 from job set of time 1760505465000 ms
13:17:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505465000 ms.0 from job set of time 1760505465000 ms
13:17:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505465000 ms (execution: 0.002 s)
13:17:45.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 7 from persistence list
13:17:45.010 [block-manager-storage-async-thread-pool-18] INFO  org.apache.spark.storage.BlockManager - Removing RDD 7
13:17:45.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[7] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505465000 ms
13:17:45.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505459000 ms
13:17:45.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505459000 ms
13:17:45.824 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:45.825 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:45.826 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:45.826 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:45.827 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:45.827 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:45.827 [receiver-supervisor-future-11] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:45.829 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:45.829 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:45.829 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:45.830 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:45.830 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:47.832 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:47.833 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:47.834 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:47.834 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:47.835 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:47.835 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:47.835 [receiver-supervisor-future-12] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:47.836 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:47.836 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:47.837 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:47.837 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:47.837 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:48.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505468000 ms
13:17:48.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505468000 ms.0 from job set of time 1760505468000 ms
13:17:48.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505468000 ms.0 from job set of time 1760505468000 ms
13:17:48.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505468000 ms (execution: 0.001 s)
13:17:48.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:17:48.010 [block-manager-storage-async-thread-pool-21] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:17:48.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505468000 ms
13:17:48.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505462000 ms
13:17:48.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505462000 ms
13:17:49.838 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:49.839 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:49.839 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:49.839 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:49.840 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:49.840 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:49.840 [receiver-supervisor-future-13] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:49.841 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:49.841 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:49.841 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:49.841 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:49.842 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:51.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505471000 ms
13:17:51.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505471000 ms.0 from job set of time 1760505471000 ms
13:17:51.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505471000 ms.0 from job set of time 1760505471000 ms
13:17:51.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505471000 ms (execution: 0.001 s)
13:17:51.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 9 from persistence list
13:17:51.005 [block-manager-storage-async-thread-pool-24] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:17:51.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[9] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505471000 ms
13:17:51.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505465000 ms
13:17:51.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505465000 ms
13:17:51.844 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:51.845 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:51.845 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:51.846 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:51.847 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:51.847 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:51.847 [receiver-supervisor-future-14] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:51.848 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:51.849 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:51.849 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:51.849 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:51.849 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:53.850 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:53.851 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:53.852 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:53.852 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:53.853 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:53.853 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:53.853 [receiver-supervisor-future-15] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:53.855 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:53.855 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:53.855 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:53.856 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:53.856 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:54.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505474000 ms
13:17:54.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505474000 ms.0 from job set of time 1760505474000 ms
13:17:54.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505474000 ms.0 from job set of time 1760505474000 ms
13:17:54.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505474000 ms (execution: 0.001 s)
13:17:54.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 10 from persistence list
13:17:54.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[10] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505474000 ms
13:17:54.008 [block-manager-storage-async-thread-pool-27] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:17:54.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505468000 ms
13:17:54.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505468000 ms
13:17:55.861 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:55.862 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:55.863 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:55.863 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:55.864 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:55.864 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:55.864 [receiver-supervisor-future-16] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:55.865 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:55.865 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:55.865 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:55.866 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:55.866 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:57.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505477000 ms
13:17:57.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505477000 ms.0 from job set of time 1760505477000 ms
13:17:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505477000 ms.0 from job set of time 1760505477000 ms
13:17:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505477000 ms (execution: 0.002 s)
13:17:57.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 11 from persistence list
13:17:57.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[11] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505477000 ms
13:17:57.009 [block-manager-storage-async-thread-pool-30] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:17:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505471000 ms
13:17:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505471000 ms
13:17:57.872 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:57.873 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:57.873 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:57.874 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:57.875 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:57.875 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:57.875 [receiver-supervisor-future-17] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:57.876 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:57.876 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:57.876 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:57.877 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:57.877 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:17:59.882 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:17:59.884 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:17:59.884 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:17:59.884 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:17:59.885 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:17:59.885 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:17:59.885 [receiver-supervisor-future-18] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:17:59.886 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:17:59.886 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:17:59.886 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:17:59.887 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:17:59.887 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:00.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505480000 ms
13:18:00.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505480000 ms.0 from job set of time 1760505480000 ms
13:18:00.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505480000 ms.0 from job set of time 1760505480000 ms
13:18:00.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505480000 ms (execution: 0.001 s)
13:18:00.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 12 from persistence list
13:18:00.007 [block-manager-storage-async-thread-pool-33] INFO  org.apache.spark.storage.BlockManager - Removing RDD 12
13:18:00.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[12] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505480000 ms
13:18:00.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505474000 ms
13:18:00.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505474000 ms
13:18:01.888 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:01.889 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:01.889 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:01.889 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:01.890 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:01.890 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:01.890 [receiver-supervisor-future-19] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:01.891 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:01.891 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:01.891 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:01.891 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:01.892 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:03.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505483000 ms
13:18:03.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505483000 ms.0 from job set of time 1760505483000 ms
13:18:03.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505483000 ms.0 from job set of time 1760505483000 ms
13:18:03.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505483000 ms (execution: 0.000 s)
13:18:03.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 13 from persistence list
13:18:03.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[13] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505483000 ms
13:18:03.009 [block-manager-storage-async-thread-pool-36] INFO  org.apache.spark.storage.BlockManager - Removing RDD 13
13:18:03.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505477000 ms
13:18:03.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505477000 ms
13:18:03.893 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:03.894 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:03.895 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:03.895 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:03.896 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:03.896 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:03.896 [receiver-supervisor-future-20] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:03.897 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:03.897 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:03.897 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:03.897 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:03.898 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:05.903 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:05.904 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:05.904 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:05.904 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:05.905 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:05.905 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:05.905 [receiver-supervisor-future-21] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:05.906 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:05.906 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:05.906 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:05.906 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:05.906 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:06.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505486000 ms
13:18:06.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505486000 ms.0 from job set of time 1760505486000 ms
13:18:06.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505486000 ms.0 from job set of time 1760505486000 ms
13:18:06.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505486000 ms (execution: 0.001 s)
13:18:06.003 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 14 from persistence list
13:18:06.004 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[14] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505486000 ms
13:18:06.004 [block-manager-storage-async-thread-pool-39] INFO  org.apache.spark.storage.BlockManager - Removing RDD 14
13:18:06.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505480000 ms
13:18:06.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505480000 ms
13:18:07.911 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:07.912 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:07.912 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:07.912 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:07.913 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:07.913 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:07.913 [receiver-supervisor-future-22] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:07.913 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:07.913 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:07.913 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:07.914 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:07.914 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:09.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505489000 ms
13:18:09.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505489000 ms.0 from job set of time 1760505489000 ms
13:18:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505489000 ms.0 from job set of time 1760505489000 ms
13:18:09.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505489000 ms (execution: 0.001 s)
13:18:09.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:18:09.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505489000 ms
13:18:09.009 [block-manager-storage-async-thread-pool-42] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:18:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505483000 ms
13:18:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505483000 ms
13:18:09.919 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:09.919 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:09.919 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:09.920 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:09.920 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:09.920 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:09.920 [receiver-supervisor-future-23] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:09.921 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:09.921 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:09.921 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:09.921 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:09.921 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:11.921 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:11.922 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:11.923 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:11.923 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:11.924 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:11.924 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:11.924 [receiver-supervisor-future-24] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:11.925 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:11.926 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:11.926 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:11.926 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:11.926 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:12.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505492000 ms
13:18:12.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505492000 ms.0 from job set of time 1760505492000 ms
13:18:12.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505492000 ms.0 from job set of time 1760505492000 ms
13:18:12.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505492000 ms (execution: 0.001 s)
13:18:12.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 16 from persistence list
13:18:12.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[16] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505492000 ms
13:18:12.010 [block-manager-storage-async-thread-pool-45] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:18:12.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505486000 ms
13:18:12.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505486000 ms
13:18:13.931 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:13.933 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:13.933 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:13.934 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:13.935 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:13.935 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:13.935 [receiver-supervisor-future-25] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:13.937 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:13.937 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:13.937 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:13.938 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:13.938 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:15.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505495000 ms
13:18:15.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505495000 ms.0 from job set of time 1760505495000 ms
13:18:15.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505495000 ms.0 from job set of time 1760505495000 ms
13:18:15.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505495000 ms (execution: 0.001 s)
13:18:15.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 17 from persistence list
13:18:15.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[17] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505495000 ms
13:18:15.006 [block-manager-storage-async-thread-pool-48] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:18:15.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505489000 ms
13:18:15.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505489000 ms
13:18:15.940 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:15.941 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:15.942 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:15.942 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:15.944 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:15.944 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:15.944 [receiver-supervisor-future-26] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:15.945 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:15.945 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:15.945 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:15.946 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:15.946 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:17.949 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:17.950 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:17.951 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:17.951 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:17.952 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:17.952 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:17.952 [receiver-supervisor-future-27] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:17.954 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:17.954 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:17.954 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:17.954 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:17.955 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:18.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505498000 ms
13:18:18.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505498000 ms.0 from job set of time 1760505498000 ms
13:18:18.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505498000 ms.0 from job set of time 1760505498000 ms
13:18:18.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505498000 ms (execution: 0.001 s)
13:18:18.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 18 from persistence list
13:18:18.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[18] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505498000 ms
13:18:18.012 [block-manager-storage-async-thread-pool-51] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:18:18.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505492000 ms
13:18:18.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505492000 ms
13:18:19.956 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:19.956 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:19.957 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:19.957 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:19.957 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:19.957 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:19.957 [receiver-supervisor-future-28] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:19.958 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:19.958 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:19.958 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:19.958 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:19.958 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:21.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505501000 ms
13:18:21.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505501000 ms.0 from job set of time 1760505501000 ms
13:18:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505501000 ms.0 from job set of time 1760505501000 ms
13:18:21.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505501000 ms (execution: 0.001 s)
13:18:21.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 19 from persistence list
13:18:21.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[19] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505501000 ms
13:18:21.010 [block-manager-storage-async-thread-pool-54] INFO  org.apache.spark.storage.BlockManager - Removing RDD 19
13:18:21.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505495000 ms
13:18:21.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505495000 ms
13:18:21.963 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:21.964 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:21.965 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:21.965 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:21.966 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:21.966 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:21.966 [receiver-supervisor-future-29] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:21.968 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:21.968 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:21.968 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:21.969 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:21.969 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:23.974 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:23.975 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:23.975 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:23.975 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:23.976 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:23.976 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:23.976 [receiver-supervisor-future-30] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:23.979 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:23.979 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:23.979 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:23.979 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:23.979 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:24.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505504000 ms
13:18:24.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505504000 ms.0 from job set of time 1760505504000 ms
13:18:24.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505504000 ms.0 from job set of time 1760505504000 ms
13:18:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505504000 ms (execution: 0.001 s)
13:18:24.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 20 from persistence list
13:18:24.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[20] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505504000 ms
13:18:24.005 [block-manager-storage-async-thread-pool-57] INFO  org.apache.spark.storage.BlockManager - Removing RDD 20
13:18:24.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505498000 ms
13:18:24.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505498000 ms
13:18:25.982 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:25.983 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:25.983 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:25.984 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:25.984 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:25.984 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:25.984 [receiver-supervisor-future-31] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:25.986 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:25.986 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:25.986 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:25.986 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:25.987 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505507000 ms
13:18:27.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505507000 ms.0 from job set of time 1760505507000 ms
13:18:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505507000 ms.0 from job set of time 1760505507000 ms
13:18:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505507000 ms (execution: 0.001 s)
13:18:27.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 21 from persistence list
13:18:27.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[21] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505507000 ms
13:18:27.011 [block-manager-storage-async-thread-pool-60] INFO  org.apache.spark.storage.BlockManager - Removing RDD 21
13:18:27.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505501000 ms
13:18:27.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505501000 ms
13:18:27.992 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:27.994 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:27.994 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:27.994 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:27.994 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:27.994 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:27.994 [receiver-supervisor-future-32] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:27.996 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:27.996 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:27.996 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:27.996 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:27.996 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:30.000 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:30.001 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:30.001 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:30.002 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:30.003 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:30.003 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:30.003 [receiver-supervisor-future-33] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:30.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505510000 ms
13:18:30.005 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:30.005 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:30.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505510000 ms.0 from job set of time 1760505510000 ms
13:18:30.005 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:30.005 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:30.006 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:30.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505510000 ms.0 from job set of time 1760505510000 ms
13:18:30.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505510000 ms (execution: 0.001 s)
13:18:30.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 22 from persistence list
13:18:30.008 [block-manager-storage-async-thread-pool-63] INFO  org.apache.spark.storage.BlockManager - Removing RDD 22
13:18:30.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[22] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505510000 ms
13:18:30.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505504000 ms
13:18:30.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505504000 ms
13:18:32.011 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:32.012 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:32.012 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:32.012 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:32.013 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:32.013 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:32.014 [receiver-supervisor-future-34] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:32.015 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:32.015 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:32.015 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:32.016 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:32.016 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505513000 ms
13:18:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505513000 ms.0 from job set of time 1760505513000 ms
13:18:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505513000 ms.0 from job set of time 1760505513000 ms
13:18:33.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505513000 ms (execution: 0.001 s)
13:18:33.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 23 from persistence list
13:18:33.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[23] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505513000 ms
13:18:33.009 [block-manager-storage-async-thread-pool-66] INFO  org.apache.spark.storage.BlockManager - Removing RDD 23
13:18:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505507000 ms
13:18:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505507000 ms
13:18:34.019 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:34.021 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:34.021 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:34.021 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:34.022 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:34.022 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:34.022 [receiver-supervisor-future-35] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:34.023 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:34.023 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:34.023 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:34.024 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:34.024 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:36.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505516000 ms
13:18:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505516000 ms.0 from job set of time 1760505516000 ms
13:18:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505516000 ms.0 from job set of time 1760505516000 ms
13:18:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505516000 ms (execution: 0.001 s)
13:18:36.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 24 from persistence list
13:18:36.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[24] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505516000 ms
13:18:36.009 [block-manager-storage-async-thread-pool-69] INFO  org.apache.spark.storage.BlockManager - Removing RDD 24
13:18:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505510000 ms
13:18:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505510000 ms
13:18:36.029 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:36.030 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:36.030 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:36.030 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:36.031 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:36.031 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:36.032 [receiver-supervisor-future-36] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:36.033 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:36.033 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:36.033 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:36.034 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:36.034 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:38.037 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:38.039 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:38.039 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:38.040 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:38.042 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:38.042 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:38.042 [receiver-supervisor-future-37] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:38.043 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:38.043 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:38.043 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:38.044 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:38.044 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:39.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505519000 ms
13:18:39.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505519000 ms.0 from job set of time 1760505519000 ms
13:18:39.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505519000 ms.0 from job set of time 1760505519000 ms
13:18:39.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505519000 ms (execution: 0.001 s)
13:18:39.003 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 25 from persistence list
13:18:39.004 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[25] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505519000 ms
13:18:39.004 [block-manager-storage-async-thread-pool-72] INFO  org.apache.spark.storage.BlockManager - Removing RDD 25
13:18:39.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505513000 ms
13:18:39.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505513000 ms
13:18:40.048 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:40.049 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:40.049 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:40.049 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:40.051 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:40.051 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:40.051 [receiver-supervisor-future-38] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:40.052 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:40.052 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:40.052 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:40.053 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:40.053 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:42.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505522000 ms
13:18:42.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505522000 ms.0 from job set of time 1760505522000 ms
13:18:42.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505522000 ms.0 from job set of time 1760505522000 ms
13:18:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505522000 ms (execution: 0.001 s)
13:18:42.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 26 from persistence list
13:18:42.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[26] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505522000 ms
13:18:42.008 [block-manager-storage-async-thread-pool-75] INFO  org.apache.spark.storage.BlockManager - Removing RDD 26
13:18:42.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505516000 ms
13:18:42.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505516000 ms
13:18:42.056 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:42.057 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:42.057 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:42.057 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:42.058 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:42.058 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:42.059 [receiver-supervisor-future-39] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:42.060 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:42.060 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:42.060 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:42.060 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:42.060 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:44.064 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:44.065 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:44.066 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:44.067 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:44.068 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:44.068 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:44.068 [receiver-supervisor-future-40] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:44.069 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:44.070 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:44.070 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:44.070 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:44.070 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:45.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505525000 ms
13:18:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505525000 ms.0 from job set of time 1760505525000 ms
13:18:45.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505525000 ms.0 from job set of time 1760505525000 ms
13:18:45.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505525000 ms (execution: 0.001 s)
13:18:45.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 27 from persistence list
13:18:45.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[27] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505525000 ms
13:18:45.008 [block-manager-storage-async-thread-pool-78] INFO  org.apache.spark.storage.BlockManager - Removing RDD 27
13:18:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505519000 ms
13:18:45.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505519000 ms
13:18:46.074 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:46.075 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:46.076 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:46.076 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:46.077 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:46.077 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:46.087 [receiver-supervisor-future-41] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:46.088 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:46.088 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:46.088 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:46.089 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:46.089 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:48.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505528000 ms
13:18:48.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505528000 ms.0 from job set of time 1760505528000 ms
13:18:48.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505528000 ms.0 from job set of time 1760505528000 ms
13:18:48.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505528000 ms (execution: 0.001 s)
13:18:48.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 28 from persistence list
13:18:48.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[28] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505528000 ms
13:18:48.008 [block-manager-storage-async-thread-pool-81] INFO  org.apache.spark.storage.BlockManager - Removing RDD 28
13:18:48.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505522000 ms
13:18:48.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505522000 ms
13:18:48.093 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:48.094 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:48.095 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:48.095 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:48.096 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:48.096 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:48.096 [receiver-supervisor-future-42] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:48.098 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:48.098 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:48.098 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:48.098 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:48.098 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:50.100 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:50.102 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:50.102 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:50.102 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:50.103 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:50.103 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:50.104 [receiver-supervisor-future-43] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:50.105 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:50.106 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:50.106 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:50.106 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:50.107 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:51.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505531000 ms
13:18:51.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505531000 ms.0 from job set of time 1760505531000 ms
13:18:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505531000 ms.0 from job set of time 1760505531000 ms
13:18:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505531000 ms (execution: 0.002 s)
13:18:51.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 29 from persistence list
13:18:51.009 [block-manager-storage-async-thread-pool-84] INFO  org.apache.spark.storage.BlockManager - Removing RDD 29
13:18:51.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[29] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505531000 ms
13:18:51.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505525000 ms
13:18:51.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505525000 ms
13:18:52.111 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:52.113 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:52.113 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:52.113 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:52.114 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:52.115 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:52.115 [receiver-supervisor-future-44] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:52.116 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:52.117 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:52.117 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:52.117 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:52.117 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:54.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505534000 ms
13:18:54.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505534000 ms.0 from job set of time 1760505534000 ms
13:18:54.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505534000 ms.0 from job set of time 1760505534000 ms
13:18:54.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 30 from persistence list
13:18:54.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505534000 ms (execution: 0.001 s)
13:18:54.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[30] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505534000 ms
13:18:54.007 [block-manager-storage-async-thread-pool-87] INFO  org.apache.spark.storage.BlockManager - Removing RDD 30
13:18:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505528000 ms
13:18:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505528000 ms
13:18:54.121 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:54.122 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:54.123 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:54.123 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:54.124 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:54.124 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:54.124 [receiver-supervisor-future-45] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:54.126 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:54.126 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:54.126 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:54.126 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:54.127 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:56.127 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:56.128 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:56.129 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:56.129 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:56.130 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:56.130 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:56.130 [receiver-supervisor-future-46] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:56.132 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:56.132 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:56.133 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:56.133 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:56.133 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:18:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505537000 ms
13:18:57.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505537000 ms.0 from job set of time 1760505537000 ms
13:18:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505537000 ms.0 from job set of time 1760505537000 ms
13:18:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505537000 ms (execution: 0.001 s)
13:18:57.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 31 from persistence list
13:18:57.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[31] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505537000 ms
13:18:57.010 [block-manager-storage-async-thread-pool-90] INFO  org.apache.spark.storage.BlockManager - Removing RDD 31
13:18:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505531000 ms
13:18:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505531000 ms
13:18:58.138 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:18:58.139 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:18:58.139 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:18:58.139 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:18:58.139 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:18:58.139 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:18:58.139 [receiver-supervisor-future-47] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:18:58.139 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:18:58.139 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:18:58.139 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:18:58.140 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:18:58.140 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:00.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505540000 ms
13:19:00.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505540000 ms.0 from job set of time 1760505540000 ms
13:19:00.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505540000 ms.0 from job set of time 1760505540000 ms
13:19:00.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505540000 ms (execution: 0.001 s)
13:19:00.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 32 from persistence list
13:19:00.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[32] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505540000 ms
13:19:00.009 [block-manager-storage-async-thread-pool-93] INFO  org.apache.spark.storage.BlockManager - Removing RDD 32
13:19:00.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505534000 ms
13:19:00.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505534000 ms
13:19:00.145 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:00.145 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49655
13:19:00.145 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:00.145 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:00.145 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:00.145 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:00.145 [receiver-supervisor-future-48] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:00.146 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:00.146 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:00.146 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:00.146 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:00.146 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:00.158 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:19:00.160 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:19:00.160 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:19:00.160 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:19:00.160 [dispatcher-event-loop-4] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:19:00.160 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:19:06.975 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:19:06.997 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:19:07.026 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:19:07.026 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:19:07.027 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:19:07.027 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:19:07.034 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:19:07.039 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:19:07.039 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:19:07.056 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:19:07.057 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:19:07.057 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:19:07.057 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:19:07.057 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:19:07.148 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49719.
13:19:07.156 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:19:07.167 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:19:07.173 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:19:07.174 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:19:07.175 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:19:07.186 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-cc98e4d3-3e5a-4281-8252-aeb7306a64f2
13:19:07.212 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:19:07.218 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:19:07.236 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5933ms to org.sparkproject.jetty.util.log.Slf4jLog
13:19:07.284 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:19:07.291 [main] INFO  org.sparkproject.jetty.server.Server - Started @5988ms
13:19:07.306 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@27068a50{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:19:07.306 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:19:07.313 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e868ef5{/,null,AVAILABLE,@Spark}
13:19:07.341 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:19:07.344 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:19:07.350 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49720.
13:19:07.350 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49720
13:19:07.350 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:19:07.352 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49720, None)
13:19:07.353 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49720 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49720, None)
13:19:07.354 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49720, None)
13:19:07.354 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49720, None)
13:19:07.432 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e868ef5{/,null,STOPPED,@Spark}
13:19:07.433 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11bd803{/jobs,null,AVAILABLE,@Spark}
13:19:07.433 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75798d03{/jobs/json,null,AVAILABLE,@Spark}
13:19:07.434 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@442f92e6{/jobs/job,null,AVAILABLE,@Spark}
13:19:07.434 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a55f148{/jobs/job/json,null,AVAILABLE,@Spark}
13:19:07.435 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae2ed38{/stages,null,AVAILABLE,@Spark}
13:19:07.435 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/stages/json,null,AVAILABLE,@Spark}
13:19:07.437 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1431267b{/stages/stage,null,AVAILABLE,@Spark}
13:19:07.437 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/stages/stage/json,null,AVAILABLE,@Spark}
13:19:07.437 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/stages/pool,null,AVAILABLE,@Spark}
13:19:07.438 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/stages/pool/json,null,AVAILABLE,@Spark}
13:19:07.438 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/storage,null,AVAILABLE,@Spark}
13:19:07.439 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/storage/json,null,AVAILABLE,@Spark}
13:19:07.439 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/storage/rdd,null,AVAILABLE,@Spark}
13:19:07.439 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/storage/rdd/json,null,AVAILABLE,@Spark}
13:19:07.440 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/environment,null,AVAILABLE,@Spark}
13:19:07.441 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/environment/json,null,AVAILABLE,@Spark}
13:19:07.441 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/executors,null,AVAILABLE,@Spark}
13:19:07.442 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/executors/json,null,AVAILABLE,@Spark}
13:19:07.442 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/executors/threadDump,null,AVAILABLE,@Spark}
13:19:07.443 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:19:07.447 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/static,null,AVAILABLE,@Spark}
13:19:07.447 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@167279d1{/,null,AVAILABLE,@Spark}
13:19:07.448 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d96250{/api,null,AVAILABLE,@Spark}
13:19:07.448 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d178d55{/jobs/job/kill,null,AVAILABLE,@Spark}
13:19:07.449 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a480135{/stages/stage/kill,null,AVAILABLE,@Spark}
13:19:07.451 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33364212{/metrics/json,null,AVAILABLE,@Spark}
13:19:07.609 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:19:07.609 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:19:07.610 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:19:07.610 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:19:07.610 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:19:07.610 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@4fb25368
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:19:07.611 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@63ef338e
13:19:07.632 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760505549000
13:19:07.632 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760505549000 ms
13:19:07.633 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:19:07.634 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/streaming,null,AVAILABLE,@Spark}
13:19:07.635 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/streaming/json,null,AVAILABLE,@Spark}
13:19:07.635 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18da4dd{/streaming/batch,null,AVAILABLE,@Spark}
13:19:07.635 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/streaming/batch/json,null,AVAILABLE,@Spark}
13:19:07.636 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/static/streaming,null,AVAILABLE,@Spark}
13:19:07.636 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:19:07.638 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:19:07.641 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:35) with 1 output partitions
13:19:07.641 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:35)
13:19:07.641 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:19:07.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:19:07.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:19:07.692 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:19:08.100 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:19:08.103 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49720 (size: 35.5 KiB, free: 2004.6 MiB)
13:19:08.105 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:19:08.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:19:08.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:19:08.137 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:19:08.143 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:19:08.256 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760505548400
13:19:08.257 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:19:08.257 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:19:08.259 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:08.259 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:08.259 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:08.260 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:08.260 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:19:08.261 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:08.263 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:08.263 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:08.263 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:08.264 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:08.264 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:09.032 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505549000 ms
13:19:09.034 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505549000 ms.0 from job set of time 1760505549000 ms
13:19:09.037 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505549000 ms.0 from job set of time 1760505549000 ms
13:19:09.038 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.037 s for time 1760505549000 ms (execution: 0.004 s)
13:19:09.040 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:19:09.041 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:19:10.271 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:10.278 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:10.278 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:10.278 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:10.280 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:10.281 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:10.280 [receiver-supervisor-future-1] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:10.281 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:10.282 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:10.282 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:10.282 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:10.283 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:12.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505552000 ms
13:19:12.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505552000 ms.0 from job set of time 1760505552000 ms
13:19:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505552000 ms.0 from job set of time 1760505552000 ms
13:19:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505552000 ms (execution: 0.001 s)
13:19:12.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:19:12.022 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:19:12.023 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505552000 ms
13:19:12.023 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:19:12.024 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:19:12.288 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:12.289 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:12.290 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:12.290 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:12.291 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:12.291 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:12.291 [receiver-supervisor-future-2] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:12.292 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:12.292 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:12.292 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:12.293 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:12.293 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:14.299 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:14.300 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:14.301 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:14.301 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:14.302 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:14.302 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:14.302 [receiver-supervisor-future-3] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:14.304 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:14.304 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:14.304 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:14.305 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:14.305 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:15.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505555000 ms
13:19:15.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505555000 ms.0 from job set of time 1760505555000 ms
13:19:15.012 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505555000 ms.0 from job set of time 1760505555000 ms
13:19:15.013 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.012 s for time 1760505555000 ms (execution: 0.001 s)
13:19:15.013 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 2 from persistence list
13:19:15.015 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[2] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505555000 ms
13:19:15.015 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:19:15.017 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505549000 ms
13:19:15.017 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505549000 ms
13:19:16.308 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:16.310 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:16.310 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:16.310 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:16.312 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:16.312 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:16.312 [receiver-supervisor-future-4] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:16.315 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:16.315 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:16.315 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:16.316 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:16.316 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:18.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505558000 ms
13:19:18.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505558000 ms.0 from job set of time 1760505558000 ms
13:19:18.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505558000 ms.0 from job set of time 1760505558000 ms
13:19:18.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505558000 ms (execution: 0.001 s)
13:19:18.011 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 3 from persistence list
13:19:18.013 [block-manager-storage-async-thread-pool-6] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:19:18.013 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[3] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505558000 ms
13:19:18.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505552000 ms
13:19:18.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505552000 ms
13:19:18.319 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:18.321 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:18.321 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:18.322 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:18.323 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:18.323 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:18.323 [receiver-supervisor-future-5] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:18.324 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:18.324 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:18.324 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:18.324 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:18.325 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:20.330 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:20.330 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:20.331 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:20.331 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:20.331 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:20.331 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:20.331 [receiver-supervisor-future-6] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:20.332 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:20.332 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:20.332 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:20.332 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:20.332 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:21.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505561000 ms
13:19:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505561000 ms.0 from job set of time 1760505561000 ms
13:19:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505561000 ms.0 from job set of time 1760505561000 ms
13:19:21.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505561000 ms (execution: 0.000 s)
13:19:21.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 4 from persistence list
13:19:21.010 [block-manager-storage-async-thread-pool-9] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:19:21.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[4] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505561000 ms
13:19:21.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505555000 ms
13:19:21.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505555000 ms
13:19:22.337 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:22.339 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:22.339 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:22.339 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:22.340 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:22.341 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:22.341 [receiver-supervisor-future-7] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:22.342 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:22.342 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:22.342 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:22.343 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:22.343 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505564000 ms
13:19:24.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505564000 ms.0 from job set of time 1760505564000 ms
13:19:24.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505564000 ms.0 from job set of time 1760505564000 ms
13:19:24.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.010 s for time 1760505564000 ms (execution: 0.003 s)
13:19:24.011 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 5 from persistence list
13:19:24.012 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 5
13:19:24.012 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[5] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505564000 ms
13:19:24.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505558000 ms
13:19:24.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505558000 ms
13:19:24.348 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:24.349 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:24.350 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:24.350 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:24.351 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:24.351 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:24.351 [receiver-supervisor-future-8] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:24.353 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:24.353 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:24.353 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:24.354 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:24.354 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:26.358 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:26.359 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:26.360 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:26.360 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:26.361 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:26.361 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:26.361 [receiver-supervisor-future-9] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:26.363 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:26.363 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:26.363 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:26.363 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:26.364 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:27.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505567000 ms
13:19:27.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505567000 ms.0 from job set of time 1760505567000 ms
13:19:27.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505567000 ms.0 from job set of time 1760505567000 ms
13:19:27.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505567000 ms (execution: 0.001 s)
13:19:27.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 6 from persistence list
13:19:27.006 [block-manager-storage-async-thread-pool-15] INFO  org.apache.spark.storage.BlockManager - Removing RDD 6
13:19:27.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[6] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505567000 ms
13:19:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505561000 ms
13:19:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505561000 ms
13:19:28.365 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:28.366 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:28.366 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:28.366 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:28.368 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:28.368 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:28.368 [receiver-supervisor-future-10] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:28.369 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:28.369 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:28.369 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:28.369 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:28.370 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:30.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505570000 ms
13:19:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505570000 ms.0 from job set of time 1760505570000 ms
13:19:30.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505570000 ms.0 from job set of time 1760505570000 ms
13:19:30.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505570000 ms (execution: 0.002 s)
13:19:30.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 7 from persistence list
13:19:30.011 [block-manager-storage-async-thread-pool-18] INFO  org.apache.spark.storage.BlockManager - Removing RDD 7
13:19:30.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[7] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505570000 ms
13:19:30.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505564000 ms
13:19:30.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505564000 ms
13:19:30.370 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:30.371 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:30.371 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:30.372 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:30.373 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:30.373 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:30.373 [receiver-supervisor-future-11] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:30.374 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:30.374 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:30.374 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:30.374 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:30.375 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:32.376 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:32.377 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:32.377 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:32.377 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:32.378 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:32.379 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:32.378 [receiver-supervisor-future-12] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:32.379 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:32.380 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:32.380 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:32.380 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:32.380 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:33.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505573000 ms
13:19:33.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505573000 ms.0 from job set of time 1760505573000 ms
13:19:33.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505573000 ms.0 from job set of time 1760505573000 ms
13:19:33.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505573000 ms (execution: 0.002 s)
13:19:33.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:19:33.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505573000 ms
13:19:33.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505567000 ms
13:19:33.012 [block-manager-storage-async-thread-pool-21] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:19:33.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505567000 ms
13:19:34.385 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:19:34.387 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49719
13:19:34.387 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:19:34.387 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:19:34.388 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:19:34.388 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:19:34.388 [receiver-supervisor-future-13] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:19:34.390 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:19:34.390 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:19:34.390 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:19:34.391 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:19:34.391 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:19:36.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505576000 ms
13:19:36.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505576000 ms.0 from job set of time 1760505576000 ms
13:19:36.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505576000 ms.0 from job set of time 1760505576000 ms
13:19:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505576000 ms (execution: 0.000 s)
13:19:36.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 9 from persistence list
13:19:36.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[9] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505576000 ms
13:19:36.007 [block-manager-storage-async-thread-pool-24] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:19:36.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505570000 ms
13:19:36.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505570000 ms
13:19:36.150 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:19:36.152 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:19:36.152 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:19:36.152 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:19:36.152 [dispatcher-event-loop-9] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:19:36.153 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:20:30.625 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:20:30.646 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:20:30.675 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:20:30.675 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:20:30.675 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:20:30.676 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:20:30.683 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:20:30.688 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:20:30.688 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:20:30.705 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:20:30.706 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:20:30.706 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:20:30.706 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:20:30.706 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:20:30.797 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49744.
13:20:30.805 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:20:30.816 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:20:30.822 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:20:30.822 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:20:30.823 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:20:30.834 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-ad146a4c-e65e-4a4b-841b-97651a285997
13:20:30.854 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:20:30.859 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:20:30.873 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5895ms to org.sparkproject.jetty.util.log.Slf4jLog
13:20:30.913 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:20:30.919 [main] INFO  org.sparkproject.jetty.server.Server - Started @5941ms
13:20:30.932 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@27068a50{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:20:30.933 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:20:30.939 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e868ef5{/,null,AVAILABLE,@Spark}
13:20:30.968 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:20:30.971 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:20:30.978 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49745.
13:20:30.978 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49745
13:20:30.978 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:20:30.980 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49745, None)
13:20:30.981 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49745 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49745, None)
13:20:30.982 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49745, None)
13:20:30.983 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49745, None)
13:20:31.046 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e868ef5{/,null,STOPPED,@Spark}
13:20:31.047 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11bd803{/jobs,null,AVAILABLE,@Spark}
13:20:31.047 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75798d03{/jobs/json,null,AVAILABLE,@Spark}
13:20:31.048 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@442f92e6{/jobs/job,null,AVAILABLE,@Spark}
13:20:31.048 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a55f148{/jobs/job/json,null,AVAILABLE,@Spark}
13:20:31.049 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae2ed38{/stages,null,AVAILABLE,@Spark}
13:20:31.049 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/stages/json,null,AVAILABLE,@Spark}
13:20:31.049 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1431267b{/stages/stage,null,AVAILABLE,@Spark}
13:20:31.050 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/stages/stage/json,null,AVAILABLE,@Spark}
13:20:31.050 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/stages/pool,null,AVAILABLE,@Spark}
13:20:31.050 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/stages/pool/json,null,AVAILABLE,@Spark}
13:20:31.051 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/storage,null,AVAILABLE,@Spark}
13:20:31.051 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/storage/json,null,AVAILABLE,@Spark}
13:20:31.051 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/storage/rdd,null,AVAILABLE,@Spark}
13:20:31.052 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/storage/rdd/json,null,AVAILABLE,@Spark}
13:20:31.052 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/environment,null,AVAILABLE,@Spark}
13:20:31.053 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/environment/json,null,AVAILABLE,@Spark}
13:20:31.053 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/executors,null,AVAILABLE,@Spark}
13:20:31.053 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/executors/json,null,AVAILABLE,@Spark}
13:20:31.054 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/executors/threadDump,null,AVAILABLE,@Spark}
13:20:31.055 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:20:31.058 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/static,null,AVAILABLE,@Spark}
13:20:31.059 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@167279d1{/,null,AVAILABLE,@Spark}
13:20:31.060 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d96250{/api,null,AVAILABLE,@Spark}
13:20:31.060 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d178d55{/jobs/job/kill,null,AVAILABLE,@Spark}
13:20:31.061 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a480135{/stages/stage/kill,null,AVAILABLE,@Spark}
13:20:31.063 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33364212{/metrics/json,null,AVAILABLE,@Spark}
13:20:31.214 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:20:31.214 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:20:31.215 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:20:31.215 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@45a8349
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:20:31.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3987f6e8
13:20:31.237 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760505633000
13:20:31.238 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760505633000 ms
13:20:31.238 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:20:31.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4beabeec{/streaming,null,AVAILABLE,@Spark}
13:20:31.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b22d8a1{/streaming/json,null,AVAILABLE,@Spark}
13:20:31.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18da4dd{/streaming/batch,null,AVAILABLE,@Spark}
13:20:31.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68880c21{/streaming/batch/json,null,AVAILABLE,@Spark}
13:20:31.241 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47ffe971{/static/streaming,null,AVAILABLE,@Spark}
13:20:31.241 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:20:31.243 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:20:31.246 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:35) with 1 output partitions
13:20:31.246 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:35)
13:20:31.246 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:20:31.246 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:20:31.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:20:31.301 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:20:31.437 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:20:31.438 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49745 (size: 35.5 KiB, free: 2004.6 MiB)
13:20:31.439 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:20:31.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:20:31.446 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:20:31.467 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:20:31.475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:20:31.588 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760505631600
13:20:31.588 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:20:31.589 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:20:31.590 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:31.591 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:31.591 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:31.592 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:31.592 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:20:31.592 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:136) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:31.595 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:31.595 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:31.595 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:31.596 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)
	at org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2377)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:31.596 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:33.037 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505633000 ms
13:20:33.039 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505633000 ms.0 from job set of time 1760505633000 ms
13:20:33.042 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505633000 ms.0 from job set of time 1760505633000 ms
13:20:33.043 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.042 s for time 1760505633000 ms (execution: 0.004 s)
13:20:33.046 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:20:33.047 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:20:33.602 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:33.609 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:33.610 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:33.610 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:33.611 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:33.612 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:33.611 [receiver-supervisor-future-1] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:33.613 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:33.613 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:33.613 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:33.614 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:33.614 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:35.616 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:35.617 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:35.618 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:35.618 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:35.619 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:35.619 [receiver-supervisor-future-1] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:35.619 [receiver-supervisor-future-2] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:35.621 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:35.621 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:35.621 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:35.622 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:35.623 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:36.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505636000 ms
13:20:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505636000 ms.0 from job set of time 1760505636000 ms
13:20:36.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505636000 ms.0 from job set of time 1760505636000 ms
13:20:36.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505636000 ms (execution: 0.001 s)
13:20:36.011 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:20:36.023 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:20:36.024 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505636000 ms
13:20:36.025 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:20:36.025 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:20:37.625 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:37.626 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:37.626 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:37.627 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:37.628 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:37.628 [receiver-supervisor-future-2] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:37.628 [receiver-supervisor-future-3] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:37.629 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:37.629 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:37.630 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:37.630 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:37.630 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:39.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505639000 ms
13:20:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505639000 ms.0 from job set of time 1760505639000 ms
13:20:39.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505639000 ms.0 from job set of time 1760505639000 ms
13:20:39.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505639000 ms (execution: 0.001 s)
13:20:39.011 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 2 from persistence list
13:20:39.012 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:20:39.013 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[2] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505639000 ms
13:20:39.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505633000 ms
13:20:39.015 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505633000 ms
13:20:39.634 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:39.636 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:39.636 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:39.636 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:39.638 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:39.638 [receiver-supervisor-future-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:39.638 [receiver-supervisor-future-4] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:39.640 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:39.640 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:39.640 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:39.640 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:39.641 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:41.646 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:41.646 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:41.647 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:41.647 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:41.647 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:41.647 [receiver-supervisor-future-4] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:41.647 [receiver-supervisor-future-5] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:41.648 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:41.648 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:41.648 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:41.648 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:41.649 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:42.001 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505642000 ms
13:20:42.001 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505642000 ms.0 from job set of time 1760505642000 ms
13:20:42.001 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505642000 ms.0 from job set of time 1760505642000 ms
13:20:42.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.001 s for time 1760505642000 ms (execution: 0.000 s)
13:20:42.002 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 3 from persistence list
13:20:42.002 [block-manager-storage-async-thread-pool-6] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:20:42.002 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[3] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505642000 ms
13:20:42.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505636000 ms
13:20:42.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505636000 ms
13:20:43.654 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:43.655 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:43.655 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:43.655 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:43.655 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:43.655 [receiver-supervisor-future-5] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:43.655 [receiver-supervisor-future-6] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:43.656 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:43.656 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:43.656 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:43.656 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:43.657 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:45.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505645000 ms
13:20:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505645000 ms.0 from job set of time 1760505645000 ms
13:20:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505645000 ms.0 from job set of time 1760505645000 ms
13:20:45.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505645000 ms (execution: 0.000 s)
13:20:45.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 4 from persistence list
13:20:45.007 [block-manager-storage-async-thread-pool-9] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:20:45.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[4] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505645000 ms
13:20:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505639000 ms
13:20:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505639000 ms
13:20:45.662 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:45.662 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:45.662 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:45.663 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:45.663 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:45.663 [receiver-supervisor-future-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:45.663 [receiver-supervisor-future-7] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:45.664 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:45.664 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:45.664 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:45.664 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:45.664 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:47.669 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:47.671 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:47.671 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:47.671 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:47.672 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:47.673 [receiver-supervisor-future-7] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:47.673 [receiver-supervisor-future-8] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:47.674 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:47.674 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:47.674 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:47.675 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:47.676 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:48.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505648000 ms
13:20:48.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505648000 ms.0 from job set of time 1760505648000 ms
13:20:48.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505648000 ms.0 from job set of time 1760505648000 ms
13:20:48.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505648000 ms (execution: 0.001 s)
13:20:48.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 5 from persistence list
13:20:48.005 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 5
13:20:48.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[5] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505648000 ms
13:20:48.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505642000 ms
13:20:48.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505642000 ms
13:20:49.680 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:49.681 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:49.681 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:49.682 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:49.683 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:49.683 [receiver-supervisor-future-8] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:49.683 [receiver-supervisor-future-9] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:49.684 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:49.684 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:49.685 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:49.685 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:49.685 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:51.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505651000 ms
13:20:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505651000 ms.0 from job set of time 1760505651000 ms
13:20:51.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505651000 ms.0 from job set of time 1760505651000 ms
13:20:51.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505651000 ms (execution: 0.001 s)
13:20:51.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 6 from persistence list
13:20:51.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[6] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505651000 ms
13:20:51.012 [block-manager-storage-async-thread-pool-15] INFO  org.apache.spark.storage.BlockManager - Removing RDD 6
13:20:51.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505645000 ms
13:20:51.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505645000 ms
13:20:51.687 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:51.689 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:51.689 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:51.689 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:51.691 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:51.691 [receiver-supervisor-future-9] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:51.691 [receiver-supervisor-future-10] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:51.693 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:51.693 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:51.693 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:51.693 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:51.694 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:53.697 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:53.699 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:53.699 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:53.699 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:53.700 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:53.701 [receiver-supervisor-future-10] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:53.701 [receiver-supervisor-future-11] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:53.702 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:53.702 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:53.702 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:53.702 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:53.703 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:54.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505654000 ms
13:20:54.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505654000 ms.0 from job set of time 1760505654000 ms
13:20:54.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505654000 ms.0 from job set of time 1760505654000 ms
13:20:54.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505654000 ms (execution: 0.001 s)
13:20:54.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 7 from persistence list
13:20:54.009 [block-manager-storage-async-thread-pool-18] INFO  org.apache.spark.storage.BlockManager - Removing RDD 7
13:20:54.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[7] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505654000 ms
13:20:54.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505648000 ms
13:20:54.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505648000 ms
13:20:55.703 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:55.705 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:55.706 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:55.706 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:55.707 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:55.707 [receiver-supervisor-future-11] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:55.707 [receiver-supervisor-future-12] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:55.708 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:55.709 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:55.709 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:55.709 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:55.710 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:57.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505657000 ms
13:20:57.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505657000 ms.0 from job set of time 1760505657000 ms
13:20:57.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505657000 ms.0 from job set of time 1760505657000 ms
13:20:57.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505657000 ms (execution: 0.001 s)
13:20:57.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:20:57.009 [block-manager-storage-async-thread-pool-21] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:20:57.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505657000 ms
13:20:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505651000 ms
13:20:57.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505651000 ms
13:20:57.712 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:57.714 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:57.714 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:57.714 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:57.715 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:57.715 [receiver-supervisor-future-12] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:57.715 [receiver-supervisor-future-13] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:57.718 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:57.718 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:57.719 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:57.719 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:57.720 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:20:59.722 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:20:59.724 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:20:59.724 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:20:59.724 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:20:59.726 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:20:59.726 [receiver-supervisor-future-13] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:20:59.726 [receiver-supervisor-future-14] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:20:59.728 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:20:59.728 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:20:59.728 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:20:59.728 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:20:59.728 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:00.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505660000 ms
13:21:00.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505660000 ms.0 from job set of time 1760505660000 ms
13:21:00.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505660000 ms.0 from job set of time 1760505660000 ms
13:21:00.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505660000 ms (execution: 0.001 s)
13:21:00.003 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 9 from persistence list
13:21:00.004 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[9] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505660000 ms
13:21:00.004 [block-manager-storage-async-thread-pool-24] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:21:00.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505654000 ms
13:21:00.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505654000 ms
13:21:01.732 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:01.733 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:01.734 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:01.734 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:01.735 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:01.735 [receiver-supervisor-future-14] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:01.735 [receiver-supervisor-future-15] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:01.737 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:01.737 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:01.737 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:01.737 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:01.739 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:03.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505663000 ms
13:21:03.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505663000 ms.0 from job set of time 1760505663000 ms
13:21:03.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505663000 ms.0 from job set of time 1760505663000 ms
13:21:03.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505663000 ms (execution: 0.001 s)
13:21:03.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 10 from persistence list
13:21:03.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[10] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505663000 ms
13:21:03.008 [block-manager-storage-async-thread-pool-27] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:21:03.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505657000 ms
13:21:03.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505657000 ms
13:21:03.744 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:03.745 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:03.745 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:03.746 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:03.747 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:03.747 [receiver-supervisor-future-15] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:03.747 [receiver-supervisor-future-16] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:03.749 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:03.749 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:03.749 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:03.749 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:03.750 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:05.755 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:05.756 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:05.757 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:05.757 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:05.758 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:05.758 [receiver-supervisor-future-16] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:05.758 [receiver-supervisor-future-17] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:05.759 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:05.759 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:05.759 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:05.760 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:05.760 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:06.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505666000 ms
13:21:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505666000 ms.0 from job set of time 1760505666000 ms
13:21:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505666000 ms.0 from job set of time 1760505666000 ms
13:21:06.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505666000 ms (execution: 0.000 s)
13:21:06.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 11 from persistence list
13:21:06.009 [block-manager-storage-async-thread-pool-30] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:21:06.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[11] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505666000 ms
13:21:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505660000 ms
13:21:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505660000 ms
13:21:07.764 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:07.765 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:07.766 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:07.766 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:07.767 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:07.768 [receiver-supervisor-future-17] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:07.768 [receiver-supervisor-future-18] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:07.769 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:07.769 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:07.769 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:07.770 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:07.770 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:09.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505669000 ms
13:21:09.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505669000 ms.0 from job set of time 1760505669000 ms
13:21:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505669000 ms.0 from job set of time 1760505669000 ms
13:21:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505669000 ms (execution: 0.001 s)
13:21:09.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 12 from persistence list
13:21:09.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[12] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505669000 ms
13:21:09.009 [block-manager-storage-async-thread-pool-33] INFO  org.apache.spark.storage.BlockManager - Removing RDD 12
13:21:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505663000 ms
13:21:09.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505663000 ms
13:21:09.774 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:09.775 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:09.776 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:09.776 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:09.778 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:09.778 [receiver-supervisor-future-18] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:09.778 [receiver-supervisor-future-19] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:09.780 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:09.780 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:09.780 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:09.781 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:09.781 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:11.786 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:11.787 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:11.788 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:11.788 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:11.790 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:11.790 [receiver-supervisor-future-19] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:11.790 [receiver-supervisor-future-20] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:11.791 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:11.791 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:11.791 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:11.792 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:11.792 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:12.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505672000 ms
13:21:12.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505672000 ms.0 from job set of time 1760505672000 ms
13:21:12.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505672000 ms.0 from job set of time 1760505672000 ms
13:21:12.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505672000 ms (execution: 0.002 s)
13:21:12.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 13 from persistence list
13:21:12.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[13] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505672000 ms
13:21:12.011 [block-manager-storage-async-thread-pool-36] INFO  org.apache.spark.storage.BlockManager - Removing RDD 13
13:21:12.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505666000 ms
13:21:12.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505666000 ms
13:21:13.797 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:13.799 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:13.799 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:13.799 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:13.800 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:13.800 [receiver-supervisor-future-20] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:13.801 [receiver-supervisor-future-21] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:13.802 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:13.802 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:13.802 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:13.803 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:13.803 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:15.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505675000 ms
13:21:15.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505675000 ms.0 from job set of time 1760505675000 ms
13:21:15.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505675000 ms.0 from job set of time 1760505675000 ms
13:21:15.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505675000 ms (execution: 0.001 s)
13:21:15.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 14 from persistence list
13:21:15.011 [block-manager-storage-async-thread-pool-39] INFO  org.apache.spark.storage.BlockManager - Removing RDD 14
13:21:15.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[14] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505675000 ms
13:21:15.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505669000 ms
13:21:15.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505669000 ms
13:21:15.808 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:15.810 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:15.810 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:15.810 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:15.811 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:15.811 [receiver-supervisor-future-21] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:15.811 [receiver-supervisor-future-22] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:15.813 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:15.813 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:15.813 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:15.814 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:15.814 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:17.819 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:17.821 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:17.821 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:17.821 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:17.823 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:17.823 [receiver-supervisor-future-22] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:17.823 [receiver-supervisor-future-23] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:17.824 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:17.824 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:17.824 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:17.825 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:17.825 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:18.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505678000 ms
13:21:18.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505678000 ms.0 from job set of time 1760505678000 ms
13:21:18.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505678000 ms.0 from job set of time 1760505678000 ms
13:21:18.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505678000 ms (execution: 0.001 s)
13:21:18.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:21:18.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505678000 ms
13:21:18.008 [block-manager-storage-async-thread-pool-42] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:21:18.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505672000 ms
13:21:18.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505672000 ms
13:21:19.831 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:19.832 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:19.832 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:19.832 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:19.833 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:19.833 [receiver-supervisor-future-23] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:19.833 [receiver-supervisor-future-24] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:19.835 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:19.835 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:19.835 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:19.835 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:19.835 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:21.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505681000 ms
13:21:21.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505681000 ms.0 from job set of time 1760505681000 ms
13:21:21.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505681000 ms.0 from job set of time 1760505681000 ms
13:21:21.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505681000 ms (execution: 0.000 s)
13:21:21.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 16 from persistence list
13:21:21.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[16] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505681000 ms
13:21:21.005 [block-manager-storage-async-thread-pool-45] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:21:21.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505675000 ms
13:21:21.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505675000 ms
13:21:21.837 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:21.838 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:21.838 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:21.839 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:21.840 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:21.840 [receiver-supervisor-future-24] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:21.840 [receiver-supervisor-future-25] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:21.842 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:21.842 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:21.842 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:21.842 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:21.842 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:23.848 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:23.849 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:23.849 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:23.849 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:23.851 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:23.851 [receiver-supervisor-future-25] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:23.851 [receiver-supervisor-future-26] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:23.853 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:23.853 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:23.853 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:23.853 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:23.853 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:24.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505684000 ms
13:21:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505684000 ms.0 from job set of time 1760505684000 ms
13:21:24.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505684000 ms.0 from job set of time 1760505684000 ms
13:21:24.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505684000 ms (execution: 0.001 s)
13:21:24.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 17 from persistence list
13:21:24.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[17] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505684000 ms
13:21:24.006 [block-manager-storage-async-thread-pool-48] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:21:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505678000 ms
13:21:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505678000 ms
13:21:25.854 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:25.855 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:25.855 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:25.855 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:25.857 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:25.857 [receiver-supervisor-future-26] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:25.857 [receiver-supervisor-future-27] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:25.859 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:25.859 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:25.859 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:25.859 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:25.859 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505687000 ms
13:21:27.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505687000 ms.0 from job set of time 1760505687000 ms
13:21:27.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505687000 ms.0 from job set of time 1760505687000 ms
13:21:27.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505687000 ms (execution: 0.001 s)
13:21:27.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 18 from persistence list
13:21:27.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[18] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505687000 ms
13:21:27.010 [block-manager-storage-async-thread-pool-51] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:21:27.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505681000 ms
13:21:27.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505681000 ms
13:21:27.862 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:27.863 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:27.864 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:27.864 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:27.865 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:27.865 [receiver-supervisor-future-27] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:27.865 [receiver-supervisor-future-28] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:27.867 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:27.867 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:27.867 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:27.868 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:27.868 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:29.870 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:29.871 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:29.871 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:29.872 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:29.873 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:29.873 [receiver-supervisor-future-28] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:29.873 [receiver-supervisor-future-29] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:29.875 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:29.875 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:29.875 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:29.875 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:29.876 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:30.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505690000 ms
13:21:30.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505690000 ms.0 from job set of time 1760505690000 ms
13:21:30.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505690000 ms.0 from job set of time 1760505690000 ms
13:21:30.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505690000 ms (execution: 0.001 s)
13:21:30.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 19 from persistence list
13:21:30.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[19] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505690000 ms
13:21:30.007 [block-manager-storage-async-thread-pool-54] INFO  org.apache.spark.storage.BlockManager - Removing RDD 19
13:21:30.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505684000 ms
13:21:30.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505684000 ms
13:21:31.879 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:31.880 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:31.882 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:31.882 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:31.883 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:31.883 [receiver-supervisor-future-29] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:31.883 [receiver-supervisor-future-30] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:31.885 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:31.885 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:31.885 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:31.886 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:31.886 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:33.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505693000 ms
13:21:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505693000 ms.0 from job set of time 1760505693000 ms
13:21:33.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505693000 ms.0 from job set of time 1760505693000 ms
13:21:33.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505693000 ms (execution: 0.001 s)
13:21:33.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 20 from persistence list
13:21:33.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[20] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505693000 ms
13:21:33.010 [block-manager-storage-async-thread-pool-57] INFO  org.apache.spark.storage.BlockManager - Removing RDD 20
13:21:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505687000 ms
13:21:33.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505687000 ms
13:21:33.889 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:33.890 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:33.890 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:33.890 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:33.891 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:33.891 [receiver-supervisor-future-30] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:33.891 [receiver-supervisor-future-31] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:33.892 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:33.892 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:33.892 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:33.892 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:33.892 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:35.896 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:35.897 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:35.898 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:35.898 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:35.899 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:35.899 [receiver-supervisor-future-31] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:35.899 [receiver-supervisor-future-32] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:35.900 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:35.900 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:35.900 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:35.900 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:35.901 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:36.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505696000 ms
13:21:36.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505696000 ms.0 from job set of time 1760505696000 ms
13:21:36.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505696000 ms.0 from job set of time 1760505696000 ms
13:21:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505696000 ms (execution: 0.001 s)
13:21:36.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 21 from persistence list
13:21:36.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[21] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505696000 ms
13:21:36.008 [block-manager-storage-async-thread-pool-60] INFO  org.apache.spark.storage.BlockManager - Removing RDD 21
13:21:36.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505690000 ms
13:21:36.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505690000 ms
13:21:37.906 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:37.907 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:37.908 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:37.908 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:37.909 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:37.909 [receiver-supervisor-future-32] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:37.909 [receiver-supervisor-future-33] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:37.911 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:37.911 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:37.911 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:37.912 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:37.912 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:39.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505699000 ms
13:21:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505699000 ms.0 from job set of time 1760505699000 ms
13:21:39.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505699000 ms.0 from job set of time 1760505699000 ms
13:21:39.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505699000 ms (execution: 0.001 s)
13:21:39.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 22 from persistence list
13:21:39.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[22] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505699000 ms
13:21:39.011 [block-manager-storage-async-thread-pool-63] INFO  org.apache.spark.storage.BlockManager - Removing RDD 22
13:21:39.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505693000 ms
13:21:39.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505693000 ms
13:21:39.917 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:39.918 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:39.918 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:39.918 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:39.919 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:39.920 [receiver-supervisor-future-33] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:39.920 [receiver-supervisor-future-34] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:39.921 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:39.921 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:39.921 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:39.922 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:39.922 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:41.927 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:41.929 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:41.929 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:41.929 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:41.931 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:41.931 [receiver-supervisor-future-34] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:41.931 [receiver-supervisor-future-35] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:41.932 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:41.932 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:41.932 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:41.933 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:41.933 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:42.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505702000 ms
13:21:42.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505702000 ms.0 from job set of time 1760505702000 ms
13:21:42.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505702000 ms.0 from job set of time 1760505702000 ms
13:21:42.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505702000 ms (execution: 0.000 s)
13:21:42.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 23 from persistence list
13:21:42.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[23] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505702000 ms
13:21:42.007 [block-manager-storage-async-thread-pool-66] INFO  org.apache.spark.storage.BlockManager - Removing RDD 23
13:21:42.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505696000 ms
13:21:42.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505696000 ms
13:21:43.935 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:43.936 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:43.936 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:43.937 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:43.938 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:43.938 [receiver-supervisor-future-35] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:43.938 [receiver-supervisor-future-36] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:43.940 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:43.940 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:43.940 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:43.940 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:43.941 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:45.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505705000 ms
13:21:45.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505705000 ms.0 from job set of time 1760505705000 ms
13:21:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505705000 ms.0 from job set of time 1760505705000 ms
13:21:45.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505705000 ms (execution: 0.001 s)
13:21:45.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 24 from persistence list
13:21:45.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[24] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505705000 ms
13:21:45.007 [block-manager-storage-async-thread-pool-69] INFO  org.apache.spark.storage.BlockManager - Removing RDD 24
13:21:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505699000 ms
13:21:45.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505699000 ms
13:21:45.941 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:45.941 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:45.941 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:45.941 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:45.942 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:45.942 [receiver-supervisor-future-36] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:45.942 [receiver-supervisor-future-37] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:45.942 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:45.942 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:45.942 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:45.942 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:45.943 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:47.943 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:47.944 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:47.944 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:47.944 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:47.944 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:47.944 [receiver-supervisor-future-37] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:47.944 [receiver-supervisor-future-38] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:47.945 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:47.945 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:47.945 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:47.945 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:47.945 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:48.001 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505708000 ms
13:21:48.001 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505708000 ms.0 from job set of time 1760505708000 ms
13:21:48.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505708000 ms.0 from job set of time 1760505708000 ms
13:21:48.002 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 25 from persistence list
13:21:48.002 [block-manager-storage-async-thread-pool-72] INFO  org.apache.spark.storage.BlockManager - Removing RDD 25
13:21:48.002 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[25] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505708000 ms
13:21:48.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.002 s for time 1760505708000 ms (execution: 0.001 s)
13:21:48.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505702000 ms
13:21:48.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505702000 ms
13:21:49.951 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:49.952 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:49.952 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:49.952 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:49.954 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:49.954 [receiver-supervisor-future-38] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:49.954 [receiver-supervisor-future-39] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:49.955 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:49.955 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:49.955 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:49.956 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:49.956 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:51.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505711000 ms
13:21:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505711000 ms.0 from job set of time 1760505711000 ms
13:21:51.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505711000 ms.0 from job set of time 1760505711000 ms
13:21:51.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505711000 ms (execution: 0.001 s)
13:21:51.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 26 from persistence list
13:21:51.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[26] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505711000 ms
13:21:51.011 [block-manager-storage-async-thread-pool-75] INFO  org.apache.spark.storage.BlockManager - Removing RDD 26
13:21:51.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505705000 ms
13:21:51.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505705000 ms
13:21:51.961 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:51.963 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:51.963 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:51.964 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:51.965 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:51.965 [receiver-supervisor-future-39] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:51.965 [receiver-supervisor-future-40] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:51.965 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:51.965 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:51.965 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:51.966 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:51.966 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:53.971 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:53.974 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:53.974 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:53.974 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:53.975 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:53.976 [receiver-supervisor-future-40] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:53.976 [receiver-supervisor-future-41] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:53.978 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:53.978 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:53.978 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:53.978 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:53.979 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:54.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505714000 ms
13:21:54.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505714000 ms.0 from job set of time 1760505714000 ms
13:21:54.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505714000 ms.0 from job set of time 1760505714000 ms
13:21:54.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505714000 ms (execution: 0.001 s)
13:21:54.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 27 from persistence list
13:21:54.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[27] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505714000 ms
13:21:54.011 [block-manager-storage-async-thread-pool-78] INFO  org.apache.spark.storage.BlockManager - Removing RDD 27
13:21:54.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505708000 ms
13:21:54.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505708000 ms
13:21:55.981 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:55.983 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:55.983 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:55.983 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:55.984 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:55.985 [receiver-supervisor-future-41] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:55.985 [receiver-supervisor-future-42] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:55.986 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:55.986 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:55.986 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:55.987 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:55.987 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505717000 ms
13:21:57.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505717000 ms.0 from job set of time 1760505717000 ms
13:21:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505717000 ms.0 from job set of time 1760505717000 ms
13:21:57.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 28 from persistence list
13:21:57.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505717000 ms (execution: 0.001 s)
13:21:57.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[28] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505717000 ms
13:21:57.010 [block-manager-storage-async-thread-pool-81] INFO  org.apache.spark.storage.BlockManager - Removing RDD 28
13:21:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505711000 ms
13:21:57.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505711000 ms
13:21:57.990 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:21:57.991 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:21:57.991 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:21:57.991 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:21:57.993 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:21:57.993 [receiver-supervisor-future-42] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:21:57.993 [receiver-supervisor-future-43] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:21:57.994 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:21:57.994 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:21:57.994 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:21:57.995 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:21:57.995 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:21:59.999 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:00.000 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:00.001 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:00.001 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:00.003 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:00.003 [receiver-supervisor-future-43] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:00.003 [receiver-supervisor-future-44] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:00.005 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:00.005 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:00.005 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:00.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505720000 ms
13:22:00.006 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:00.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505720000 ms.0 from job set of time 1760505720000 ms
13:22:00.006 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:00.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505720000 ms.0 from job set of time 1760505720000 ms
13:22:00.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505720000 ms (execution: 0.001 s)
13:22:00.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 29 from persistence list
13:22:00.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[29] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505720000 ms
13:22:00.009 [block-manager-storage-async-thread-pool-84] INFO  org.apache.spark.storage.BlockManager - Removing RDD 29
13:22:00.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505714000 ms
13:22:00.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505714000 ms
13:22:02.010 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:02.011 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:02.011 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:02.011 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:02.013 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:02.013 [receiver-supervisor-future-44] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:02.013 [receiver-supervisor-future-45] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:02.014 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:02.014 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:02.014 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:02.015 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:02.015 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:03.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505723000 ms
13:22:03.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505723000 ms.0 from job set of time 1760505723000 ms
13:22:03.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505723000 ms.0 from job set of time 1760505723000 ms
13:22:03.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505723000 ms (execution: 0.001 s)
13:22:03.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 30 from persistence list
13:22:03.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[30] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505723000 ms
13:22:03.006 [block-manager-storage-async-thread-pool-87] INFO  org.apache.spark.storage.BlockManager - Removing RDD 30
13:22:03.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505717000 ms
13:22:03.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505717000 ms
13:22:04.019 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:04.020 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:04.020 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:04.020 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:04.022 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:04.022 [receiver-supervisor-future-45] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:04.022 [receiver-supervisor-future-46] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:04.024 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:04.024 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:04.024 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:04.024 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:04.024 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:06.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505726000 ms
13:22:06.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505726000 ms.0 from job set of time 1760505726000 ms
13:22:06.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505726000 ms.0 from job set of time 1760505726000 ms
13:22:06.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505726000 ms (execution: 0.001 s)
13:22:06.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 31 from persistence list
13:22:06.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[31] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505726000 ms
13:22:06.009 [block-manager-storage-async-thread-pool-90] INFO  org.apache.spark.storage.BlockManager - Removing RDD 31
13:22:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505720000 ms
13:22:06.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505720000 ms
13:22:06.030 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:06.030 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:06.031 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:06.031 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:06.032 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:06.032 [receiver-supervisor-future-46] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:06.032 [receiver-supervisor-future-47] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:06.034 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:06.034 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:06.034 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:06.034 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:06.034 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:08.039 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:08.040 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:08.040 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:08.040 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:08.041 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:08.041 [receiver-supervisor-future-47] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:08.041 [receiver-supervisor-future-48] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:08.043 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:08.043 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:08.043 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:08.043 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:08.043 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:09.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505729000 ms
13:22:09.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505729000 ms.0 from job set of time 1760505729000 ms
13:22:09.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505729000 ms.0 from job set of time 1760505729000 ms
13:22:09.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505729000 ms (execution: 0.001 s)
13:22:09.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 32 from persistence list
13:22:09.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[32] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505729000 ms
13:22:09.010 [block-manager-storage-async-thread-pool-93] INFO  org.apache.spark.storage.BlockManager - Removing RDD 32
13:22:09.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505723000 ms
13:22:09.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505723000 ms
13:22:10.045 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:10.047 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:10.047 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:10.047 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:10.048 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:10.048 [receiver-supervisor-future-48] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:10.048 [receiver-supervisor-future-49] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:10.050 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:10.050 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:10.050 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:10.051 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:10.051 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:12.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505732000 ms
13:22:12.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505732000 ms.0 from job set of time 1760505732000 ms
13:22:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505732000 ms.0 from job set of time 1760505732000 ms
13:22:12.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505732000 ms (execution: 0.001 s)
13:22:12.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 33 from persistence list
13:22:12.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[33] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505732000 ms
13:22:12.008 [block-manager-storage-async-thread-pool-96] INFO  org.apache.spark.storage.BlockManager - Removing RDD 33
13:22:12.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505726000 ms
13:22:12.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505726000 ms
13:22:12.056 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:12.057 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:12.057 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:12.057 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:12.058 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:12.058 [receiver-supervisor-future-49] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:12.058 [receiver-supervisor-future-50] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:12.060 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:12.060 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:12.060 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:12.061 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:12.061 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:14.064 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:14.065 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:14.066 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:14.066 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:14.067 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:14.067 [receiver-supervisor-future-50] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:14.067 [receiver-supervisor-future-51] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:14.070 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:14.070 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:14.070 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:14.071 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:14.072 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:15.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505735000 ms
13:22:15.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505735000 ms.0 from job set of time 1760505735000 ms
13:22:15.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505735000 ms.0 from job set of time 1760505735000 ms
13:22:15.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505735000 ms (execution: 0.002 s)
13:22:15.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 34 from persistence list
13:22:15.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[34] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505735000 ms
13:22:15.007 [block-manager-storage-async-thread-pool-99] INFO  org.apache.spark.storage.BlockManager - Removing RDD 34
13:22:15.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505729000 ms
13:22:15.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505729000 ms
13:22:16.077 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:16.078 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:16.078 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:16.079 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:16.080 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:16.080 [receiver-supervisor-future-51] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:16.080 [receiver-supervisor-future-52] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:16.082 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:16.082 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:16.082 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:16.082 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:16.082 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:18.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505738000 ms
13:22:18.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505738000 ms.0 from job set of time 1760505738000 ms
13:22:18.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505738000 ms.0 from job set of time 1760505738000 ms
13:22:18.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505738000 ms (execution: 0.001 s)
13:22:18.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 35 from persistence list
13:22:18.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[35] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505738000 ms
13:22:18.007 [block-manager-storage-async-thread-pool-102] INFO  org.apache.spark.storage.BlockManager - Removing RDD 35
13:22:18.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505732000 ms
13:22:18.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505732000 ms
13:22:18.086 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:18.088 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:18.088 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:18.088 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:18.089 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:18.090 [receiver-supervisor-future-52] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:18.090 [receiver-supervisor-future-53] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:18.092 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:18.092 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:18.092 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:18.093 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:18.093 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:20.096 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:20.097 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:20.098 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:20.098 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:20.099 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:20.099 [receiver-supervisor-future-53] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:20.099 [receiver-supervisor-future-54] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:20.101 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:20.101 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:20.101 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:20.101 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:20.102 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:21.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505741000 ms
13:22:21.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505741000 ms.0 from job set of time 1760505741000 ms
13:22:21.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505741000 ms.0 from job set of time 1760505741000 ms
13:22:21.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505741000 ms (execution: 0.001 s)
13:22:21.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 36 from persistence list
13:22:21.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[36] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505741000 ms
13:22:21.006 [block-manager-storage-async-thread-pool-105] INFO  org.apache.spark.storage.BlockManager - Removing RDD 36
13:22:21.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505735000 ms
13:22:21.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505735000 ms
13:22:22.104 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:22.105 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:22.105 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:22.106 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:22.107 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:22.107 [receiver-supervisor-future-54] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:22.107 [receiver-supervisor-future-55] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:22.109 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:22.109 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:22.109 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:22.109 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:22.110 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:24.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505744000 ms.0 from job set of time 1760505744000 ms
13:22:24.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505744000 ms
13:22:24.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505744000 ms.0 from job set of time 1760505744000 ms
13:22:24.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505744000 ms (execution: 0.001 s)
13:22:24.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 37 from persistence list
13:22:24.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[37] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505744000 ms
13:22:24.007 [block-manager-storage-async-thread-pool-108] INFO  org.apache.spark.storage.BlockManager - Removing RDD 37
13:22:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505738000 ms
13:22:24.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505738000 ms
13:22:24.113 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:24.114 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:24.115 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:24.115 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:24.116 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:24.116 [receiver-supervisor-future-55] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:24.116 [receiver-supervisor-future-56] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:24.118 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:24.118 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:24.118 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:24.118 [dispatcher-event-loop-2] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:24.118 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:26.124 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:26.125 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:26.125 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:26.125 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:26.127 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:26.127 [receiver-supervisor-future-56] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:26.127 [receiver-supervisor-future-57] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:26.128 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:26.129 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:26.129 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:26.129 [dispatcher-event-loop-4] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:26.129 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:27.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505747000 ms
13:22:27.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505747000 ms.0 from job set of time 1760505747000 ms
13:22:27.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505747000 ms.0 from job set of time 1760505747000 ms
13:22:27.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505747000 ms (execution: 0.001 s)
13:22:27.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 38 from persistence list
13:22:27.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[38] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505747000 ms
13:22:27.006 [block-manager-storage-async-thread-pool-111] INFO  org.apache.spark.storage.BlockManager - Removing RDD 38
13:22:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505741000 ms
13:22:27.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505741000 ms
13:22:28.134 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:28.134 [dispatcher-event-loop-5] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:28.134 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:28.134 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:28.135 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:28.135 [receiver-supervisor-future-57] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:28.135 [receiver-supervisor-future-58] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:28.136 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:28.136 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:28.136 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:28.136 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:28.136 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:30.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505750000 ms
13:22:30.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505750000 ms.0 from job set of time 1760505750000 ms
13:22:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505750000 ms.0 from job set of time 1760505750000 ms
13:22:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505750000 ms (execution: 0.002 s)
13:22:30.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 39 from persistence list
13:22:30.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[39] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505750000 ms
13:22:30.009 [block-manager-storage-async-thread-pool-114] INFO  org.apache.spark.storage.BlockManager - Removing RDD 39
13:22:30.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505744000 ms
13:22:30.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505744000 ms
13:22:30.141 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:30.143 [dispatcher-event-loop-7] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:30.143 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:30.143 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:30.144 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:30.144 [receiver-supervisor-future-58] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:30.144 [receiver-supervisor-future-59] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:30.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:30.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:30.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:30.146 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:30.146 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:32.150 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:32.151 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:32.151 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:32.151 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:32.152 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:32.153 [receiver-supervisor-future-59] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:32.153 [receiver-supervisor-future-60] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:32.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:32.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:32.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:32.154 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:32.154 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505753000 ms
13:22:33.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505753000 ms.0 from job set of time 1760505753000 ms
13:22:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505753000 ms.0 from job set of time 1760505753000 ms
13:22:33.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505753000 ms (execution: 0.001 s)
13:22:33.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 40 from persistence list
13:22:33.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[40] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505753000 ms
13:22:33.008 [block-manager-storage-async-thread-pool-117] INFO  org.apache.spark.storage.BlockManager - Removing RDD 40
13:22:33.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505747000 ms
13:22:33.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505747000 ms
13:22:34.159 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:34.161 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:34.161 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:34.161 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:34.162 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:34.162 [receiver-supervisor-future-60] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:34.162 [receiver-supervisor-future-61] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:34.164 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:34.164 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:34.164 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:34.164 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:34.164 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:36.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505756000 ms
13:22:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505756000 ms.0 from job set of time 1760505756000 ms
13:22:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505756000 ms.0 from job set of time 1760505756000 ms
13:22:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505756000 ms (execution: 0.002 s)
13:22:36.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 41 from persistence list
13:22:36.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[41] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505756000 ms
13:22:36.009 [block-manager-storage-async-thread-pool-120] INFO  org.apache.spark.storage.BlockManager - Removing RDD 41
13:22:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505750000 ms
13:22:36.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505750000 ms
13:22:36.169 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:36.170 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:36.170 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:36.170 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:36.170 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:36.170 [receiver-supervisor-future-61] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:36.170 [receiver-supervisor-future-62] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:36.171 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:36.171 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:36.171 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:36.171 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:36.171 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:38.176 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:38.177 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:38.178 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:38.178 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:38.179 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:38.179 [receiver-supervisor-future-62] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:38.179 [receiver-supervisor-future-63] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:38.181 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:38.181 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:38.181 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:38.181 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:38.181 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:39.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505759000 ms
13:22:39.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505759000 ms.0 from job set of time 1760505759000 ms
13:22:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505759000 ms.0 from job set of time 1760505759000 ms
13:22:39.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505759000 ms (execution: 0.001 s)
13:22:39.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 42 from persistence list
13:22:39.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[42] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505759000 ms
13:22:39.010 [block-manager-storage-async-thread-pool-123] INFO  org.apache.spark.storage.BlockManager - Removing RDD 42
13:22:39.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505753000 ms
13:22:39.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505753000 ms
13:22:40.185 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:40.186 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:40.186 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:40.186 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:40.187 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:40.187 [receiver-supervisor-future-63] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:40.187 [receiver-supervisor-future-64] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:40.188 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:40.188 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:40.188 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:40.189 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:40.189 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:42.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505762000 ms
13:22:42.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505762000 ms.0 from job set of time 1760505762000 ms
13:22:42.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505762000 ms.0 from job set of time 1760505762000 ms
13:22:42.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505762000 ms (execution: 0.001 s)
13:22:42.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 43 from persistence list
13:22:42.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[43] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505762000 ms
13:22:42.007 [block-manager-storage-async-thread-pool-126] INFO  org.apache.spark.storage.BlockManager - Removing RDD 43
13:22:42.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505756000 ms
13:22:42.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505756000 ms
13:22:42.193 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:42.194 [dispatcher-event-loop-4] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:42.195 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:42.195 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:42.196 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:42.196 [receiver-supervisor-future-64] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:42.196 [receiver-supervisor-future-65] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:42.198 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:42.198 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:42.198 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:42.198 [dispatcher-event-loop-5] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:42.198 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:44.201 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:44.202 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:44.203 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:44.203 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:44.204 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:44.204 [receiver-supervisor-future-65] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:44.204 [receiver-supervisor-future-66] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:44.206 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:44.206 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:44.206 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:44.206 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:44.207 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:45.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505765000 ms
13:22:45.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505765000 ms.0 from job set of time 1760505765000 ms
13:22:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505765000 ms.0 from job set of time 1760505765000 ms
13:22:45.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505765000 ms (execution: 0.001 s)
13:22:45.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 44 from persistence list
13:22:45.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[44] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505765000 ms
13:22:45.010 [block-manager-storage-async-thread-pool-129] INFO  org.apache.spark.storage.BlockManager - Removing RDD 44
13:22:45.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505759000 ms
13:22:45.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505759000 ms
13:22:46.212 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:46.213 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:46.213 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:46.213 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:46.214 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:46.214 [receiver-supervisor-future-66] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:46.214 [receiver-supervisor-future-67] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:46.216 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:46.216 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:46.216 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:46.216 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:46.217 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:48.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505768000 ms
13:22:48.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505768000 ms.0 from job set of time 1760505768000 ms
13:22:48.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505768000 ms.0 from job set of time 1760505768000 ms
13:22:48.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505768000 ms (execution: 0.001 s)
13:22:48.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 45 from persistence list
13:22:48.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[45] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505768000 ms
13:22:48.007 [block-manager-storage-async-thread-pool-132] INFO  org.apache.spark.storage.BlockManager - Removing RDD 45
13:22:48.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505762000 ms
13:22:48.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505762000 ms
13:22:48.218 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:48.219 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:48.220 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:48.220 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:48.221 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:48.221 [receiver-supervisor-future-67] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:48.221 [receiver-supervisor-future-68] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:48.222 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:48.222 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:48.222 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:48.223 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:48.223 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:50.228 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:50.229 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:50.229 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:50.229 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:50.230 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:50.230 [receiver-supervisor-future-68] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:50.230 [receiver-supervisor-future-69] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:50.232 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:50.232 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:50.232 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:50.232 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:50.232 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:51.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505771000 ms
13:22:51.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505771000 ms.0 from job set of time 1760505771000 ms
13:22:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505771000 ms.0 from job set of time 1760505771000 ms
13:22:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505771000 ms (execution: 0.001 s)
13:22:51.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 46 from persistence list
13:22:51.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[46] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505771000 ms
13:22:51.009 [block-manager-storage-async-thread-pool-135] INFO  org.apache.spark.storage.BlockManager - Removing RDD 46
13:22:51.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505765000 ms
13:22:51.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505765000 ms
13:22:52.236 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:52.237 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:52.237 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:52.238 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:52.239 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:52.239 [receiver-supervisor-future-69] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:52.239 [receiver-supervisor-future-70] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:52.240 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:52.240 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:52.241 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:52.241 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:52.241 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505774000 ms
13:22:54.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505774000 ms.0 from job set of time 1760505774000 ms
13:22:54.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505774000 ms.0 from job set of time 1760505774000 ms
13:22:54.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505774000 ms (execution: 0.001 s)
13:22:54.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 47 from persistence list
13:22:54.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[47] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505774000 ms
13:22:54.010 [block-manager-storage-async-thread-pool-138] INFO  org.apache.spark.storage.BlockManager - Removing RDD 47
13:22:54.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505768000 ms
13:22:54.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505768000 ms
13:22:54.243 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:54.244 [dispatcher-event-loop-8] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:54.244 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:54.244 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:54.245 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:54.245 [receiver-supervisor-future-70] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:54.245 [receiver-supervisor-future-71] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:54.246 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:54.246 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:54.246 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:54.246 [dispatcher-event-loop-9] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:54.247 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:56.252 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:56.253 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:56.254 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:56.254 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:56.255 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:56.255 [receiver-supervisor-future-71] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:56.255 [receiver-supervisor-future-72] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9999
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_452]
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_452]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_452]
	at java.net.Socket.connect(Socket.java:556) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:452) ~[?:1.8.0_452]
	at java.net.Socket.<init>(Socket.java:229) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]
	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_452]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_452]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_452]
13:22:56.257 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999: java.net.ConnectException: Connection refused (Connection refused)
13:22:56.257 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:22:56.257 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:22:56.257 [dispatcher-event-loop-1] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9999 - java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at java.net.Socket.connect(Socket.java:556)
	at java.net.Socket.<init>(Socket.java:452)
	at java.net.Socket.<init>(Socket.java:229)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

13:22:56.257 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:22:57.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505777000 ms
13:22:57.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505777000 ms.0 from job set of time 1760505777000 ms
13:22:57.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505777000 ms.0 from job set of time 1760505777000 ms
13:22:57.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505777000 ms (execution: 0.001 s)
13:22:57.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 48 from persistence list
13:22:57.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[48] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505777000 ms
13:22:57.006 [block-manager-storage-async-thread-pool-141] INFO  org.apache.spark.storage.BlockManager - Removing RDD 48
13:22:57.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505771000 ms
13:22:57.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505771000 ms
13:22:58.258 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver again
13:22:58.259 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49744
13:22:58.259 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:22:58.259 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:22:58.261 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connected to 127.0.0.1:9999
13:22:58.263 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:22:58.264 [receiver-supervisor-future-72] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver started again
13:22:58.412 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505778200 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:22:58.414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505778200 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:22:58.420 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:22:58.421 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505778200 replicated to only 0 peer(s) instead of 1 peers
13:22:58.427 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505778200
13:22:58.604 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505778400 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:22:58.605 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505778400 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:22:58.606 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:22:58.607 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505778400 replicated to only 0 peer(s) instead of 1 peers
13:22:58.607 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505778400
13:22:58.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505778600 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:22:58.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505778600 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:22:58.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:22:58.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505778600 replicated to only 0 peer(s) instead of 1 peers
13:22:58.806 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505778600
13:22:59.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505779000 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:22:59.205 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505779000 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:22:59.206 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:22:59.206 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505779000 replicated to only 0 peer(s) instead of 1 peers
13:22:59.207 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505779000
13:23:00.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505780000 ms
13:23:00.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505780000 ms.0 from job set of time 1760505780000 ms
13:23:00.030 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:00.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (print at sparkStreamingSocket.java:33) with 1 output partitions
13:23:00.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (print at sparkStreamingSocket.java:33)
13:23:00.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:00.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:00.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:00.040 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:00.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:00.042 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:00.043 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:23:00.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(0))
13:23:00.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
13:23:00.046 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:00.047 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
13:23:00.088 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505778200 locally
13:23:00.095 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 849 bytes result sent to driver
13:23:00.097 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 52 ms on 172.20.10.7 (executor driver) (1/1)
13:23:00.098 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
13:23:00.099 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (print at sparkStreamingSocket.java:33) finished in 0.061 s
13:23:00.100 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:00.100 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
13:23:00.100 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: print at sparkStreamingSocket.java:33, took 0.069261 s
13:23:00.104 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:00.104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (print at sparkStreamingSocket.java:33) with 3 output partitions
13:23:00.104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (print at sparkStreamingSocket.java:33)
13:23:00.104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:00.104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:00.105 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:00.106 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:00.107 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:00.108 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:00.108 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
13:23:00.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 2 (BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(1, 2, 3))
13:23:00.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 3 tasks resource profile 0
13:23:00.110 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:00.110 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 3) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:00.110 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 4) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:00.110 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
13:23:00.111 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 3)
13:23:00.110 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 4)
13:23:00.112 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505778400 locally
13:23:00.112 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505778600 locally
13:23:00.113 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 806 bytes result sent to driver
13:23:00.113 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 3 ms on 172.20.10.7 (executor driver) (1/3)
13:23:00.113 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505779000 locally
13:23:00.113 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 3). 849 bytes result sent to driver
13:23:00.113 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 4). 806 bytes result sent to driver
13:23:00.114 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 3) in 4 ms on 172.20.10.7 (executor driver) (2/3)
13:23:00.114 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 4) in 4 ms on 172.20.10.7 (executor driver) (3/3)
13:23:00.114 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13:23:00.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (print at sparkStreamingSocket.java:33) finished in 0.009 s
13:23:00.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:00.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
13:23:00.115 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: print at sparkStreamingSocket.java:33, took 0.010372 s
13:23:00.115 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505780000 ms.0 from job set of time 1760505780000 ms
13:23:00.115 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.115 s for time 1760505780000 ms (execution: 0.105 s)
13:23:00.115 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 49 from persistence list
13:23:00.115 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[49] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505780000 ms
13:23:00.115 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505774000 ms
13:23:00.115 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505774000 ms
13:23:00.115 [block-manager-storage-async-thread-pool-144] INFO  org.apache.spark.storage.BlockManager - Removing RDD 49
13:23:03.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505783000 ms
13:23:03.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505783000 ms.0 from job set of time 1760505783000 ms
13:23:03.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505783000 ms.0 from job set of time 1760505783000 ms
13:23:03.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505783000 ms (execution: 0.001 s)
13:23:03.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 50 from persistence list
13:23:03.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505783000 ms
13:23:03.009 [block-manager-storage-async-thread-pool-147] INFO  org.apache.spark.storage.BlockManager - Removing RDD 50
13:23:03.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505777000 ms
13:23:03.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505777000 ms
13:23:03.018 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505778200 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:03.019 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505778400 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:03.019 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505779000 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:03.019 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505778600 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:04.403 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505784200 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:04.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505784200 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:04.405 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:04.406 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505784200 replicated to only 0 peer(s) instead of 1 peers
13:23:04.406 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505784200
13:23:04.807 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505784600 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:04.809 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505784600 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:04.810 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:04.810 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505784600 replicated to only 0 peer(s) instead of 1 peers
13:23:04.811 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505784600
13:23:05.006 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505784800 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:05.007 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505784800 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:05.008 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:05.008 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505784800 replicated to only 0 peer(s) instead of 1 peers
13:23:05.009 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505784800
13:23:05.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505785000 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:05.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505785000 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:05.205 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:05.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505785000 replicated to only 0 peer(s) instead of 1 peers
13:23:05.206 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505785000
13:23:05.402 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505785200 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:05.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505785200 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:05.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:05.404 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505785200 replicated to only 0 peer(s) instead of 1 peers
13:23:05.405 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505785200
13:23:05.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505785400 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:05.604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505785400 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:05.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:05.605 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505785400 replicated to only 0 peer(s) instead of 1 peers
13:23:05.606 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505785400
13:23:05.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505785600 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:05.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505785600 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:05.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:05.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505785600 replicated to only 0 peer(s) instead of 1 peers
13:23:05.806 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505785600
13:23:06.017 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505785800 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:06.020 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505785800 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:06.021 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:06.021 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505785800 replicated to only 0 peer(s) instead of 1 peers
13:23:06.022 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505785800
13:23:06.023 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505786000 ms
13:23:06.024 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505786000 ms.0 from job set of time 1760505786000 ms
13:23:06.033 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.20.10.7:49745 in memory (size: 1713.0 B, free: 2004.6 MiB)
13:23:06.038 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.20.10.7:49745 in memory (size: 1713.0 B, free: 2004.6 MiB)
13:23:06.038 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:06.039 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (print at sparkStreamingSocket.java:33) with 1 output partitions
13:23:06.039 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (print at sparkStreamingSocket.java:33)
13:23:06.039 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:06.039 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:06.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:06.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:06.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:06.044 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:06.044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
13:23:06.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(0))
13:23:06.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
13:23:06.046 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 5) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.046 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 5)
13:23:06.048 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505784200 locally
13:23:06.049 [Executor task launch worker for task 0.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 5). 806 bytes result sent to driver
13:23:06.049 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 5) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:23:06.049 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
13:23:06.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (print at sparkStreamingSocket.java:33) finished in 0.008 s
13:23:06.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:06.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
13:23:06.050 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: print at sparkStreamingSocket.java:33, took 0.011254 s
13:23:06.054 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:06.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (print at sparkStreamingSocket.java:33) with 4 output partitions
13:23:06.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (print at sparkStreamingSocket.java:33)
13:23:06.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:06.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:06.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:06.058 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:06.059 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:06.059 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:06.059 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:23:06.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 4 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
13:23:06.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 4 tasks resource profile 0
13:23:06.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 6) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 7) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 8) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.061 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 9) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.061 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 6)
13:23:06.061 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 7)
13:23:06.061 [Executor task launch worker for task 2.0 in stage 4.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 8)
13:23:06.061 [Executor task launch worker for task 3.0 in stage 4.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 9)
13:23:06.062 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505784800 locally
13:23:06.062 [Executor task launch worker for task 3.0 in stage 4.0 (TID 9)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505785200 locally
13:23:06.062 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505784600 locally
13:23:06.062 [Executor task launch worker for task 2.0 in stage 4.0 (TID 8)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505785000 locally
13:23:06.063 [Executor task launch worker for task 1.0 in stage 4.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 7). 763 bytes result sent to driver
13:23:06.063 [Executor task launch worker for task 3.0 in stage 4.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 9). 763 bytes result sent to driver
13:23:06.063 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 6). 806 bytes result sent to driver
13:23:06.063 [Executor task launch worker for task 2.0 in stage 4.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 8). 806 bytes result sent to driver
13:23:06.063 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 7) in 3 ms on 172.20.10.7 (executor driver) (1/4)
13:23:06.064 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 9) in 4 ms on 172.20.10.7 (executor driver) (2/4)
13:23:06.064 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 6) in 4 ms on 172.20.10.7 (executor driver) (3/4)
13:23:06.064 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 8) in 4 ms on 172.20.10.7 (executor driver) (4/4)
13:23:06.064 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
13:23:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (print at sparkStreamingSocket.java:33) finished in 0.007 s
13:23:06.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:06.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
13:23:06.065 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: print at sparkStreamingSocket.java:33, took 0.010179 s
13:23:06.068 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (print at sparkStreamingSocket.java:33) with 2 output partitions
13:23:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (print at sparkStreamingSocket.java:33)
13:23:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:06.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:06.070 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:06.071 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:06.071 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:06.071 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
13:23:06.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 5 (BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(5, 6))
13:23:06.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 2 tasks resource profile 0
13:23:06.072 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 10) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.072 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 11) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:06.072 [Executor task launch worker for task 0.0 in stage 5.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 10)
13:23:06.072 [Executor task launch worker for task 1.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 11)
13:23:06.073 [Executor task launch worker for task 1.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505785600 locally
13:23:06.073 [Executor task launch worker for task 0.0 in stage 5.0 (TID 10)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505785400 locally
13:23:06.073 [Executor task launch worker for task 1.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 11). 763 bytes result sent to driver
13:23:06.074 [Executor task launch worker for task 0.0 in stage 5.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 10). 763 bytes result sent to driver
13:23:06.074 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 11) in 2 ms on 172.20.10.7 (executor driver) (1/2)
13:23:06.074 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 10) in 2 ms on 172.20.10.7 (executor driver) (2/2)
13:23:06.074 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
13:23:06.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (print at sparkStreamingSocket.java:33) finished in 0.004 s
13:23:06.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:06.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
13:23:06.074 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: print at sparkStreamingSocket.java:33, took 0.006200 s
13:23:06.075 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505786000 ms.0 from job set of time 1760505786000 ms
13:23:06.075 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.075 s for time 1760505786000 ms (execution: 0.051 s)
13:23:06.075 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 51 from persistence list
13:23:06.075 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[51] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505786000 ms
13:23:06.075 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505780000 ms
13:23:06.075 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505780000 ms
13:23:06.075 [block-manager-storage-async-thread-pool-168] INFO  org.apache.spark.storage.BlockManager - Removing RDD 51
13:23:06.402 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505786200 stored as values in memory (estimated size 12.0 B, free 2004.5 MiB)
13:23:06.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505786200 in memory on 172.20.10.7:49745 (size: 12.0 B, free: 2004.6 MiB)
13:23:06.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:06.404 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505786200 replicated to only 0 peer(s) instead of 1 peers
13:23:06.405 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505786200
13:23:06.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505786600 stored as values in memory (estimated size 9.0 B, free 2004.5 MiB)
13:23:06.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505786600 in memory on 172.20.10.7:49745 (size: 9.0 B, free: 2004.6 MiB)
13:23:06.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:06.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505786600 replicated to only 0 peer(s) instead of 1 peers
13:23:06.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505786600
13:23:07.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505786800 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:23:07.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505786800 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:07.003 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:07.003 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505786800 replicated to only 0 peer(s) instead of 1 peers
13:23:07.003 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505786800
13:23:09.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505789000 ms
13:23:09.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505789000 ms.0 from job set of time 1760505789000 ms
13:23:09.017 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:09.018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (print at sparkStreamingSocket.java:33) with 1 output partitions
13:23:09.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (print at sparkStreamingSocket.java:33)
13:23:09.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:09.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:09.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (BlockRDD[53] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:09.024 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 2.8 KiB, free 2004.5 MiB)
13:23:09.026 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.5 MiB)
13:23:09.027 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:09.027 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
13:23:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (BlockRDD[53] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(0))
13:23:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0
13:23:09.030 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 12) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:09.030 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 12)
13:23:09.033 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505785800 locally
13:23:09.035 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 12). 849 bytes result sent to driver
13:23:09.035 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 12) in 6 ms on 172.20.10.7 (executor driver) (1/1)
13:23:09.035 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
13:23:09.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (print at sparkStreamingSocket.java:33) finished in 0.014 s
13:23:09.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:09.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 6: Stage finished
13:23:09.037 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: print at sparkStreamingSocket.java:33, took 0.019284 s
13:23:09.046 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (print at sparkStreamingSocket.java:33) with 3 output partitions
13:23:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (print at sparkStreamingSocket.java:33)
13:23:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (BlockRDD[53] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:09.049 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 2.8 KiB, free 2004.4 MiB)
13:23:09.050 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.4 MiB)
13:23:09.050 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:09.050 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
13:23:09.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 7 (BlockRDD[53] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(1, 2, 3))
13:23:09.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 3 tasks resource profile 0
13:23:09.051 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 13) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:09.052 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 14) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:09.052 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 15) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:09.052 [Executor task launch worker for task 1.0 in stage 7.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 14)
13:23:09.052 [Executor task launch worker for task 0.0 in stage 7.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 13)
13:23:09.052 [Executor task launch worker for task 2.0 in stage 7.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 15)
13:23:09.054 [Executor task launch worker for task 1.0 in stage 7.0 (TID 14)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505786600 locally
13:23:09.054 [Executor task launch worker for task 0.0 in stage 7.0 (TID 13)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505786200 locally
13:23:09.054 [Executor task launch worker for task 1.0 in stage 7.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 14). 764 bytes result sent to driver
13:23:09.054 [Executor task launch worker for task 0.0 in stage 7.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 13). 810 bytes result sent to driver
13:23:09.054 [Executor task launch worker for task 2.0 in stage 7.0 (TID 15)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505786800 locally
13:23:09.054 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 14) in 3 ms on 172.20.10.7 (executor driver) (1/3)
13:23:09.055 [Executor task launch worker for task 2.0 in stage 7.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 15). 806 bytes result sent to driver
13:23:09.055 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 13) in 4 ms on 172.20.10.7 (executor driver) (2/3)
13:23:09.055 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 15) in 3 ms on 172.20.10.7 (executor driver) (3/3)
13:23:09.055 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
13:23:09.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (print at sparkStreamingSocket.java:33) finished in 0.007 s
13:23:09.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:09.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
13:23:09.056 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: print at sparkStreamingSocket.java:33, took 0.009285 s
13:23:09.056 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505789000 ms.0 from job set of time 1760505789000 ms
13:23:09.056 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.056 s for time 1760505789000 ms (execution: 0.052 s)
13:23:09.056 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 52 from persistence list
13:23:09.056 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[52] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505789000 ms
13:23:09.056 [block-manager-storage-async-thread-pool-171] INFO  org.apache.spark.storage.BlockManager - Removing RDD 52
13:23:09.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505784200 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505784600 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505784800 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505785000 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.059 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505785200 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.059 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505783000 ms
13:23:09.059 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505783000 ms
13:23:09.060 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505785400 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:09.060 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505785600 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:12.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505792000 ms
13:23:12.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505792000 ms.0 from job set of time 1760505792000 ms
13:23:12.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505792000 ms.0 from job set of time 1760505792000 ms
13:23:12.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.006 s for time 1760505792000 ms (execution: 0.001 s)
13:23:12.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 53 from persistence list
13:23:12.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[53] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505792000 ms
13:23:12.008 [block-manager-storage-async-thread-pool-195] INFO  org.apache.spark.storage.BlockManager - Removing RDD 53
13:23:12.011 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505786200 on 172.20.10.7:49745 in memory (size: 12.0 B, free: 2004.6 MiB)
13:23:12.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505786000 ms
13:23:12.013 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505785800 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:12.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505786000 ms
13:23:12.013 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505786600 on 172.20.10.7:49745 in memory (size: 9.0 B, free: 2004.6 MiB)
13:23:12.014 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505786800 on 172.20.10.7:49745 in memory (size: 8.0 B, free: 2004.6 MiB)
13:23:15.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505795000 ms
13:23:15.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505795000 ms.0 from job set of time 1760505795000 ms
13:23:15.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505795000 ms.0 from job set of time 1760505795000 ms
13:23:15.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505795000 ms (execution: 0.001 s)
13:23:15.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 54 from persistence list
13:23:15.006 [block-manager-storage-async-thread-pool-108] INFO  org.apache.spark.storage.BlockManager - Removing RDD 54
13:23:15.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[54] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505795000 ms
13:23:15.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505789000 ms
13:23:15.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505789000 ms
13:23:18.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505798000 ms
13:23:18.002 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505798000 ms.0 from job set of time 1760505798000 ms
13:23:18.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505798000 ms.0 from job set of time 1760505798000 ms
13:23:18.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.003 s for time 1760505798000 ms (execution: 0.001 s)
13:23:18.004 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 55 from persistence list
13:23:18.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[55] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505798000 ms
13:23:18.005 [block-manager-storage-async-thread-pool-197] INFO  org.apache.spark.storage.BlockManager - Removing RDD 55
13:23:18.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505792000 ms
13:23:18.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505792000 ms
13:23:21.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505801000 ms
13:23:21.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505801000 ms.0 from job set of time 1760505801000 ms
13:23:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505801000 ms.0 from job set of time 1760505801000 ms
13:23:21.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505801000 ms (execution: 0.000 s)
13:23:21.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 56 from persistence list
13:23:21.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[56] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505801000 ms
13:23:21.009 [block-manager-storage-async-thread-pool-199] INFO  org.apache.spark.storage.BlockManager - Removing RDD 56
13:23:21.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505795000 ms
13:23:21.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505795000 ms
13:23:24.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505804000 ms
13:23:24.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505804000 ms.0 from job set of time 1760505804000 ms
13:23:24.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505804000 ms.0 from job set of time 1760505804000 ms
13:23:24.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505804000 ms (execution: 0.000 s)
13:23:24.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 57 from persistence list
13:23:24.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[57] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505804000 ms
13:23:24.009 [block-manager-storage-async-thread-pool-202] INFO  org.apache.spark.storage.BlockManager - Removing RDD 57
13:23:24.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505798000 ms
13:23:24.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505798000 ms
13:23:27.002 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505807000 ms
13:23:27.003 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505807000 ms.0 from job set of time 1760505807000 ms
13:23:27.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505807000 ms.0 from job set of time 1760505807000 ms
13:23:27.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505807000 ms (execution: 0.001 s)
13:23:27.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 58 from persistence list
13:23:27.006 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[58] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505807000 ms
13:23:27.006 [block-manager-storage-async-thread-pool-205] INFO  org.apache.spark.storage.BlockManager - Removing RDD 58
13:23:27.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505801000 ms
13:23:27.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505801000 ms
13:23:30.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505810000 ms
13:23:30.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505810000 ms.0 from job set of time 1760505810000 ms
13:23:30.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505810000 ms.0 from job set of time 1760505810000 ms
13:23:30.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505810000 ms (execution: 0.000 s)
13:23:30.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 59 from persistence list
13:23:30.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[59] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505810000 ms
13:23:30.009 [block-manager-storage-async-thread-pool-207] INFO  org.apache.spark.storage.BlockManager - Removing RDD 59
13:23:30.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505804000 ms
13:23:30.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505804000 ms
13:23:33.003 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505813000 ms
13:23:33.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505813000 ms.0 from job set of time 1760505813000 ms
13:23:33.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505813000 ms.0 from job set of time 1760505813000 ms
13:23:33.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.004 s for time 1760505813000 ms (execution: 0.000 s)
13:23:33.005 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 60 from persistence list
13:23:33.005 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[60] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505813000 ms
13:23:33.005 [block-manager-storage-async-thread-pool-210] INFO  org.apache.spark.storage.BlockManager - Removing RDD 60
13:23:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505807000 ms
13:23:33.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505807000 ms
13:23:36.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505816000 ms
13:23:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505816000 ms.0 from job set of time 1760505816000 ms
13:23:36.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505816000 ms.0 from job set of time 1760505816000 ms
13:23:36.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505816000 ms (execution: 0.001 s)
13:23:36.008 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 61 from persistence list
13:23:36.009 [block-manager-storage-async-thread-pool-213] INFO  org.apache.spark.storage.BlockManager - Removing RDD 61
13:23:36.009 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[61] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505816000 ms
13:23:36.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505810000 ms
13:23:36.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505810000 ms
13:23:39.006 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505819000 ms
13:23:39.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505819000 ms.0 from job set of time 1760505819000 ms
13:23:39.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505819000 ms.0 from job set of time 1760505819000 ms
13:23:39.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505819000 ms (execution: 0.001 s)
13:23:39.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 62 from persistence list
13:23:39.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[62] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505819000 ms
13:23:39.008 [block-manager-storage-async-thread-pool-216] INFO  org.apache.spark.storage.BlockManager - Removing RDD 62
13:23:39.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505813000 ms
13:23:39.008 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505813000 ms
13:23:42.005 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505822000 ms
13:23:42.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505822000 ms.0 from job set of time 1760505822000 ms
13:23:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505822000 ms.0 from job set of time 1760505822000 ms
13:23:42.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.007 s for time 1760505822000 ms (execution: 0.002 s)
13:23:42.007 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 63 from persistence list
13:23:42.008 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[63] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505822000 ms
13:23:42.008 [block-manager-storage-async-thread-pool-219] INFO  org.apache.spark.storage.BlockManager - Removing RDD 63
13:23:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505816000 ms
13:23:42.009 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505816000 ms
13:23:45.004 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505825000 ms
13:23:45.004 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505825000 ms.0 from job set of time 1760505825000 ms
13:23:45.005 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505825000 ms.0 from job set of time 1760505825000 ms
13:23:45.006 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.005 s for time 1760505825000 ms (execution: 0.001 s)
13:23:45.006 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 64 from persistence list
13:23:45.007 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[64] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505825000 ms
13:23:45.007 [block-manager-storage-async-thread-pool-222] INFO  org.apache.spark.storage.BlockManager - Removing RDD 64
13:23:45.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505819000 ms
13:23:45.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505819000 ms
13:23:48.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505828000 ms
13:23:48.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505828000 ms.0 from job set of time 1760505828000 ms
13:23:48.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505828000 ms.0 from job set of time 1760505828000 ms
13:23:48.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505828000 ms (execution: 0.001 s)
13:23:48.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 65 from persistence list
13:23:48.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[65] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505828000 ms
13:23:48.010 [block-manager-storage-async-thread-pool-225] INFO  org.apache.spark.storage.BlockManager - Removing RDD 65
13:23:48.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505822000 ms
13:23:48.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505822000 ms
13:23:51.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505831000 ms
13:23:51.007 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505831000 ms.0 from job set of time 1760505831000 ms
13:23:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505831000 ms.0 from job set of time 1760505831000 ms
13:23:51.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.008 s for time 1760505831000 ms (execution: 0.001 s)
13:23:51.009 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 66 from persistence list
13:23:51.010 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[66] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505831000 ms
13:23:51.010 [block-manager-storage-async-thread-pool-228] INFO  org.apache.spark.storage.BlockManager - Removing RDD 66
13:23:51.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505825000 ms
13:23:51.010 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505825000 ms
13:23:54.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505834000 ms
13:23:54.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505834000 ms.0 from job set of time 1760505834000 ms
13:23:54.009 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505834000 ms.0 from job set of time 1760505834000 ms
13:23:54.010 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.009 s for time 1760505834000 ms (execution: 0.002 s)
13:23:54.010 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 67 from persistence list
13:23:54.011 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[67] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505834000 ms
13:23:54.011 [block-manager-storage-async-thread-pool-231] INFO  org.apache.spark.storage.BlockManager - Removing RDD 67
13:23:54.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505828000 ms
13:23:54.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505828000 ms
13:23:55.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505835600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:23:55.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505835600 in memory on 172.20.10.7:49745 (size: 8.0 B, free: 2004.6 MiB)
13:23:55.803 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:55.804 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505835600 replicated to only 0 peer(s) instead of 1 peers
13:23:55.804 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505835600
13:23:56.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505835800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:23:56.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505835800 in memory on 172.20.10.7:49745 (size: 7.0 B, free: 2004.6 MiB)
13:23:56.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:56.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505835800 replicated to only 0 peer(s) instead of 1 peers
13:23:56.005 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505835800
13:23:56.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505836000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:23:56.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505836000 in memory on 172.20.10.7:49745 (size: 7.0 B, free: 2004.6 MiB)
13:23:56.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:56.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505836000 replicated to only 0 peer(s) instead of 1 peers
13:23:56.205 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505836000
13:23:56.401 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505836200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:23:56.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505836200 in memory on 172.20.10.7:49745 (size: 7.0 B, free: 2004.6 MiB)
13:23:56.403 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:23:56.403 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505836200 replicated to only 0 peer(s) instead of 1 peers
13:23:56.404 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505836200
13:23:57.007 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505837000 ms
13:23:57.008 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505837000 ms.0 from job set of time 1760505837000 ms
13:23:57.023 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:57.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (print at sparkStreamingSocket.java:33) with 1 output partitions
13:23:57.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (print at sparkStreamingSocket.java:33)
13:23:57.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:57.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:57.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (BlockRDD[69] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:57.030 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 2.8 KiB, free 2004.4 MiB)
13:23:57.032 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.4 MiB)
13:23:57.033 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:57.034 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1513
13:23:57.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (BlockRDD[69] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(0))
13:23:57.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0
13:23:57.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 16) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:57.036 [Executor task launch worker for task 0.0 in stage 8.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 16)
13:23:57.039 [Executor task launch worker for task 0.0 in stage 8.0 (TID 16)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505835600 locally
13:23:57.040 [Executor task launch worker for task 0.0 in stage 8.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 16). 849 bytes result sent to driver
13:23:57.040 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 16) in 4 ms on 172.20.10.7 (executor driver) (1/1)
13:23:57.040 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
13:23:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (print at sparkStreamingSocket.java:33) finished in 0.012 s
13:23:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished
13:23:57.041 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: print at sparkStreamingSocket.java:33, took 0.017299 s
13:23:57.049 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: print at sparkStreamingSocket.java:33
13:23:57.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 (print at sparkStreamingSocket.java:33) with 3 output partitions
13:23:57.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (print at sparkStreamingSocket.java:33)
13:23:57.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:23:57.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:23:57.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (BlockRDD[69] at socketTextStream at sparkStreamingSocket.java:31), which has no missing parents
13:23:57.052 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 2.8 KiB, free 2004.4 MiB)
13:23:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 1713.0 B, free 2004.4 MiB)
13:23:57.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.20.10.7:49745 (size: 1713.0 B, free: 2004.6 MiB)
13:23:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1513
13:23:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (BlockRDD[69] at socketTextStream at sparkStreamingSocket.java:31) (first 15 tasks are for partitions Vector(1, 2, 3))
13:23:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks resource profile 0
13:23:57.054 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 17) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:57.054 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 18) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:57.054 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 19) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4394 bytes) taskResourceAssignments Map()
13:23:57.055 [Executor task launch worker for task 0.0 in stage 9.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 17)
13:23:57.055 [Executor task launch worker for task 1.0 in stage 9.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 18)
13:23:57.055 [Executor task launch worker for task 2.0 in stage 9.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 19)
13:23:57.055 [Executor task launch worker for task 2.0 in stage 9.0 (TID 19)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505836200 locally
13:23:57.055 [Executor task launch worker for task 1.0 in stage 9.0 (TID 18)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505836000 locally
13:23:57.056 [Executor task launch worker for task 0.0 in stage 9.0 (TID 17)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505835800 locally
13:23:57.056 [Executor task launch worker for task 1.0 in stage 9.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 18). 762 bytes result sent to driver
13:23:57.056 [Executor task launch worker for task 2.0 in stage 9.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 19). 762 bytes result sent to driver
13:23:57.056 [Executor task launch worker for task 0.0 in stage 9.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 17). 762 bytes result sent to driver
13:23:57.057 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 19) in 3 ms on 172.20.10.7 (executor driver) (1/3)
13:23:57.057 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 17) in 3 ms on 172.20.10.7 (executor driver) (2/3)
13:23:57.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 18) in 3 ms on 172.20.10.7 (executor driver) (3/3)
13:23:57.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
13:23:57.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (print at sparkStreamingSocket.java:33) finished in 0.007 s
13:23:57.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
13:23:57.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 9: Stage finished
13:23:57.058 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: print at sparkStreamingSocket.java:33, took 0.008321 s
13:23:57.058 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505837000 ms.0 from job set of time 1760505837000 ms
13:23:57.058 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.058 s for time 1760505837000 ms (execution: 0.051 s)
13:23:57.058 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 68 from persistence list
13:23:57.059 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[68] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505837000 ms
13:23:57.059 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505831000 ms
13:23:57.059 [block-manager-storage-async-thread-pool-234] INFO  org.apache.spark.storage.BlockManager - Removing RDD 68
13:23:57.059 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505831000 ms
13:23:59.074 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:23:59.076 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:23:59.076 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:23:59.076 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:23:59.076 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Closed socket to 127.0.0.1:9999
13:23:59.076 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:23:59.076 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:23:59.076 [dispatcher-event-loop-6] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
13:23:59.076 [Socket Receiver] WARN  org.apache.spark.streaming.dstream.SocketReceiver - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:23:59.077 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:23:59.077 [receiver-supervisor-future-73] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:23:59.077 [receiver-supervisor-future-73] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
13:23:59.077 [receiver-supervisor-future-73] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:23:59.077 [dispatcher-event-loop-3] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:25:00.733 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:25:00.760 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:25:00.795 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:25:00.795 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:25:00.795 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:25:00.795 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:25:00.804 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:25:00.809 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:25:00.809 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:25:00.832 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:25:00.832 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:25:00.832 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:25:00.833 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:25:00.833 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:25:00.935 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49874.
13:25:00.948 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:25:00.963 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:25:00.971 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:25:00.971 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:25:00.973 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:25:00.985 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-1efbc8c7-2a5c-44f1-a713-4aff761a3885
13:25:01.005 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:25:01.010 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:25:01.028 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5984ms to org.sparkproject.jetty.util.log.Slf4jLog
13:25:01.074 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:25:01.081 [main] INFO  org.sparkproject.jetty.server.Server - Started @6037ms
13:25:01.096 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@5328a9c1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:25:01.096 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:25:01.104 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30af7377{/,null,AVAILABLE,@Spark}
13:25:01.139 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:25:01.143 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:25:01.154 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49875.
13:25:01.155 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49875
13:25:01.156 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:25:01.158 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49875, None)
13:25:01.160 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49875 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49875, None)
13:25:01.163 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49875, None)
13:25:01.164 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49875, None)
13:25:01.259 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@30af7377{/,null,STOPPED,@Spark}
13:25:01.259 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75798d03{/jobs,null,AVAILABLE,@Spark}
13:25:01.260 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ffcf674{/jobs/json,null,AVAILABLE,@Spark}
13:25:01.260 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a55f148{/jobs/job,null,AVAILABLE,@Spark}
13:25:01.261 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae2ed38{/jobs/job/json,null,AVAILABLE,@Spark}
13:25:01.261 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/stages,null,AVAILABLE,@Spark}
13:25:01.261 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12b5454f{/stages/json,null,AVAILABLE,@Spark}
13:25:01.262 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/stages/stage,null,AVAILABLE,@Spark}
13:25:01.262 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/stages/stage/json,null,AVAILABLE,@Spark}
13:25:01.262 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/stages/pool,null,AVAILABLE,@Spark}
13:25:01.263 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/stages/pool/json,null,AVAILABLE,@Spark}
13:25:01.263 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/storage,null,AVAILABLE,@Spark}
13:25:01.263 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/storage/json,null,AVAILABLE,@Spark}
13:25:01.264 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/storage/rdd,null,AVAILABLE,@Spark}
13:25:01.265 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/storage/rdd/json,null,AVAILABLE,@Spark}
13:25:01.265 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/environment,null,AVAILABLE,@Spark}
13:25:01.266 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/environment/json,null,AVAILABLE,@Spark}
13:25:01.266 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/executors,null,AVAILABLE,@Spark}
13:25:01.267 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/executors/json,null,AVAILABLE,@Spark}
13:25:01.267 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/executors/threadDump,null,AVAILABLE,@Spark}
13:25:01.268 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:25:01.272 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30d25c03{/static,null,AVAILABLE,@Spark}
13:25:01.272 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d96250{/,null,AVAILABLE,@Spark}
13:25:01.273 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c1fa7d4{/api,null,AVAILABLE,@Spark}
13:25:01.273 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a480135{/jobs/job/kill,null,AVAILABLE,@Spark}
13:25:01.274 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@681e144{/stages/stage/kill,null,AVAILABLE,@Spark}
13:25:01.276 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@745c2004{/metrics/json,null,AVAILABLE,@Spark}
13:25:01.456 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:25:01.456 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:25:01.457 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:25:01.457 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:25:01.457 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@10b659cb
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Slide time = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Storage level = Serialized 1x Replicated
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Checkpoint interval = null
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Remember interval = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@5ba6d227
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Slide time = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@356edfd7
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@1fe2125c
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:25:01.458 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:25:01.459 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:25:01.459 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:25:01.459 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3db8786a
13:25:01.479 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760505903000
13:25:01.479 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760505903000 ms
13:25:01.480 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:25:01.481 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/streaming,null,AVAILABLE,@Spark}
13:25:01.481 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/streaming/json,null,AVAILABLE,@Spark}
13:25:01.481 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/streaming/batch,null,AVAILABLE,@Spark}
13:25:01.482 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/streaming/batch/json,null,AVAILABLE,@Spark}
13:25:01.482 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/static/streaming,null,AVAILABLE,@Spark}
13:25:01.482 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:25:01.506 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:25:01.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:67) with 1 output partitions
13:25:01.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:67)
13:25:01.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:25:01.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:01.511 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:25:01.557 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:25:01.910 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:25:01.912 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49875 (size: 35.5 KiB, free: 2004.6 MiB)
13:25:01.914 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:25:01.922 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:25:01.922 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:25:01.944 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:25:01.951 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:25:02.071 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760505902200
13:25:02.071 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:25:02.072 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:25:02.074 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49874
13:25:02.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:25:02.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:25:02.074 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connected to 127.0.0.1:9999
13:25:02.075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:25:02.075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:25:02.806 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505902600 stored as values in memory (estimated size 7.0 B, free 2004.5 MiB)
13:25:02.806 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505902600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:02.810 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:02.810 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505902600 replicated to only 0 peer(s) instead of 1 peers
13:25:02.812 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505902600
13:25:03.006 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505902800 stored as values in memory (estimated size 7.0 B, free 2004.5 MiB)
13:25:03.008 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505902800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:03.009 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:03.009 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505902800 replicated to only 0 peer(s) instead of 1 peers
13:25:03.010 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505902800
13:25:03.046 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505903000 ms
13:25:03.048 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505903000 ms.0 from job set of time 1760505903000 ms
13:25:03.074 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:03.083 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 0
13:25:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (sortByKey at sparkStreamingSocket.java:61)
13:25:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
13:25:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
13:25:03.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:03.090 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 6.2 KiB, free 2004.5 MiB)
13:25:03.091 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.5 MiB)
13:25:03.092 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.6 MiB)
13:25:03.092 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:25:03.093 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0))
13:25:03.093 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
13:25:03.094 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:03.095 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
13:25:03.143 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505902600 locally
13:25:03.163 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1220 bytes result sent to driver
13:25:03.165 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 72 ms on 172.20.10.7 (executor driver) (1/1)
13:25:03.166 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
13:25:03.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at sparkStreamingSocket.java:49) finished in 0.081 s
13:25:03.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:03.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:03.169 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
13:25:03.169 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:03.171 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:03.176 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:25:03.176 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:03.177 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.6 MiB)
13:25:03.177 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
13:25:03.178 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:03.178 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 10 tasks resource profile 0
13:25:03.179 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.179 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 3) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.180 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 4) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.180 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 5) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.180 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 6) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.180 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 7) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.180 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 8) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.181 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 9) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.181 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 10) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.181 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
13:25:03.181 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 3)
13:25:03.182 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 4)
13:25:03.182 [Executor task launch worker for task 3.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 5)
13:25:03.182 [Executor task launch worker for task 4.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 6)
13:25:03.182 [Executor task launch worker for task 5.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 7)
13:25:03.184 [Executor task launch worker for task 7.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 9)
13:25:03.184 [Executor task launch worker for task 6.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 8)
13:25:03.184 [Executor task launch worker for task 8.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 10)
13:25:03.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505903000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:03.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505903000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:03.205 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:03.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505903000 replicated to only 0 peer(s) instead of 1 peers
13:25:03.205 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505903000
13:25:03.210 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 6.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 8.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 5.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 4.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.210 [Executor task launch worker for task 3.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.211 [Executor task launch worker for task 7.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.211 [Executor task launch worker for task 5.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 6.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 7.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 8.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 4.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.211 [Executor task launch worker for task 3.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:03.212 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.212 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
13:25:03.219 [Executor task launch worker for task 3.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 5). 1517 bytes result sent to driver
13:25:03.219 [Executor task launch worker for task 4.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 6). 1517 bytes result sent to driver
13:25:03.219 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 3). 1517 bytes result sent to driver
13:25:03.219 [Executor task launch worker for task 2.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 4). 1517 bytes result sent to driver
13:25:03.219 [Executor task launch worker for task 5.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 7). 1517 bytes result sent to driver
13:25:03.219 [Executor task launch worker for task 8.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 10). 1517 bytes result sent to driver
13:25:03.220 [Executor task launch worker for task 6.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 8). 1517 bytes result sent to driver
13:25:03.220 [Executor task launch worker for task 7.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 9). 1517 bytes result sent to driver
13:25:03.220 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 11) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.221 [Executor task launch worker for task 9.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 11)
13:25:03.222 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1520 bytes result sent to driver
13:25:03.222 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 5) in 42 ms on 172.20.10.7 (executor driver) (1/10)
13:25:03.224 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 6) in 44 ms on 172.20.10.7 (executor driver) (2/10)
13:25:03.224 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 3) in 45 ms on 172.20.10.7 (executor driver) (3/10)
13:25:03.225 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 7) in 45 ms on 172.20.10.7 (executor driver) (4/10)
13:25:03.225 [Executor task launch worker for task 9.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.226 [Executor task launch worker for task 9.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.226 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 4) in 46 ms on 172.20.10.7 (executor driver) (5/10)
13:25:03.227 [Executor task launch worker for task 9.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 11). 1517 bytes result sent to driver
13:25:03.227 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 9) in 46 ms on 172.20.10.7 (executor driver) (6/10)
13:25:03.227 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 8) in 47 ms on 172.20.10.7 (executor driver) (7/10)
13:25:03.228 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 48 ms on 172.20.10.7 (executor driver) (8/10)
13:25:03.228 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 10) in 47 ms on 172.20.10.7 (executor driver) (9/10)
13:25:03.228 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 11) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:03.228 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13:25:03.230 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (sortByKey at sparkStreamingSocket.java:61) finished in 0.056 s
13:25:03.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:03.232 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
13:25:03.232 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: sortByKey at sparkStreamingSocket.java:61, took 0.157887 s
13:25:03.241 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:03.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 1
13:25:03.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:03.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at sparkStreamingSocket.java:61)
13:25:03.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
13:25:03.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
13:25:03.243 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:03.245 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:03.246 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:03.246 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.6 MiB)
13:25:03.246 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
13:25:03.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:03.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 10 tasks resource profile 0
13:25:03.248 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 12) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.248 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 13) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.248 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 14) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 15) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 16) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 17) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 4.0 (TID 18) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 4.0 (TID 19) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.249 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 4.0 (TID 20) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.250 [Executor task launch worker for task 0.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 12)
13:25:03.250 [Executor task launch worker for task 1.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 13)
13:25:03.250 [Executor task launch worker for task 2.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 14)
13:25:03.250 [Executor task launch worker for task 5.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 4.0 (TID 17)
13:25:03.250 [Executor task launch worker for task 4.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 4.0 (TID 16)
13:25:03.250 [Executor task launch worker for task 7.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 4.0 (TID 19)
13:25:03.250 [Executor task launch worker for task 6.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 4.0 (TID 18)
13:25:03.250 [Executor task launch worker for task 3.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 15)
13:25:03.250 [Executor task launch worker for task 8.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 4.0 (TID 20)
13:25:03.257 [Executor task launch worker for task 0.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 4.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 2.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 1.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 8.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 3.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.257 [Executor task launch worker for task 7.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.258 [Executor task launch worker for task 0.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 2.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:03.258 [Executor task launch worker for task 8.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 3.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 6.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.258 [Executor task launch worker for task 4.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 6.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:03.258 [Executor task launch worker for task 7.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 1.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.258 [Executor task launch worker for task 5.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.258 [Executor task launch worker for task 5.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.261 [Executor task launch worker for task 1.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 13). 1340 bytes result sent to driver
13:25:03.262 [Executor task launch worker for task 3.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 15). 1340 bytes result sent to driver
13:25:03.262 [Executor task launch worker for task 2.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 14). 1340 bytes result sent to driver
13:25:03.263 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 4.0 (TID 21) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:03.263 [Executor task launch worker for task 7.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 4.0 (TID 19). 1340 bytes result sent to driver
13:25:03.263 [Executor task launch worker for task 8.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 4.0 (TID 20). 1340 bytes result sent to driver
13:25:03.263 [Executor task launch worker for task 9.0 in stage 4.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 4.0 (TID 21)
13:25:03.264 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 13) in 16 ms on 172.20.10.7 (executor driver) (1/10)
13:25:03.264 [Executor task launch worker for task 6.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 4.0 (TID 18). 1340 bytes result sent to driver
13:25:03.264 [Executor task launch worker for task 5.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 4.0 (TID 17). 1340 bytes result sent to driver
13:25:03.265 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 15) in 17 ms on 172.20.10.7 (executor driver) (2/10)
13:25:03.265 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 4.0 (TID 19) in 16 ms on 172.20.10.7 (executor driver) (3/10)
13:25:03.265 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 4.0 (TID 20) in 16 ms on 172.20.10.7 (executor driver) (4/10)
13:25:03.266 [Executor task launch worker for task 9.0 in stage 4.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.266 [Executor task launch worker for task 9.0 in stage 4.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:03.266 [Executor task launch worker for task 4.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 4.0 (TID 16). 1340 bytes result sent to driver
13:25:03.266 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 14) in 18 ms on 172.20.10.7 (executor driver) (5/10)
13:25:03.267 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 16) in 18 ms on 172.20.10.7 (executor driver) (6/10)
13:25:03.267 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 4.0 (TID 18) in 18 ms on 172.20.10.7 (executor driver) (7/10)
13:25:03.267 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 17) in 18 ms on 172.20.10.7 (executor driver) (8/10)
13:25:03.267 [Executor task launch worker for task 9.0 in stage 4.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 4.0 (TID 21). 1340 bytes result sent to driver
13:25:03.269 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 4.0 (TID 21) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:25:03.271 [Executor task launch worker for task 0.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 12). 1469 bytes result sent to driver
13:25:03.272 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 12) in 24 ms on 172.20.10.7 (executor driver) (10/10)
13:25:03.272 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
13:25:03.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.028 s
13:25:03.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:03.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:03.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
13:25:03.272 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:03.273 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:03.274 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:03.275 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:03.275 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.6 MiB)
13:25:03.275 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:25:03.276 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:03.276 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
13:25:03.276 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 22) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:03.277 [Executor task launch worker for task 0.0 in stage 5.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 22)
13:25:03.279 [Executor task launch worker for task 0.0 in stage 5.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:03.279 [Executor task launch worker for task 0.0 in stage 5.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:03.286 [Executor task launch worker for task 0.0 in stage 5.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 22). 1373 bytes result sent to driver
13:25:03.287 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 22) in 11 ms on 172.20.10.7 (executor driver) (1/1)
13:25:03.287 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
13:25:03.288 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at sparkStreamingSocket.java:61) finished in 0.015 s
13:25:03.288 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:03.288 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
13:25:03.289 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at sparkStreamingSocket.java:61, took 0.047847 s
13:25:03.290 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505903000 ms.0 from job set of time 1760505903000 ms
13:25:03.291 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.290 s for time 1760505903000 ms (execution: 0.243 s)
13:25:03.292 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:25:03.294 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:25:04.807 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505904600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:04.808 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505904600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:04.809 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:04.809 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505904600 replicated to only 0 peer(s) instead of 1 peers
13:25:04.810 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505904600
13:25:05.006 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505904800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:05.007 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505904800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:05.008 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:05.008 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505904800 replicated to only 0 peer(s) instead of 1 peers
13:25:05.009 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505904800
13:25:05.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505905000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:05.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505905000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:05.205 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:05.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505905000 replicated to only 0 peer(s) instead of 1 peers
13:25:05.207 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505905000
13:25:05.405 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505905200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:05.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505905200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:05.408 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:05.408 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505905200 replicated to only 0 peer(s) instead of 1 peers
13:25:05.409 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505905200
13:25:05.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505905400 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:05.605 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505905400 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:05.606 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:05.606 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505905400 replicated to only 0 peer(s) instead of 1 peers
13:25:05.607 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505905400
13:25:05.804 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505905600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:05.805 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505905600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:05.806 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:05.806 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505905600 replicated to only 0 peer(s) instead of 1 peers
13:25:05.807 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505905600
13:25:06.020 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505906000 ms
13:25:06.021 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505906000 ms.0 from job set of time 1760505906000 ms
13:25:06.034 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:06.037 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 2
13:25:06.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:06.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (sortByKey at sparkStreamingSocket.java:61)
13:25:06.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
13:25:06.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
13:25:06.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:06.044 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:06.046 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:06.046 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.5 MiB)
13:25:06.047 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
13:25:06.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
13:25:06.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 8 tasks resource profile 0
13:25:06.049 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 23) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.049 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 24) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.050 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 25) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.050 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 26) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.050 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 6.0 (TID 27) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.050 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 6.0 (TID 28) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.050 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 6.0 (TID 29) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.051 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 6.0 (TID 30) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:06.051 [Executor task launch worker for task 2.0 in stage 6.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 25)
13:25:06.052 [Executor task launch worker for task 3.0 in stage 6.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 26)
13:25:06.052 [Executor task launch worker for task 1.0 in stage 6.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 24)
13:25:06.052 [Executor task launch worker for task 5.0 in stage 6.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 6.0 (TID 28)
13:25:06.051 [Executor task launch worker for task 0.0 in stage 6.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 23)
13:25:06.052 [Executor task launch worker for task 4.0 in stage 6.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 6.0 (TID 27)
13:25:06.052 [Executor task launch worker for task 7.0 in stage 6.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 6.0 (TID 30)
13:25:06.052 [Executor task launch worker for task 6.0 in stage 6.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 6.0 (TID 29)
13:25:06.055 [Executor task launch worker for task 5.0 in stage 6.0 (TID 28)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505905200 locally
13:25:06.055 [Executor task launch worker for task 3.0 in stage 6.0 (TID 26)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505904800 locally
13:25:06.055 [Executor task launch worker for task 0.0 in stage 6.0 (TID 23)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505902800 locally
13:25:06.055 [Executor task launch worker for task 7.0 in stage 6.0 (TID 30)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505905600 locally
13:25:06.055 [Executor task launch worker for task 1.0 in stage 6.0 (TID 24)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505903000 locally
13:25:06.056 [Executor task launch worker for task 4.0 in stage 6.0 (TID 27)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505905000 locally
13:25:06.056 [Executor task launch worker for task 6.0 in stage 6.0 (TID 29)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505905400 locally
13:25:06.055 [Executor task launch worker for task 2.0 in stage 6.0 (TID 25)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505904600 locally
13:25:06.058 [Executor task launch worker for task 7.0 in stage 6.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 6.0 (TID 30). 1177 bytes result sent to driver
13:25:06.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 6.0 (TID 30) in 7 ms on 172.20.10.7 (executor driver) (1/8)
13:25:06.059 [Executor task launch worker for task 1.0 in stage 6.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 24). 1177 bytes result sent to driver
13:25:06.059 [Executor task launch worker for task 2.0 in stage 6.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 25). 1177 bytes result sent to driver
13:25:06.059 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 24) in 10 ms on 172.20.10.7 (executor driver) (2/8)
13:25:06.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 25) in 11 ms on 172.20.10.7 (executor driver) (3/8)
13:25:06.060 [Executor task launch worker for task 3.0 in stage 6.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 26). 1177 bytes result sent to driver
13:25:06.061 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 26) in 11 ms on 172.20.10.7 (executor driver) (4/8)
13:25:06.061 [Executor task launch worker for task 5.0 in stage 6.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 6.0 (TID 28). 1177 bytes result sent to driver
13:25:06.061 [Executor task launch worker for task 4.0 in stage 6.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 6.0 (TID 27). 1177 bytes result sent to driver
13:25:06.062 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 6.0 (TID 28) in 12 ms on 172.20.10.7 (executor driver) (5/8)
13:25:06.062 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 6.0 (TID 27) in 12 ms on 172.20.10.7 (executor driver) (6/8)
13:25:06.063 [Executor task launch worker for task 0.0 in stage 6.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 23). 1177 bytes result sent to driver
13:25:06.063 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 23) in 14 ms on 172.20.10.7 (executor driver) (7/8)
13:25:06.063 [Executor task launch worker for task 6.0 in stage 6.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 6.0 (TID 29). 1177 bytes result sent to driver
13:25:06.064 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 6.0 (TID 29) in 14 ms on 172.20.10.7 (executor driver) (8/8)
13:25:06.064 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
13:25:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (mapToPair at sparkStreamingSocket.java:49) finished in 0.022 s
13:25:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
13:25:06.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:06.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:06.067 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:06.068 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
13:25:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:06.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 10 tasks resource profile 0
13:25:06.069 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 31) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 32) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 33) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 34) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 35) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 36) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 37) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.070 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 38) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.071 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 7.0 (TID 39) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.071 [Executor task launch worker for task 0.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 31)
13:25:06.071 [Executor task launch worker for task 4.0 in stage 7.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 7.0 (TID 35)
13:25:06.071 [Executor task launch worker for task 3.0 in stage 7.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 34)
13:25:06.071 [Executor task launch worker for task 2.0 in stage 7.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 33)
13:25:06.071 [Executor task launch worker for task 1.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 32)
13:25:06.071 [Executor task launch worker for task 7.0 in stage 7.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 7.0 (TID 38)
13:25:06.071 [Executor task launch worker for task 5.0 in stage 7.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 7.0 (TID 36)
13:25:06.071 [Executor task launch worker for task 6.0 in stage 7.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 7.0 (TID 37)
13:25:06.071 [Executor task launch worker for task 8.0 in stage 7.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 7.0 (TID 39)
13:25:06.073 [Executor task launch worker for task 5.0 in stage 7.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.073 [Executor task launch worker for task 5.0 in stage 7.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.073 [Executor task launch worker for task 1.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.073 [Executor task launch worker for task 1.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.074 [Executor task launch worker for task 8.0 in stage 7.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.074 [Executor task launch worker for task 5.0 in stage 7.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 7.0 (TID 36). 1517 bytes result sent to driver
13:25:06.074 [Executor task launch worker for task 8.0 in stage 7.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.074 [Executor task launch worker for task 3.0 in stage 7.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.074 [Executor task launch worker for task 7.0 in stage 7.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.074 [Executor task launch worker for task 3.0 in stage 7.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.074 [Executor task launch worker for task 7.0 in stage 7.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:06.074 [Executor task launch worker for task 1.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 32). 1517 bytes result sent to driver
13:25:06.074 [Executor task launch worker for task 4.0 in stage 7.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.074 [Executor task launch worker for task 4.0 in stage 7.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.074 [Executor task launch worker for task 0.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1032.0 B) non-empty blocks including 8 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.075 [Executor task launch worker for task 0.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.075 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 7.0 (TID 40) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.075 [Executor task launch worker for task 4.0 in stage 7.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 7.0 (TID 35). 1517 bytes result sent to driver
13:25:06.075 [Executor task launch worker for task 9.0 in stage 7.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 7.0 (TID 40)
13:25:06.075 [Executor task launch worker for task 6.0 in stage 7.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.075 [Executor task launch worker for task 7.0 in stage 7.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 7.0 (TID 38). 1517 bytes result sent to driver
13:25:06.075 [Executor task launch worker for task 6.0 in stage 7.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.076 [Executor task launch worker for task 8.0 in stage 7.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 7.0 (TID 39). 1517 bytes result sent to driver
13:25:06.076 [Executor task launch worker for task 2.0 in stage 7.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.076 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 36) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:06.076 [Executor task launch worker for task 2.0 in stage 7.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:06.076 [Executor task launch worker for task 3.0 in stage 7.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 34). 1517 bytes result sent to driver
13:25:06.076 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 32) in 7 ms on 172.20.10.7 (executor driver) (2/10)
13:25:06.076 [Executor task launch worker for task 6.0 in stage 7.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 7.0 (TID 37). 1517 bytes result sent to driver
13:25:06.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 35) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:06.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 7.0 (TID 38) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:06.077 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 7.0 (TID 39) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:06.077 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 34) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:25:06.077 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 7.0 (TID 37) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:06.077 [Executor task launch worker for task 2.0 in stage 7.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 33). 1517 bytes result sent to driver
13:25:06.078 [Executor task launch worker for task 9.0 in stage 7.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.078 [Executor task launch worker for task 9.0 in stage 7.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.078 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 33) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:25:06.078 [Executor task launch worker for task 9.0 in stage 7.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 7.0 (TID 40). 1517 bytes result sent to driver
13:25:06.079 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 7.0 (TID 40) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:25:06.080 [Executor task launch worker for task 0.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 31). 1520 bytes result sent to driver
13:25:06.080 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 31) in 11 ms on 172.20.10.7 (executor driver) (10/10)
13:25:06.081 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
13:25:06.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (sortByKey at sparkStreamingSocket.java:61) finished in 0.015 s
13:25:06.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:06.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
13:25:06.081 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: sortByKey at sparkStreamingSocket.java:61, took 0.047080 s
13:25:06.090 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:06.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 3
13:25:06.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:06.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (collect at sparkStreamingSocket.java:61)
13:25:06.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
13:25:06.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
13:25:06.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:06.093 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:06.094 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:06.094 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:06.094 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
13:25:06.095 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:06.095 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 10 tasks resource profile 0
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 41) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 42) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 43) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 44) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 9.0 (TID 45) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 9.0 (TID 46) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 9.0 (TID 47) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 9.0 (TID 48) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.099 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 9.0 (TID 49) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.100 [Executor task launch worker for task 0.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 41)
13:25:06.100 [Executor task launch worker for task 3.0 in stage 9.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 44)
13:25:06.100 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.5 MiB)
13:25:06.100 [Executor task launch worker for task 5.0 in stage 9.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 9.0 (TID 46)
13:25:06.100 [Executor task launch worker for task 6.0 in stage 9.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 9.0 (TID 47)
13:25:06.100 [Executor task launch worker for task 8.0 in stage 9.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 9.0 (TID 49)
13:25:06.100 [Executor task launch worker for task 2.0 in stage 9.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 43)
13:25:06.100 [Executor task launch worker for task 1.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 42)
13:25:06.100 [Executor task launch worker for task 7.0 in stage 9.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 9.0 (TID 48)
13:25:06.100 [Executor task launch worker for task 4.0 in stage 9.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 9.0 (TID 45)
13:25:06.101 [Executor task launch worker for task 3.0 in stage 9.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.101 [Executor task launch worker for task 3.0 in stage 9.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.102 [Executor task launch worker for task 3.0 in stage 9.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 44). 1297 bytes result sent to driver
13:25:06.103 [Executor task launch worker for task 5.0 in stage 9.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.103 [Executor task launch worker for task 5.0 in stage 9.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.103 [Executor task launch worker for task 8.0 in stage 9.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.103 [Executor task launch worker for task 8.0 in stage 9.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.103 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 9.0 (TID 50) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:06.103 [Executor task launch worker for task 7.0 in stage 9.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.103 [Executor task launch worker for task 7.0 in stage 9.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.103 [Executor task launch worker for task 5.0 in stage 9.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 9.0 (TID 46). 1340 bytes result sent to driver
13:25:06.103 [Executor task launch worker for task 4.0 in stage 9.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.103 [Executor task launch worker for task 2.0 in stage 9.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.104 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 44) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:25:06.104 [Executor task launch worker for task 0.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1032.0 B) non-empty blocks including 8 (1032.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.104 [Executor task launch worker for task 4.0 in stage 9.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.104 [Executor task launch worker for task 2.0 in stage 9.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.104 [Executor task launch worker for task 9.0 in stage 9.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 9.0 (TID 50)
13:25:06.104 [Executor task launch worker for task 0.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.104 [Executor task launch worker for task 6.0 in stage 9.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.104 [Executor task launch worker for task 6.0 in stage 9.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.103 [Executor task launch worker for task 1.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.104 [Executor task launch worker for task 1.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:06.105 [Executor task launch worker for task 9.0 in stage 9.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.105 [Executor task launch worker for task 9.0 in stage 9.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.105 [Executor task launch worker for task 7.0 in stage 9.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 9.0 (TID 48). 1340 bytes result sent to driver
13:25:06.105 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 9.0 (TID 46) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:25:06.105 [Executor task launch worker for task 8.0 in stage 9.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 9.0 (TID 49). 1340 bytes result sent to driver
13:25:06.106 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 9.0 (TID 49) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:06.106 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 9.0 (TID 48) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:25:06.106 [Executor task launch worker for task 2.0 in stage 9.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 43). 1340 bytes result sent to driver
13:25:06.106 [Executor task launch worker for task 1.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 42). 1340 bytes result sent to driver
13:25:06.107 [Executor task launch worker for task 6.0 in stage 9.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 9.0 (TID 47). 1340 bytes result sent to driver
13:25:06.107 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 43) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:25:06.107 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 42) in 8 ms on 172.20.10.7 (executor driver) (6/10)
13:25:06.107 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 9.0 (TID 47) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:25:06.107 [Executor task launch worker for task 4.0 in stage 9.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 9.0 (TID 45). 1340 bytes result sent to driver
13:25:06.108 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 9.0 (TID 45) in 9 ms on 172.20.10.7 (executor driver) (8/10)
13:25:06.108 [Executor task launch worker for task 9.0 in stage 9.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 9.0 (TID 50). 1297 bytes result sent to driver
13:25:06.108 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 9.0 (TID 50) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:25:06.109 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:06.110 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:25:06.112 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.6 MiB)
13:25:06.112 [Executor task launch worker for task 0.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 41). 1469 bytes result sent to driver
13:25:06.112 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 41) in 14 ms on 172.20.10.7 (executor driver) (10/10)
13:25:06.113 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.021 s
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:06.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:06.114 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:06.114 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:06.115 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:06.115 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.6 MiB)
13:25:06.116 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:25:06.116 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1513
13:25:06.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:06.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0
13:25:06.117 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 51) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:06.117 [Executor task launch worker for task 0.0 in stage 10.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 51)
13:25:06.119 [Executor task launch worker for task 0.0 in stage 10.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:06.119 [Executor task launch worker for task 0.0 in stage 10.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:06.120 [Executor task launch worker for task 0.0 in stage 10.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 51). 1416 bytes result sent to driver
13:25:06.120 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 51) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:25:06.120 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
13:25:06.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (collect at sparkStreamingSocket.java:61) finished in 0.007 s
13:25:06.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:06.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
13:25:06.121 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: collect at sparkStreamingSocket.java:61, took 0.031288 s
13:25:06.122 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505906000 ms.0 from job set of time 1760505906000 ms
13:25:06.122 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.122 s for time 1760505906000 ms (execution: 0.101 s)
13:25:06.122 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 4 from persistence list
13:25:06.123 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 3 from persistence list
13:25:06.124 [block-manager-storage-async-thread-pool-21] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:25:06.124 [block-manager-storage-async-thread-pool-22] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:25:06.124 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 2 from persistence list
13:25:06.124 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:25:06.124 [block-manager-storage-async-thread-pool-25] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:25:06.125 [block-manager-storage-async-thread-pool-29] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:25:06.125 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505906000 ms
13:25:06.126 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:25:06.126 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:25:06.126 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505902600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:09.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505909000 ms
13:25:09.014 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505909000 ms.0 from job set of time 1760505909000 ms
13:25:09.024 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:09.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 17 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 4
13:25:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (sortByKey at sparkStreamingSocket.java:61)
13:25:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 11)
13:25:09.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:09.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:09.033 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:09.034 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:09.035 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.6 MiB)
13:25:09.035 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1513
13:25:09.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:09.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 10 tasks resource profile 0
13:25:09.037 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 52) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.037 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 53) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.037 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 54) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.037 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 55) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 56) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 57) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 58) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 59) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 12.0 (TID 60) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.038 [Executor task launch worker for task 1.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 53)
13:25:09.038 [Executor task launch worker for task 0.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 52)
13:25:09.039 [Executor task launch worker for task 4.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 56)
13:25:09.039 [Executor task launch worker for task 5.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 57)
13:25:09.039 [Executor task launch worker for task 6.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 58)
13:25:09.039 [Executor task launch worker for task 7.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 59)
13:25:09.039 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 12.0 (TID 60)
13:25:09.039 [Executor task launch worker for task 3.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 55)
13:25:09.038 [Executor task launch worker for task 2.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 54)
13:25:09.042 [Executor task launch worker for task 0.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 4.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 0.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 4.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 5.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 6.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 1.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 7.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 5.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 1.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 7.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 2.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 3.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.042 [Executor task launch worker for task 2.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:09.042 [Executor task launch worker for task 3.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.042 [Executor task launch worker for task 0.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 52). 1517 bytes result sent to driver
13:25:09.042 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.043 [Executor task launch worker for task 1.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 53). 1517 bytes result sent to driver
13:25:09.043 [Executor task launch worker for task 4.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 56). 1517 bytes result sent to driver
13:25:09.043 [Executor task launch worker for task 5.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 57). 1517 bytes result sent to driver
13:25:09.043 [Executor task launch worker for task 7.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 59). 1517 bytes result sent to driver
13:25:09.042 [Executor task launch worker for task 6.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.043 [Executor task launch worker for task 2.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 54). 1517 bytes result sent to driver
13:25:09.043 [Executor task launch worker for task 3.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 55). 1517 bytes result sent to driver
13:25:09.043 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 12.0 (TID 61) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.043 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 12.0 (TID 60). 1517 bytes result sent to driver
13:25:09.043 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 12.0 (TID 61)
13:25:09.043 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 52) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:09.043 [Executor task launch worker for task 6.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 58). 1517 bytes result sent to driver
13:25:09.044 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 53) in 7 ms on 172.20.10.7 (executor driver) (2/10)
13:25:09.045 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 57) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:25:09.045 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 56) in 8 ms on 172.20.10.7 (executor driver) (4/10)
13:25:09.045 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.045 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 59) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:25:09.045 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.045 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 54) in 8 ms on 172.20.10.7 (executor driver) (6/10)
13:25:09.045 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 55) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:25:09.046 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 12.0 (TID 61). 1474 bytes result sent to driver
13:25:09.046 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 12.0 (TID 60) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:25:09.046 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 58) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:25:09.046 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 12.0 (TID 61) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:25:09.046 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
13:25:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 (sortByKey at sparkStreamingSocket.java:61) finished in 0.015 s
13:25:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:09.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished
13:25:09.047 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: sortByKey at sparkStreamingSocket.java:61, took 0.022303 s
13:25:09.052 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:09.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 5
13:25:09.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:09.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (collect at sparkStreamingSocket.java:61)
13:25:09.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
13:25:09.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
13:25:09.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:09.055 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:09.056 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:09.057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.6 MiB)
13:25:09.057 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1513
13:25:09.057 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:09.057 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 10 tasks resource profile 0
13:25:09.058 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 62) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.058 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 63) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.058 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 64) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.059 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 65) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.059 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 66) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.059 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 67) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.059 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 68) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.059 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 69) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.060 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 14.0 (TID 70) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.060 [Executor task launch worker for task 1.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 63)
13:25:09.060 [Executor task launch worker for task 0.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 62)
13:25:09.060 [Executor task launch worker for task 2.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 64)
13:25:09.060 [Executor task launch worker for task 3.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 65)
13:25:09.060 [Executor task launch worker for task 4.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 66)
13:25:09.060 [Executor task launch worker for task 5.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 67)
13:25:09.060 [Executor task launch worker for task 6.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 68)
13:25:09.061 [Executor task launch worker for task 7.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 69)
13:25:09.061 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 14.0 (TID 70)
13:25:09.062 [Executor task launch worker for task 0.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 0.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 7.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 4.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 2.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 1.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 7.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 4.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 1.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 5.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 6.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 5.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 6.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 2.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 3.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 3.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.063 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.063 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.064 [Executor task launch worker for task 5.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 67). 1340 bytes result sent to driver
13:25:09.064 [Executor task launch worker for task 2.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 64). 1340 bytes result sent to driver
13:25:09.065 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 14.0 (TID 71) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:09.065 [Executor task launch worker for task 0.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 62). 1340 bytes result sent to driver
13:25:09.065 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 14.0 (TID 71)
13:25:09.065 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 67) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:09.065 [Executor task launch worker for task 3.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 65). 1340 bytes result sent to driver
13:25:09.066 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 64) in 7 ms on 172.20.10.7 (executor driver) (2/10)
13:25:09.066 [Executor task launch worker for task 7.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 69). 1297 bytes result sent to driver
13:25:09.066 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 62) in 8 ms on 172.20.10.7 (executor driver) (3/10)
13:25:09.066 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 65) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:25:09.067 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 14.0 (TID 70). 1340 bytes result sent to driver
13:25:09.067 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 69) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:25:09.067 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 14.0 (TID 70) in 8 ms on 172.20.10.7 (executor driver) (6/10)
13:25:09.067 [Executor task launch worker for task 6.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 68). 1340 bytes result sent to driver
13:25:09.067 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.067 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 68) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:25:09.068 [Executor task launch worker for task 1.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 63). 1340 bytes result sent to driver
13:25:09.068 [Executor task launch worker for task 4.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 66). 1340 bytes result sent to driver
13:25:09.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 63) in 10 ms on 172.20.10.7 (executor driver) (8/10)
13:25:09.068 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 14.0 (TID 71). 1340 bytes result sent to driver
13:25:09.068 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 66) in 9 ms on 172.20.10.7 (executor driver) (9/10)
13:25:09.068 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 14.0 (TID 71) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:25:09.068 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.014 s
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:09.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:09.070 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:09.071 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:09.071 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:09.072 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1513
13:25:09.072 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:09.072 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0
13:25:09.073 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 72) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:09.073 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 72)
13:25:09.074 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:09.074 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:09.075 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 72). 1185 bytes result sent to driver
13:25:09.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 72) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:25:09.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
13:25:09.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (collect at sparkStreamingSocket.java:61) finished in 0.005 s
13:25:09.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:09.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished
13:25:09.076 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: collect at sparkStreamingSocket.java:61, took 0.023054 s
13:25:09.076 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505909000 ms.0 from job set of time 1760505909000 ms
13:25:09.076 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.076 s for time 1760505909000 ms (execution: 0.062 s)
13:25:09.076 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 11 from persistence list
13:25:09.077 [block-manager-storage-async-thread-pool-36] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:25:09.077 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 10 from persistence list
13:25:09.077 [block-manager-storage-async-thread-pool-38] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:25:09.077 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 9 from persistence list
13:25:09.077 [block-manager-storage-async-thread-pool-42] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:25:09.078 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:25:09.078 [block-manager-storage-async-thread-pool-45] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:25:09.078 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505909000 ms
13:25:09.079 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505902800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505903000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505904600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505904800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.081 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505905000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505905200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.082 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505903000 ms
13:25:09.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505905400 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:09.082 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505903000 ms
13:25:09.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505905600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:11.404 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505911200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:11.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505911200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:11.406 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:11.406 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505911200 replicated to only 0 peer(s) instead of 1 peers
13:25:11.407 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505911200
13:25:11.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505911600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:11.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505911600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:11.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:11.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505911600 replicated to only 0 peer(s) instead of 1 peers
13:25:11.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505911600
13:25:12.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505912000 ms
13:25:12.014 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505912000 ms.0 from job set of time 1760505912000 ms
13:25:12.028 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:12.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 24 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 6
13:25:12.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:12.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (sortByKey at sparkStreamingSocket.java:61)
13:25:12.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
13:25:12.031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
13:25:12.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:12.038 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:12.040 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:12.041 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.5 MiB)
13:25:12.042 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1513
13:25:12.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1))
13:25:12.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 2 tasks resource profile 0
13:25:12.044 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 73) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:12.044 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 74) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:12.045 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 73)
13:25:12.045 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 74)
13:25:12.047 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505911200 locally
13:25:12.047 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505911600 locally
13:25:12.050 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 74). 1177 bytes result sent to driver
13:25:12.051 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 74) in 6 ms on 172.20.10.7 (executor driver) (1/2)
13:25:12.051 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 73). 1177 bytes result sent to driver
13:25:12.051 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 73) in 7 ms on 172.20.10.7 (executor driver) (2/2)
13:25:12.051 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
13:25:12.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (mapToPair at sparkStreamingSocket.java:49) finished in 0.016 s
13:25:12.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:12.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:12.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
13:25:12.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:12.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:12.054 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:12.055 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 2004.4 MiB)
13:25:12.055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.20.10.7:49875 (size: 3.7 KiB, free: 2004.5 MiB)
13:25:12.055 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1513
13:25:12.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:12.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 10 tasks resource profile 0
13:25:12.056 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 75) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 17.0 (TID 76) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 17.0 (TID 77) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 17.0 (TID 78) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 17.0 (TID 79) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 17.0 (TID 80) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 17.0 (TID 81) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 17.0 (TID 82) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.057 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 17.0 (TID 83) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.058 [Executor task launch worker for task 0.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 75)
13:25:12.058 [Executor task launch worker for task 4.0 in stage 17.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 17.0 (TID 79)
13:25:12.058 [Executor task launch worker for task 3.0 in stage 17.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 17.0 (TID 78)
13:25:12.058 [Executor task launch worker for task 6.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 17.0 (TID 81)
13:25:12.058 [Executor task launch worker for task 5.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 17.0 (TID 80)
13:25:12.058 [Executor task launch worker for task 2.0 in stage 17.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 17.0 (TID 77)
13:25:12.058 [Executor task launch worker for task 8.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 17.0 (TID 83)
13:25:12.058 [Executor task launch worker for task 1.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 17.0 (TID 76)
13:25:12.058 [Executor task launch worker for task 7.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 17.0 (TID 82)
13:25:12.060 [Executor task launch worker for task 7.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 0.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 0.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.060 [Executor task launch worker for task 7.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.060 [Executor task launch worker for task 8.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 5.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 4.0 in stage 17.0 (TID 79)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 3.0 in stage 17.0 (TID 78)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 2.0 in stage 17.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.060 [Executor task launch worker for task 1.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.062 [Executor task launch worker for task 5.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.062 [Executor task launch worker for task 4.0 in stage 17.0 (TID 79)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.062 [Executor task launch worker for task 2.0 in stage 17.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.062 [Executor task launch worker for task 0.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 75). 1520 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 3.0 in stage 17.0 (TID 78)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.062 [Executor task launch worker for task 1.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.060 [Executor task launch worker for task 6.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.062 [Executor task launch worker for task 7.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 17.0 (TID 82). 1517 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 6.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
13:25:12.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 17.0 (TID 84) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.062 [Executor task launch worker for task 8.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:12.062 [Executor task launch worker for task 5.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 17.0 (TID 80). 1517 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 2.0 in stage 17.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 17.0 (TID 77). 1517 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 3.0 in stage 17.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 17.0 (TID 78). 1517 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 6.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 17.0 (TID 81). 1517 bytes result sent to driver
13:25:12.062 [Executor task launch worker for task 1.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 17.0 (TID 76). 1517 bytes result sent to driver
13:25:12.063 [Executor task launch worker for task 9.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 17.0 (TID 84)
13:25:12.063 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 75) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:12.063 [Executor task launch worker for task 4.0 in stage 17.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 17.0 (TID 79). 1517 bytes result sent to driver
13:25:12.063 [Executor task launch worker for task 8.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 17.0 (TID 83). 1517 bytes result sent to driver
13:25:12.063 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 17.0 (TID 82) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:25:12.063 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 17.0 (TID 80) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:12.064 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 17.0 (TID 78) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:25:12.064 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 17.0 (TID 77) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:25:12.064 [Executor task launch worker for task 9.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.064 [Executor task launch worker for task 9.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.064 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 17.0 (TID 81) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:25:12.064 [Executor task launch worker for task 9.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 17.0 (TID 84). 1431 bytes result sent to driver
13:25:12.065 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 17.0 (TID 76) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:25:12.065 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 17.0 (TID 79) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:25:12.065 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 17.0 (TID 83) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:25:12.065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 17.0 (TID 84) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:25:12.065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
13:25:12.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (sortByKey at sparkStreamingSocket.java:61) finished in 0.012 s
13:25:12.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:12.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished
13:25:12.066 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: sortByKey at sparkStreamingSocket.java:61, took 0.037709 s
13:25:12.070 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:12.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 25 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 7
13:25:12.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:12.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (collect at sparkStreamingSocket.java:61)
13:25:12.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)
13:25:12.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 19)
13:25:12.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:12.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:12.073 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:12.073 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:12.073 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1513
13:25:12.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:12.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 10 tasks resource profile 0
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 85) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 86) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 87) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 88) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 19.0 (TID 89) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 19.0 (TID 90) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.077 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 19.0 (TID 91) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.077 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 19.0 (TID 92) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.077 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 19.0 (TID 93) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.077 [Executor task launch worker for task 0.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 85)
13:25:12.077 [Executor task launch worker for task 1.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 86)
13:25:12.077 [Executor task launch worker for task 2.0 in stage 19.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 87)
13:25:12.077 [Executor task launch worker for task 3.0 in stage 19.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 88)
13:25:12.077 [Executor task launch worker for task 4.0 in stage 19.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 19.0 (TID 89)
13:25:12.077 [Executor task launch worker for task 5.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 19.0 (TID 90)
13:25:12.077 [Executor task launch worker for task 6.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 19.0 (TID 91)
13:25:12.077 [Executor task launch worker for task 7.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 19.0 (TID 92)
13:25:12.077 [Executor task launch worker for task 8.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 19.0 (TID 93)
13:25:12.079 [Executor task launch worker for task 0.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 0.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.079 [Executor task launch worker for task 4.0 in stage 19.0 (TID 89)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 4.0 in stage 19.0 (TID 89)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.079 [Executor task launch worker for task 7.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 1.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 8.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.080 [Executor task launch worker for task 8.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [Executor task launch worker for task 2.0 in stage 19.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 6.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.080 [Executor task launch worker for task 2.0 in stage 19.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [Executor task launch worker for task 5.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.079 [Executor task launch worker for task 7.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [Executor task launch worker for task 3.0 in stage 19.0 (TID 88)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.080 [Executor task launch worker for task 5.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [Executor task launch worker for task 6.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.079 [Executor task launch worker for task 1.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [Executor task launch worker for task 4.0 in stage 19.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 19.0 (TID 89). 1340 bytes result sent to driver
13:25:12.080 [Executor task launch worker for task 3.0 in stage 19.0 (TID 88)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.080 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 19.0 (TID 94) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:12.080 [Executor task launch worker for task 9.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 19.0 (TID 94)
13:25:12.081 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 19.0 (TID 89) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:25:12.081 [Executor task launch worker for task 8.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 19.0 (TID 93). 1340 bytes result sent to driver
13:25:12.081 [Executor task launch worker for task 3.0 in stage 19.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 88). 1340 bytes result sent to driver
13:25:12.081 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 19.0 (TID 93) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:12.082 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 88) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:12.082 [Executor task launch worker for task 1.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 86). 1340 bytes result sent to driver
13:25:12.082 [Executor task launch worker for task 9.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.082 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 86) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:12.082 [Executor task launch worker for task 9.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.082 [Executor task launch worker for task 0.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 85). 1426 bytes result sent to driver
13:25:12.083 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 85) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:25:12.083 [Executor task launch worker for task 7.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 19.0 (TID 92). 1340 bytes result sent to driver
13:25:12.083 [Executor task launch worker for task 2.0 in stage 19.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 87). 1340 bytes result sent to driver
13:25:12.083 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 19.0 (TID 92) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:12.083 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 87) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:12.083 [Executor task launch worker for task 6.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 19.0 (TID 91). 1340 bytes result sent to driver
13:25:12.084 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 19.0 (TID 91) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:25:12.084 [Executor task launch worker for task 5.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 19.0 (TID 90). 1340 bytes result sent to driver
13:25:12.084 [Executor task launch worker for task 9.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 19.0 (TID 94). 1297 bytes result sent to driver
13:25:12.084 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 19.0 (TID 90) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:25:12.085 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 19.0 (TID 94) in 5 ms on 172.20.10.7 (executor driver) (10/10)
13:25:12.085 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 19 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.013 s
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 20)
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:12.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:12.086 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:12.087 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:12.087 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:12.087 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1513
13:25:12.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:12.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0
13:25:12.088 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 95) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:12.088 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 95)
13:25:12.090 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:12.090 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:12.090 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 95). 1373 bytes result sent to driver
13:25:12.091 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 95) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:25:12.091 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
13:25:12.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (collect at sparkStreamingSocket.java:61) finished in 0.006 s
13:25:12.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:12.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished
13:25:12.091 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: collect at sparkStreamingSocket.java:61, took 0.021479 s
13:25:12.091 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505912000 ms.0 from job set of time 1760505912000 ms
13:25:12.092 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.091 s for time 1760505912000 ms (execution: 0.077 s)
13:25:12.092 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 18 from persistence list
13:25:12.092 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 17 from persistence list
13:25:12.092 [block-manager-storage-async-thread-pool-72] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:25:12.092 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 16 from persistence list
13:25:12.092 [block-manager-storage-async-thread-pool-74] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:25:12.092 [block-manager-storage-async-thread-pool-76] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:25:12.092 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:25:12.093 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505912000 ms
13:25:12.093 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505906000 ms
13:25:12.093 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505906000 ms
13:25:12.093 [block-manager-storage-async-thread-pool-80] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:25:13.405 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505913200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:13.406 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505913200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:13.406 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:13.407 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505913200 replicated to only 0 peer(s) instead of 1 peers
13:25:13.407 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505913200
13:25:13.604 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505913400 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:13.604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505913400 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:13.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:13.605 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505913400 replicated to only 0 peer(s) instead of 1 peers
13:25:13.606 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505913400
13:25:13.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505913600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:13.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505913600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:13.803 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:13.804 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505913600 replicated to only 0 peer(s) instead of 1 peers
13:25:13.804 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505913600
13:25:14.004 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505913800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:14.004 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505913800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:14.005 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:14.005 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505913800 replicated to only 0 peer(s) instead of 1 peers
13:25:14.006 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505913800
13:25:14.401 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505914200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:14.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505914200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:14.402 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:14.403 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505914200 replicated to only 0 peer(s) instead of 1 peers
13:25:14.403 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505914200
13:25:14.607 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505914400 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:14.608 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505914400 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.5 MiB)
13:25:14.609 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:14.609 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505914400 replicated to only 0 peer(s) instead of 1 peers
13:25:14.610 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505914400
13:25:14.807 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505914600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:14.808 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505914600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:14.808 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:14.809 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505914600 replicated to only 0 peer(s) instead of 1 peers
13:25:14.809 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505914600
13:25:15.003 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505914800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:15.004 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505914800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:15.005 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:15.005 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505914800 replicated to only 0 peer(s) instead of 1 peers
13:25:15.006 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505914800
13:25:15.031 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505915000 ms
13:25:15.031 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505915000 ms.0 from job set of time 1760505915000 ms
13:25:15.031 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:15.034 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:15.036 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.5 MiB)
13:25:15.040 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:15.042 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:15.043 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.20.10.7:49875 in memory (size: 3.7 KiB, free: 2004.6 MiB)
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 31 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 8
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (sortByKey at sparkStreamingSocket.java:61)
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 21)
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 21)
13:25:15.044 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:15.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 21 (MapPartitionsRDD[31] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:15.047 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:15.047 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:15.047 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:15.047 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.6 MiB)
13:25:15.048 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1513
13:25:15.048 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:25:15.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 7 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[31] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
13:25:15.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 7 tasks resource profile 0
13:25:15.049 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 96) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 21.0 (TID 97) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 21.0 (TID 98) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 21.0 (TID 99) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 21.0 (TID 100) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 21.0 (TID 101) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.049 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 21.0 (TID 102) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:15.050 [Executor task launch worker for task 1.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 21.0 (TID 97)
13:25:15.050 [Executor task launch worker for task 0.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 96)
13:25:15.050 [Executor task launch worker for task 4.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 21.0 (TID 100)
13:25:15.050 [Executor task launch worker for task 6.0 in stage 21.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 21.0 (TID 102)
13:25:15.050 [Executor task launch worker for task 2.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 21.0 (TID 98)
13:25:15.050 [Executor task launch worker for task 5.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 21.0 (TID 101)
13:25:15.050 [Executor task launch worker for task 3.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 21.0 (TID 99)
13:25:15.051 [Executor task launch worker for task 2.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505913600 locally
13:25:15.051 [Executor task launch worker for task 0.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505913200 locally
13:25:15.051 [Executor task launch worker for task 5.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505914400 locally
13:25:15.051 [Executor task launch worker for task 3.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505913800 locally
13:25:15.052 [Executor task launch worker for task 4.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505914200 locally
13:25:15.052 [Executor task launch worker for task 6.0 in stage 21.0 (TID 102)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505914600 locally
13:25:15.051 [Executor task launch worker for task 1.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505913400 locally
13:25:15.053 [Executor task launch worker for task 0.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 96). 1177 bytes result sent to driver
13:25:15.054 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 96) in 5 ms on 172.20.10.7 (executor driver) (1/7)
13:25:15.054 [Executor task launch worker for task 1.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 21.0 (TID 97). 1177 bytes result sent to driver
13:25:15.055 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 21.0 (TID 97) in 6 ms on 172.20.10.7 (executor driver) (2/7)
13:25:15.055 [Executor task launch worker for task 3.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 21.0 (TID 99). 1177 bytes result sent to driver
13:25:15.055 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 21.0 (TID 99) in 6 ms on 172.20.10.7 (executor driver) (3/7)
13:25:15.056 [Executor task launch worker for task 6.0 in stage 21.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 21.0 (TID 102). 1177 bytes result sent to driver
13:25:15.056 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 21.0 (TID 102) in 7 ms on 172.20.10.7 (executor driver) (4/7)
13:25:15.056 [Executor task launch worker for task 4.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 21.0 (TID 100). 1177 bytes result sent to driver
13:25:15.057 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 21.0 (TID 100) in 8 ms on 172.20.10.7 (executor driver) (5/7)
13:25:15.057 [Executor task launch worker for task 5.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 21.0 (TID 101). 1177 bytes result sent to driver
13:25:15.058 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 21.0 (TID 101) in 9 ms on 172.20.10.7 (executor driver) (6/7)
13:25:15.059 [Executor task launch worker for task 2.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 21.0 (TID 98). 1134 bytes result sent to driver
13:25:15.059 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 21.0 (TID 98) in 10 ms on 172.20.10.7 (executor driver) (7/7)
13:25:15.059 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
13:25:15.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 21 (mapToPair at sparkStreamingSocket.java:49) finished in 0.014 s
13:25:15.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:15.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:15.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 22)
13:25:15.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:15.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[34] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:15.061 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:25:15.061 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:15.061 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.6 MiB)
13:25:15.061 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1513
13:25:15.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 22 (MapPartitionsRDD[34] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:15.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 10 tasks resource profile 0
13:25:15.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 103) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 104) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 105) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 106) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 107) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 108) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 109) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 110) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 22.0 (TID 111) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.063 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 104)
13:25:15.063 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 105)
13:25:15.063 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 106)
13:25:15.063 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 103)
13:25:15.063 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 108)
13:25:15.063 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 22.0 (TID 111)
13:25:15.063 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 107)
13:25:15.063 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 110)
13:25:15.063 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 109)
13:25:15.066 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 7 (903.0 B) non-empty blocks including 7 (903.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 108). 1474 bytes result sent to driver
13:25:15.066 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.066 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.066 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.067 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 22.0 (TID 111). 1474 bytes result sent to driver
13:25:15.066 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.067 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.067 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 105). 1517 bytes result sent to driver
13:25:15.066 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.067 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 110). 1517 bytes result sent to driver
13:25:15.067 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 107). 1517 bytes result sent to driver
13:25:15.067 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 109). 1517 bytes result sent to driver
13:25:15.067 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 22.0 (TID 112) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.067 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 106). 1517 bytes result sent to driver
13:25:15.067 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 104). 1517 bytes result sent to driver
13:25:15.067 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 22.0 (TID 112)
13:25:15.068 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 108) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:15.068 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 22.0 (TID 111) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:25:15.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 105) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:15.068 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.068 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.068 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 110) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:15.068 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 107) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:15.068 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 22.0 (TID 112). 1431 bytes result sent to driver
13:25:15.069 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 109) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:15.069 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 106) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:15.069 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 104) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:25:15.069 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 22.0 (TID 112) in 2 ms on 172.20.10.7 (executor driver) (9/10)
13:25:15.069 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 103). 1520 bytes result sent to driver
13:25:15.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 103) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:15.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
13:25:15.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 (sortByKey at sparkStreamingSocket.java:61) finished in 0.010 s
13:25:15.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:15.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished
13:25:15.070 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: sortByKey at sparkStreamingSocket.java:61, took 0.028008 s
13:25:15.073 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 32 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 9
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 10 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (collect at sparkStreamingSocket.java:61)
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 24)
13:25:15.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 24 (ShuffledRDD[32] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:15.075 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:15.075 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:15.076 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.6 MiB)
13:25:15.076 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1513
13:25:15.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 24 (ShuffledRDD[32] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:15.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 24.0 with 10 tasks resource profile 0
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 24.0 (TID 113) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 24.0 (TID 114) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 24.0 (TID 115) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 24.0 (TID 116) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 24.0 (TID 117) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 24.0 (TID 118) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 24.0 (TID 119) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 24.0 (TID 120) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.077 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 24.0 (TID 121) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.078 [Executor task launch worker for task 1.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 24.0 (TID 114)
13:25:15.078 [Executor task launch worker for task 0.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 24.0 (TID 113)
13:25:15.078 [Executor task launch worker for task 3.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 24.0 (TID 116)
13:25:15.078 [Executor task launch worker for task 5.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 24.0 (TID 118)
13:25:15.078 [Executor task launch worker for task 8.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 24.0 (TID 121)
13:25:15.078 [Executor task launch worker for task 6.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 24.0 (TID 119)
13:25:15.078 [Executor task launch worker for task 4.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 24.0 (TID 117)
13:25:15.078 [Executor task launch worker for task 7.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 24.0 (TID 120)
13:25:15.078 [Executor task launch worker for task 2.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 24.0 (TID 115)
13:25:15.079 [Executor task launch worker for task 1.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.079 [Executor task launch worker for task 1.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.080 [Executor task launch worker for task 3.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.080 [Executor task launch worker for task 3.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.080 [Executor task launch worker for task 6.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.080 [Executor task launch worker for task 5.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 6.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [Executor task launch worker for task 4.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 1.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 24.0 (TID 114). 1297 bytes result sent to driver
13:25:15.081 [Executor task launch worker for task 7.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 5.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [Executor task launch worker for task 4.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [Executor task launch worker for task 7.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.080 [Executor task launch worker for task 2.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 8.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 2.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [Executor task launch worker for task 8.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [Executor task launch worker for task 0.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 7 (903.0 B) non-empty blocks including 7 (903.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.081 [Executor task launch worker for task 0.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.081 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 24.0 (TID 122) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:15.081 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 24.0 (TID 114) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:15.081 [Executor task launch worker for task 9.0 in stage 24.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 24.0 (TID 122)
13:25:15.082 [Executor task launch worker for task 3.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 24.0 (TID 116). 1340 bytes result sent to driver
13:25:15.082 [Executor task launch worker for task 2.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 24.0 (TID 115). 1340 bytes result sent to driver
13:25:15.082 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 24.0 (TID 116) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:25:15.082 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 24.0 (TID 115) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:15.082 [Executor task launch worker for task 8.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 24.0 (TID 121). 1340 bytes result sent to driver
13:25:15.083 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 24.0 (TID 121) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:15.083 [Executor task launch worker for task 9.0 in stage 24.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.083 [Executor task launch worker for task 7.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 24.0 (TID 120). 1340 bytes result sent to driver
13:25:15.083 [Executor task launch worker for task 9.0 in stage 24.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.083 [Executor task launch worker for task 4.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 24.0 (TID 117). 1340 bytes result sent to driver
13:25:15.083 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 24.0 (TID 120) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:15.083 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 24.0 (TID 117) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:15.083 [Executor task launch worker for task 5.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 24.0 (TID 118). 1340 bytes result sent to driver
13:25:15.084 [Executor task launch worker for task 6.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 24.0 (TID 119). 1340 bytes result sent to driver
13:25:15.084 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 24.0 (TID 118) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:15.084 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 24.0 (TID 119) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:25:15.084 [Executor task launch worker for task 9.0 in stage 24.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 24.0 (TID 122). 1297 bytes result sent to driver
13:25:15.084 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 24.0 (TID 122) in 3 ms on 172.20.10.7 (executor driver) (9/10)
13:25:15.085 [Executor task launch worker for task 0.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 24.0 (TID 113). 1469 bytes result sent to driver
13:25:15.085 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 24.0 (TID 113) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:15.085 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 24.0, whose tasks have all completed, from pool 
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 24 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.010 s
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 25)
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (ShuffledRDD[35] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:15.086 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.6 MiB)
13:25:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1513
13:25:15.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[35] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:15.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks resource profile 0
13:25:15.088 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 123) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:15.088 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 123)
13:25:15.089 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:15.089 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:15.089 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 123). 1330 bytes result sent to driver
13:25:15.089 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 123) in 1 ms on 172.20.10.7 (executor driver) (1/1)
13:25:15.089 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
13:25:15.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (collect at sparkStreamingSocket.java:61) finished in 0.004 s
13:25:15.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:15.090 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished
13:25:15.090 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 finished: collect at sparkStreamingSocket.java:61, took 0.016064 s
13:25:15.090 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505915000 ms.0 from job set of time 1760505915000 ms
13:25:15.090 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.090 s for time 1760505915000 ms (execution: 0.059 s)
13:25:15.090 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 25 from persistence list
13:25:15.090 [block-manager-storage-async-thread-pool-30] INFO  org.apache.spark.storage.BlockManager - Removing RDD 25
13:25:15.090 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 24 from persistence list
13:25:15.090 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 23 from persistence list
13:25:15.090 [block-manager-storage-async-thread-pool-32] INFO  org.apache.spark.storage.BlockManager - Removing RDD 24
13:25:15.090 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 22 from persistence list
13:25:15.090 [block-manager-storage-async-thread-pool-35] INFO  org.apache.spark.storage.BlockManager - Removing RDD 23
13:25:15.091 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[22] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505915000 ms
13:25:15.091 [block-manager-storage-async-thread-pool-39] INFO  org.apache.spark.storage.BlockManager - Removing RDD 22
13:25:15.091 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505909000 ms
13:25:15.091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505911200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:15.091 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505909000 ms
13:25:15.091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505911600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:15.205 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505915000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:15.205 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505915000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:15.206 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:15.206 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505915000 replicated to only 0 peer(s) instead of 1 peers
13:25:15.206 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505915000
13:25:15.402 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505915200 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:15.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505915200 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.6 MiB)
13:25:15.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:15.404 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505915200 replicated to only 0 peer(s) instead of 1 peers
13:25:15.404 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505915200
13:25:15.602 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505915400 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:15.603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505915400 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:15.604 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:15.604 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505915400 replicated to only 0 peer(s) instead of 1 peers
13:25:15.605 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505915400
13:25:15.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505915600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:15.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505915600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:15.803 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:15.803 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505915600 replicated to only 0 peer(s) instead of 1 peers
13:25:15.804 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505915600
13:25:16.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505915800 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:16.002 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505915800 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.6 MiB)
13:25:16.003 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:16.003 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505915800 replicated to only 0 peer(s) instead of 1 peers
13:25:16.003 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505915800
13:25:16.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505916000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:16.202 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505916000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:16.203 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:16.203 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505916000 replicated to only 0 peer(s) instead of 1 peers
13:25:16.204 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505916000
13:25:16.403 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505916200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:16.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505916200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:16.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:16.405 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505916200 replicated to only 0 peer(s) instead of 1 peers
13:25:16.405 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505916200
13:25:16.602 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505916400 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:16.603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505916400 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:16.603 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:16.604 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505916400 replicated to only 0 peer(s) instead of 1 peers
13:25:16.605 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505916400
13:25:16.801 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505916600 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:16.802 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505916600 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.6 MiB)
13:25:16.803 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:16.803 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505916600 replicated to only 0 peer(s) instead of 1 peers
13:25:16.804 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505916600
13:25:17.003 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505916800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:17.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505916800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:17.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:17.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505916800 replicated to only 0 peer(s) instead of 1 peers
13:25:17.005 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505916800
13:25:17.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505917000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:17.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505917000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:17.205 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:17.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505917000 replicated to only 0 peer(s) instead of 1 peers
13:25:17.206 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505917000
13:25:17.406 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505917200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:17.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505917200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:17.408 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:17.408 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505917200 replicated to only 0 peer(s) instead of 1 peers
13:25:17.408 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505917200
13:25:17.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505917400 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:17.604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505917400 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.6 MiB)
13:25:17.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:17.605 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505917400 replicated to only 0 peer(s) instead of 1 peers
13:25:17.605 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505917400
13:25:17.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505917600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:17.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505917600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:17.804 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:17.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505917600 replicated to only 0 peer(s) instead of 1 peers
13:25:17.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505917600
13:25:18.006 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505917800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:18.006 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505917800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:18.007 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:18.007 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505917800 replicated to only 0 peer(s) instead of 1 peers
13:25:18.008 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505917800
13:25:18.017 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505918000 ms
13:25:18.018 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505918000 ms.0 from job set of time 1760505918000 ms
13:25:18.028 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:18.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 38 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 10
13:25:18.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:18.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (sortByKey at sparkStreamingSocket.java:61)
13:25:18.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
13:25:18.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 26)
13:25:18.031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 26 (MapPartitionsRDD[38] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:18.036 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:18.037 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:18.037 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.5 MiB)
13:25:18.038 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1513
13:25:18.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 15 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[38] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
13:25:18.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 15 tasks resource profile 0
13:25:18.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 124) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 26.0 (TID 125) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 26.0 (TID 126) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 26.0 (TID 127) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 26.0 (TID 128) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 26.0 (TID 129) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 26.0 (TID 130) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 26.0 (TID 131) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 26.0 (TID 132) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.042 [Executor task launch worker for task 1.0 in stage 26.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 26.0 (TID 125)
13:25:18.042 [Executor task launch worker for task 0.0 in stage 26.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 124)
13:25:18.042 [Executor task launch worker for task 2.0 in stage 26.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 26.0 (TID 126)
13:25:18.042 [Executor task launch worker for task 3.0 in stage 26.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 26.0 (TID 127)
13:25:18.042 [Executor task launch worker for task 4.0 in stage 26.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 26.0 (TID 128)
13:25:18.042 [Executor task launch worker for task 5.0 in stage 26.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 26.0 (TID 129)
13:25:18.042 [Executor task launch worker for task 7.0 in stage 26.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 26.0 (TID 131)
13:25:18.042 [Executor task launch worker for task 8.0 in stage 26.0 (TID 132)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 26.0 (TID 132)
13:25:18.042 [Executor task launch worker for task 6.0 in stage 26.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 26.0 (TID 130)
13:25:18.045 [Executor task launch worker for task 8.0 in stage 26.0 (TID 132)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505916400 locally
13:25:18.045 [Executor task launch worker for task 2.0 in stage 26.0 (TID 126)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505915200 locally
13:25:18.045 [Executor task launch worker for task 4.0 in stage 26.0 (TID 128)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505915600 locally
13:25:18.045 [Executor task launch worker for task 0.0 in stage 26.0 (TID 124)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505914800 locally
13:25:18.045 [Executor task launch worker for task 6.0 in stage 26.0 (TID 130)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505916000 locally
13:25:18.045 [Executor task launch worker for task 3.0 in stage 26.0 (TID 127)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505915400 locally
13:25:18.045 [Executor task launch worker for task 1.0 in stage 26.0 (TID 125)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505915000 locally
13:25:18.045 [Executor task launch worker for task 5.0 in stage 26.0 (TID 129)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505915800 locally
13:25:18.045 [Executor task launch worker for task 7.0 in stage 26.0 (TID 131)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505916200 locally
13:25:18.048 [Executor task launch worker for task 1.0 in stage 26.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 26.0 (TID 125). 1177 bytes result sent to driver
13:25:18.048 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 26.0 (TID 133) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.049 [Executor task launch worker for task 8.0 in stage 26.0 (TID 132)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 26.0 (TID 132). 1177 bytes result sent to driver
13:25:18.049 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 26.0 (TID 125) in 9 ms on 172.20.10.7 (executor driver) (1/15)
13:25:18.049 [Executor task launch worker for task 9.0 in stage 26.0 (TID 133)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 26.0 (TID 133)
13:25:18.049 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 26.0 (TID 134) (172.20.10.7, executor driver, partition 10, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.049 [Executor task launch worker for task 4.0 in stage 26.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 26.0 (TID 128). 1177 bytes result sent to driver
13:25:18.050 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 26.0 (TID 132) in 8 ms on 172.20.10.7 (executor driver) (2/15)
13:25:18.050 [Executor task launch worker for task 10.0 in stage 26.0 (TID 134)] INFO  org.apache.spark.executor.Executor - Running task 10.0 in stage 26.0 (TID 134)
13:25:18.050 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 26.0 (TID 135) (172.20.10.7, executor driver, partition 11, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.050 [Executor task launch worker for task 9.0 in stage 26.0 (TID 133)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505916600 locally
13:25:18.050 [Executor task launch worker for task 11.0 in stage 26.0 (TID 135)] INFO  org.apache.spark.executor.Executor - Running task 11.0 in stage 26.0 (TID 135)
13:25:18.050 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 26.0 (TID 128) in 10 ms on 172.20.10.7 (executor driver) (3/15)
13:25:18.051 [Executor task launch worker for task 0.0 in stage 26.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 124). 1177 bytes result sent to driver
13:25:18.051 [Executor task launch worker for task 10.0 in stage 26.0 (TID 134)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505916800 locally
13:25:18.051 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 26.0 (TID 136) (172.20.10.7, executor driver, partition 12, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.052 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 124) in 13 ms on 172.20.10.7 (executor driver) (4/15)
13:25:18.052 [Executor task launch worker for task 5.0 in stage 26.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 26.0 (TID 129). 1177 bytes result sent to driver
13:25:18.052 [Executor task launch worker for task 12.0 in stage 26.0 (TID 136)] INFO  org.apache.spark.executor.Executor - Running task 12.0 in stage 26.0 (TID 136)
13:25:18.052 [Executor task launch worker for task 11.0 in stage 26.0 (TID 135)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505917000 locally
13:25:18.052 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 26.0 (TID 137) (172.20.10.7, executor driver, partition 13, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.052 [Executor task launch worker for task 13.0 in stage 26.0 (TID 137)] INFO  org.apache.spark.executor.Executor - Running task 13.0 in stage 26.0 (TID 137)
13:25:18.052 [Executor task launch worker for task 3.0 in stage 26.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 26.0 (TID 127). 1177 bytes result sent to driver
13:25:18.053 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 26.0 (TID 129) in 11 ms on 172.20.10.7 (executor driver) (5/15)
13:25:18.053 [Executor task launch worker for task 6.0 in stage 26.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 26.0 (TID 130). 1177 bytes result sent to driver
13:25:18.053 [Executor task launch worker for task 13.0 in stage 26.0 (TID 137)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505917400 locally
13:25:18.053 [Executor task launch worker for task 12.0 in stage 26.0 (TID 136)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505917200 locally
13:25:18.053 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 26.0 (TID 138) (172.20.10.7, executor driver, partition 14, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:18.053 [Executor task launch worker for task 14.0 in stage 26.0 (TID 138)] INFO  org.apache.spark.executor.Executor - Running task 14.0 in stage 26.0 (TID 138)
13:25:18.054 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 26.0 (TID 127) in 14 ms on 172.20.10.7 (executor driver) (6/15)
13:25:18.054 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 26.0 (TID 130) in 13 ms on 172.20.10.7 (executor driver) (7/15)
13:25:18.054 [Executor task launch worker for task 2.0 in stage 26.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 26.0 (TID 126). 1177 bytes result sent to driver
13:25:18.054 [Executor task launch worker for task 14.0 in stage 26.0 (TID 138)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505917600 locally
13:25:18.054 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 26.0 (TID 126) in 14 ms on 172.20.10.7 (executor driver) (8/15)
13:25:18.054 [Executor task launch worker for task 7.0 in stage 26.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 26.0 (TID 131). 1177 bytes result sent to driver
13:25:18.055 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 26.0 (TID 131) in 14 ms on 172.20.10.7 (executor driver) (9/15)
13:25:18.055 [Executor task launch worker for task 12.0 in stage 26.0 (TID 136)] INFO  org.apache.spark.executor.Executor - Finished task 12.0 in stage 26.0 (TID 136). 1177 bytes result sent to driver
13:25:18.055 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 26.0 (TID 136) in 4 ms on 172.20.10.7 (executor driver) (10/15)
13:25:18.056 [Executor task launch worker for task 13.0 in stage 26.0 (TID 137)] INFO  org.apache.spark.executor.Executor - Finished task 13.0 in stage 26.0 (TID 137). 1134 bytes result sent to driver
13:25:18.056 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 26.0 (TID 137) in 4 ms on 172.20.10.7 (executor driver) (11/15)
13:25:18.057 [Executor task launch worker for task 11.0 in stage 26.0 (TID 135)] INFO  org.apache.spark.executor.Executor - Finished task 11.0 in stage 26.0 (TID 135). 1177 bytes result sent to driver
13:25:18.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 26.0 (TID 135) in 7 ms on 172.20.10.7 (executor driver) (12/15)
13:25:18.057 [Executor task launch worker for task 10.0 in stage 26.0 (TID 134)] INFO  org.apache.spark.executor.Executor - Finished task 10.0 in stage 26.0 (TID 134). 1177 bytes result sent to driver
13:25:18.058 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 26.0 (TID 134) in 9 ms on 172.20.10.7 (executor driver) (13/15)
13:25:18.058 [Executor task launch worker for task 9.0 in stage 26.0 (TID 133)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 26.0 (TID 133). 1177 bytes result sent to driver
13:25:18.058 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 26.0 (TID 133) in 10 ms on 172.20.10.7 (executor driver) (14/15)
13:25:18.058 [Executor task launch worker for task 14.0 in stage 26.0 (TID 138)] INFO  org.apache.spark.executor.Executor - Finished task 14.0 in stage 26.0 (TID 138). 1134 bytes result sent to driver
13:25:18.059 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 26.0 (TID 138) in 6 ms on 172.20.10.7 (executor driver) (15/15)
13:25:18.059 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool 
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 26 (mapToPair at sparkStreamingSocket.java:49) finished in 0.025 s
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 27)
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:18.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[41] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:18.060 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:18.060 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:18.061 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:18.061 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1513
13:25:18.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 27 (MapPartitionsRDD[41] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:18.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 10 tasks resource profile 0
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 139) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 140) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 141) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 27.0 (TID 142) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 27.0 (TID 143) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 27.0 (TID 144) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.062 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 27.0 (TID 145) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 27.0 (TID 146) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.063 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 27.0 (TID 147) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.063 [Executor task launch worker for task 1.0 in stage 27.0 (TID 140)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 140)
13:25:18.063 [Executor task launch worker for task 7.0 in stage 27.0 (TID 146)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 27.0 (TID 146)
13:25:18.063 [Executor task launch worker for task 3.0 in stage 27.0 (TID 142)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 27.0 (TID 142)
13:25:18.063 [Executor task launch worker for task 0.0 in stage 27.0 (TID 139)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 139)
13:25:18.063 [Executor task launch worker for task 2.0 in stage 27.0 (TID 141)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 141)
13:25:18.063 [Executor task launch worker for task 5.0 in stage 27.0 (TID 144)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 27.0 (TID 144)
13:25:18.063 [Executor task launch worker for task 4.0 in stage 27.0 (TID 143)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 27.0 (TID 143)
13:25:18.063 [Executor task launch worker for task 6.0 in stage 27.0 (TID 145)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 27.0 (TID 145)
13:25:18.063 [Executor task launch worker for task 8.0 in stage 27.0 (TID 147)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 27.0 (TID 147)
13:25:18.065 [Executor task launch worker for task 3.0 in stage 27.0 (TID 142)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 3.0 in stage 27.0 (TID 142)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 2.0 in stage 27.0 (TID 141)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 2.0 in stage 27.0 (TID 141)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 0.0 in stage 27.0 (TID 139)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 15 (1935.0 B) non-empty blocks including 15 (1935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 1.0 in stage 27.0 (TID 140)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 7.0 in stage 27.0 (TID 146)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 6.0 in stage 27.0 (TID 145)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 0.0 in stage 27.0 (TID 139)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 8.0 in stage 27.0 (TID 147)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 6.0 in stage 27.0 (TID 145)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 3.0 in stage 27.0 (TID 142)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 27.0 (TID 142). 1517 bytes result sent to driver
13:25:18.065 [Executor task launch worker for task 8.0 in stage 27.0 (TID 147)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 7.0 in stage 27.0 (TID 146)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.065 [Executor task launch worker for task 2.0 in stage 27.0 (TID 141)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 141). 1474 bytes result sent to driver
13:25:18.065 [Executor task launch worker for task 4.0 in stage 27.0 (TID 143)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.065 [Executor task launch worker for task 1.0 in stage 27.0 (TID 140)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.066 [Executor task launch worker for task 4.0 in stage 27.0 (TID 143)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.066 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 27.0 (TID 148) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.065 [Executor task launch worker for task 5.0 in stage 27.0 (TID 144)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.066 [Executor task launch worker for task 8.0 in stage 27.0 (TID 147)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 27.0 (TID 147). 1517 bytes result sent to driver
13:25:18.066 [Executor task launch worker for task 5.0 in stage 27.0 (TID 144)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.066 [Executor task launch worker for task 7.0 in stage 27.0 (TID 146)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 27.0 (TID 146). 1517 bytes result sent to driver
13:25:18.066 [Executor task launch worker for task 6.0 in stage 27.0 (TID 145)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 27.0 (TID 145). 1517 bytes result sent to driver
13:25:18.066 [Executor task launch worker for task 1.0 in stage 27.0 (TID 140)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 140). 1517 bytes result sent to driver
13:25:18.066 [Executor task launch worker for task 4.0 in stage 27.0 (TID 143)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 27.0 (TID 143). 1517 bytes result sent to driver
13:25:18.066 [Executor task launch worker for task 9.0 in stage 27.0 (TID 148)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 27.0 (TID 148)
13:25:18.066 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 141) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:18.066 [Executor task launch worker for task 5.0 in stage 27.0 (TID 144)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 27.0 (TID 144). 1517 bytes result sent to driver
13:25:18.066 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 27.0 (TID 147) in 3 ms on 172.20.10.7 (executor driver) (2/10)
13:25:18.066 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 27.0 (TID 142) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:25:18.067 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 27.0 (TID 146) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:18.067 [Executor task launch worker for task 9.0 in stage 27.0 (TID 148)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.067 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 140) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:18.067 [Executor task launch worker for task 9.0 in stage 27.0 (TID 148)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 27.0 (TID 145) in 5 ms on 172.20.10.7 (executor driver) (6/10)
13:25:18.067 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 27.0 (TID 143) in 5 ms on 172.20.10.7 (executor driver) (7/10)
13:25:18.068 [Executor task launch worker for task 9.0 in stage 27.0 (TID 148)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 27.0 (TID 148). 1431 bytes result sent to driver
13:25:18.068 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 27.0 (TID 144) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:25:18.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 27.0 (TID 148) in 2 ms on 172.20.10.7 (executor driver) (9/10)
13:25:18.070 [Executor task launch worker for task 0.0 in stage 27.0 (TID 139)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 139). 1520 bytes result sent to driver
13:25:18.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 139) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:18.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
13:25:18.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (sortByKey at sparkStreamingSocket.java:61) finished in 0.010 s
13:25:18.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:18.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 27: Stage finished
13:25:18.070 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: sortByKey at sparkStreamingSocket.java:61, took 0.042429 s
13:25:18.074 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:18.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 39 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 11
13:25:18.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 12 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:18.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 (collect at sparkStreamingSocket.java:61)
13:25:18.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 29)
13:25:18.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 29)
13:25:18.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 29 (ShuffledRDD[39] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:18.075 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:18.076 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:18.076 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:18.076 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1513
13:25:18.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 29 (ShuffledRDD[39] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:18.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 10 tasks resource profile 0
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 149) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 150) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 151) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 152) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 29.0 (TID 153) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 29.0 (TID 154) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 29.0 (TID 155) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 29.0 (TID 156) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.078 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 29.0 (TID 157) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.078 [Executor task launch worker for task 2.0 in stage 29.0 (TID 151)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 151)
13:25:18.078 [Executor task launch worker for task 6.0 in stage 29.0 (TID 155)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 29.0 (TID 155)
13:25:18.078 [Executor task launch worker for task 8.0 in stage 29.0 (TID 157)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 29.0 (TID 157)
13:25:18.078 [Executor task launch worker for task 3.0 in stage 29.0 (TID 152)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 152)
13:25:18.078 [Executor task launch worker for task 0.0 in stage 29.0 (TID 149)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 149)
13:25:18.078 [Executor task launch worker for task 5.0 in stage 29.0 (TID 154)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 29.0 (TID 154)
13:25:18.078 [Executor task launch worker for task 4.0 in stage 29.0 (TID 153)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 29.0 (TID 153)
13:25:18.078 [Executor task launch worker for task 1.0 in stage 29.0 (TID 150)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 150)
13:25:18.078 [Executor task launch worker for task 7.0 in stage 29.0 (TID 156)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 29.0 (TID 156)
13:25:18.079 [Executor task launch worker for task 2.0 in stage 29.0 (TID 151)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 0.0 in stage 29.0 (TID 149)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 15 (1935.0 B) non-empty blocks including 15 (1935.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 2.0 in stage 29.0 (TID 151)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 0.0 in stage 29.0 (TID 149)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 1.0 in stage 29.0 (TID 150)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 5.0 in stage 29.0 (TID 154)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 3.0 in stage 29.0 (TID 152)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 1.0 in stage 29.0 (TID 150)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 7.0 in stage 29.0 (TID 156)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 5.0 in stage 29.0 (TID 154)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 8.0 in stage 29.0 (TID 157)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 7.0 in stage 29.0 (TID 156)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:18.080 [Executor task launch worker for task 6.0 in stage 29.0 (TID 155)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 3.0 in stage 29.0 (TID 152)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 8.0 in stage 29.0 (TID 157)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 6.0 in stage 29.0 (TID 155)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.080 [Executor task launch worker for task 2.0 in stage 29.0 (TID 151)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 151). 1297 bytes result sent to driver
13:25:18.080 [Executor task launch worker for task 4.0 in stage 29.0 (TID 153)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.080 [Executor task launch worker for task 4.0 in stage 29.0 (TID 153)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.081 [Executor task launch worker for task 1.0 in stage 29.0 (TID 150)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 150). 1340 bytes result sent to driver
13:25:18.081 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 29.0 (TID 158) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:18.081 [Executor task launch worker for task 8.0 in stage 29.0 (TID 157)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 29.0 (TID 157). 1297 bytes result sent to driver
13:25:18.081 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 151) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:18.081 [Executor task launch worker for task 9.0 in stage 29.0 (TID 158)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 29.0 (TID 158)
13:25:18.081 [Executor task launch worker for task 6.0 in stage 29.0 (TID 155)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 29.0 (TID 155). 1297 bytes result sent to driver
13:25:18.081 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 150) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:18.082 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 29.0 (TID 157) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:18.082 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 29.0 (TID 155) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:18.082 [Executor task launch worker for task 7.0 in stage 29.0 (TID 156)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 29.0 (TID 156). 1340 bytes result sent to driver
13:25:18.082 [Executor task launch worker for task 9.0 in stage 29.0 (TID 158)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.082 [Executor task launch worker for task 9.0 in stage 29.0 (TID 158)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.082 [Executor task launch worker for task 3.0 in stage 29.0 (TID 152)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 152). 1340 bytes result sent to driver
13:25:18.082 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 29.0 (TID 156) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:18.082 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 152) in 5 ms on 172.20.10.7 (executor driver) (6/10)
13:25:18.083 [Executor task launch worker for task 5.0 in stage 29.0 (TID 154)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 29.0 (TID 154). 1340 bytes result sent to driver
13:25:18.083 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 29.0 (TID 154) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:25:18.083 [Executor task launch worker for task 9.0 in stage 29.0 (TID 158)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 29.0 (TID 158). 1297 bytes result sent to driver
13:25:18.083 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 29.0 (TID 158) in 2 ms on 172.20.10.7 (executor driver) (8/10)
13:25:18.084 [Executor task launch worker for task 4.0 in stage 29.0 (TID 153)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 29.0 (TID 153). 1340 bytes result sent to driver
13:25:18.084 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 29.0 (TID 153) in 7 ms on 172.20.10.7 (executor driver) (9/10)
13:25:18.084 [Executor task launch worker for task 0.0 in stage 29.0 (TID 149)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 149). 1426 bytes result sent to driver
13:25:18.084 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 149) in 7 ms on 172.20.10.7 (executor driver) (10/10)
13:25:18.085 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 29 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.010 s
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 30)
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (ShuffledRDD[42] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:18.086 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:18.086 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:18.086 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1513
13:25:18.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (ShuffledRDD[42] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:18.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0
13:25:18.087 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 159) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:18.087 [Executor task launch worker for task 0.0 in stage 30.0 (TID 159)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 159)
13:25:18.087 [Executor task launch worker for task 0.0 in stage 30.0 (TID 159)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:18.087 [Executor task launch worker for task 0.0 in stage 30.0 (TID 159)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:18.088 [Executor task launch worker for task 0.0 in stage 30.0 (TID 159)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 159). 1330 bytes result sent to driver
13:25:18.088 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 159) in 2 ms on 172.20.10.7 (executor driver) (1/1)
13:25:18.088 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool 
13:25:18.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 (collect at sparkStreamingSocket.java:61) finished in 0.003 s
13:25:18.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:18.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished
13:25:18.088 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 12 finished: collect at sparkStreamingSocket.java:61, took 0.014469 s
13:25:18.089 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505918000 ms.0 from job set of time 1760505918000 ms
13:25:18.089 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.089 s for time 1760505918000 ms (execution: 0.071 s)
13:25:18.089 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 32 from persistence list
13:25:18.089 [block-manager-storage-async-thread-pool-49] INFO  org.apache.spark.storage.BlockManager - Removing RDD 32
13:25:18.089 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 31 from persistence list
13:25:18.089 [block-manager-storage-async-thread-pool-53] INFO  org.apache.spark.storage.BlockManager - Removing RDD 31
13:25:18.089 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 30 from persistence list
13:25:18.089 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 29 from persistence list
13:25:18.089 [block-manager-storage-async-thread-pool-57] INFO  org.apache.spark.storage.BlockManager - Removing RDD 30
13:25:18.089 [block-manager-storage-async-thread-pool-50] INFO  org.apache.spark.storage.BlockManager - Removing RDD 29
13:25:18.090 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[29] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505918000 ms
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505913200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505913400 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505913600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505913800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505914200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.090 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505912000 ms
13:25:18.090 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505912000 ms
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505914400 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.5 MiB)
13:25:18.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505914600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:18.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505918000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:18.203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505918000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:18.203 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:18.203 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505918000 replicated to only 0 peer(s) instead of 1 peers
13:25:18.203 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505918000
13:25:18.406 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505918200 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:18.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505918200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:18.408 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:18.408 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505918200 replicated to only 0 peer(s) instead of 1 peers
13:25:18.409 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505918200
13:25:18.602 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505918400 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:18.603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505918400 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.5 MiB)
13:25:18.604 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:18.604 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505918400 replicated to only 0 peer(s) instead of 1 peers
13:25:18.604 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505918400
13:25:18.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505918600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:18.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505918600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:18.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:18.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505918600 replicated to only 0 peer(s) instead of 1 peers
13:25:18.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505918600
13:25:19.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505918800 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:25:19.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505918800 in memory on 172.20.10.7:49875 (size: 10.0 B, free: 2004.5 MiB)
13:25:19.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:19.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505918800 replicated to only 0 peer(s) instead of 1 peers
13:25:19.005 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505918800
13:25:19.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505919000 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:19.203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505919000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:19.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:19.204 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505919000 replicated to only 0 peer(s) instead of 1 peers
13:25:19.204 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505919000
13:25:21.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505921000 ms
13:25:21.012 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505921000 ms.0 from job set of time 1760505921000 ms
13:25:21.023 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:21.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 45 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 12
13:25:21.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 13 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:21.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 32 (sortByKey at sparkStreamingSocket.java:61)
13:25:21.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 31)
13:25:21.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 31)
13:25:21.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 31 (MapPartitionsRDD[45] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:21.027 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:21.028 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:21.028 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.5 MiB)
13:25:21.028 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1513
13:25:21.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 7 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[45] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
13:25:21.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 7 tasks resource profile 0
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 160) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 31.0 (TID 161) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 31.0 (TID 162) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 31.0 (TID 163) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 31.0 (TID 164) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 31.0 (TID 165) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.031 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 31.0 (TID 166) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:21.031 [Executor task launch worker for task 1.0 in stage 31.0 (TID 161)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 31.0 (TID 161)
13:25:21.031 [Executor task launch worker for task 0.0 in stage 31.0 (TID 160)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 160)
13:25:21.031 [Executor task launch worker for task 5.0 in stage 31.0 (TID 165)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 31.0 (TID 165)
13:25:21.031 [Executor task launch worker for task 6.0 in stage 31.0 (TID 166)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 31.0 (TID 166)
13:25:21.031 [Executor task launch worker for task 4.0 in stage 31.0 (TID 164)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 31.0 (TID 164)
13:25:21.031 [Executor task launch worker for task 3.0 in stage 31.0 (TID 163)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 31.0 (TID 163)
13:25:21.031 [Executor task launch worker for task 2.0 in stage 31.0 (TID 162)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 31.0 (TID 162)
13:25:21.033 [Executor task launch worker for task 1.0 in stage 31.0 (TID 161)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505918000 locally
13:25:21.033 [Executor task launch worker for task 4.0 in stage 31.0 (TID 164)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505918600 locally
13:25:21.033 [Executor task launch worker for task 5.0 in stage 31.0 (TID 165)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505918800 locally
13:25:21.033 [Executor task launch worker for task 0.0 in stage 31.0 (TID 160)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505917800 locally
13:25:21.033 [Executor task launch worker for task 6.0 in stage 31.0 (TID 166)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505919000 locally
13:25:21.033 [Executor task launch worker for task 3.0 in stage 31.0 (TID 163)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505918400 locally
13:25:21.033 [Executor task launch worker for task 2.0 in stage 31.0 (TID 162)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505918200 locally
13:25:21.035 [Executor task launch worker for task 4.0 in stage 31.0 (TID 164)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 31.0 (TID 164). 1177 bytes result sent to driver
13:25:21.035 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 31.0 (TID 164) in 5 ms on 172.20.10.7 (executor driver) (1/7)
13:25:21.036 [Executor task launch worker for task 3.0 in stage 31.0 (TID 163)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 31.0 (TID 163). 1177 bytes result sent to driver
13:25:21.036 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 31.0 (TID 163) in 6 ms on 172.20.10.7 (executor driver) (2/7)
13:25:21.036 [Executor task launch worker for task 5.0 in stage 31.0 (TID 165)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 31.0 (TID 165). 1177 bytes result sent to driver
13:25:21.036 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 31.0 (TID 165) in 6 ms on 172.20.10.7 (executor driver) (3/7)
13:25:21.037 [Executor task launch worker for task 6.0 in stage 31.0 (TID 166)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 31.0 (TID 166). 1177 bytes result sent to driver
13:25:21.037 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 31.0 (TID 166) in 7 ms on 172.20.10.7 (executor driver) (4/7)
13:25:21.037 [Executor task launch worker for task 0.0 in stage 31.0 (TID 160)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 160). 1177 bytes result sent to driver
13:25:21.038 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 160) in 8 ms on 172.20.10.7 (executor driver) (5/7)
13:25:21.038 [Executor task launch worker for task 1.0 in stage 31.0 (TID 161)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 31.0 (TID 161). 1177 bytes result sent to driver
13:25:21.039 [Executor task launch worker for task 2.0 in stage 31.0 (TID 162)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 31.0 (TID 162). 1177 bytes result sent to driver
13:25:21.039 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 31.0 (TID 161) in 9 ms on 172.20.10.7 (executor driver) (6/7)
13:25:21.039 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 31.0 (TID 162) in 9 ms on 172.20.10.7 (executor driver) (7/7)
13:25:21.039 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool 
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 31 (mapToPair at sparkStreamingSocket.java:49) finished in 0.013 s
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 32)
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:21.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 32 (MapPartitionsRDD[48] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:21.041 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:21.045 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:21.047 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:21.047 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 25 from broadcast at DAGScheduler.scala:1513
13:25:21.048 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:21.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 32 (MapPartitionsRDD[48] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:21.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 32.0 with 10 tasks resource profile 0
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 32.0 (TID 167) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 32.0 (TID 168) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.5 MiB)
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 32.0 (TID 169) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 32.0 (TID 170) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 32.0 (TID 171) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 32.0 (TID 172) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 32.0 (TID 173) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 32.0 (TID 174) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 32.0 (TID 175) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.049 [Executor task launch worker for task 3.0 in stage 32.0 (TID 170)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 32.0 (TID 170)
13:25:21.049 [Executor task launch worker for task 1.0 in stage 32.0 (TID 168)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 32.0 (TID 168)
13:25:21.049 [Executor task launch worker for task 5.0 in stage 32.0 (TID 172)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 32.0 (TID 172)
13:25:21.049 [Executor task launch worker for task 2.0 in stage 32.0 (TID 169)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 32.0 (TID 169)
13:25:21.049 [Executor task launch worker for task 4.0 in stage 32.0 (TID 171)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 32.0 (TID 171)
13:25:21.049 [Executor task launch worker for task 8.0 in stage 32.0 (TID 175)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 32.0 (TID 175)
13:25:21.049 [Executor task launch worker for task 0.0 in stage 32.0 (TID 167)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 32.0 (TID 167)
13:25:21.049 [Executor task launch worker for task 7.0 in stage 32.0 (TID 174)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 32.0 (TID 174)
13:25:21.050 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:21.049 [Executor task launch worker for task 6.0 in stage 32.0 (TID 173)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 32.0 (TID 173)
13:25:21.051 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:21.051 [Executor task launch worker for task 4.0 in stage 32.0 (TID 171)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 0.0 in stage 32.0 (TID 167)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 7 (903.0 B) non-empty blocks including 7 (903.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 2.0 in stage 32.0 (TID 169)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 4.0 in stage 32.0 (TID 171)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.051 [Executor task launch worker for task 1.0 in stage 32.0 (TID 168)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 5.0 in stage 32.0 (TID 172)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 3.0 in stage 32.0 (TID 170)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 8.0 in stage 32.0 (TID 175)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.051 [Executor task launch worker for task 0.0 in stage 32.0 (TID 167)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 5.0 in stage 32.0 (TID 172)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 6.0 in stage 32.0 (TID 173)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.052 [Executor task launch worker for task 6.0 in stage 32.0 (TID 173)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 7.0 in stage 32.0 (TID 174)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.052 [Executor task launch worker for task 4.0 in stage 32.0 (TID 171)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 32.0 (TID 171). 1474 bytes result sent to driver
13:25:21.052 [Executor task launch worker for task 7.0 in stage 32.0 (TID 174)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 8.0 in stage 32.0 (TID 175)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 2.0 in stage 32.0 (TID 169)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 3.0 in stage 32.0 (TID 170)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 7.0 in stage 32.0 (TID 174)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 32.0 (TID 174). 1517 bytes result sent to driver
13:25:21.052 [Executor task launch worker for task 5.0 in stage 32.0 (TID 172)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 32.0 (TID 172). 1474 bytes result sent to driver
13:25:21.052 [Executor task launch worker for task 1.0 in stage 32.0 (TID 168)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.052 [Executor task launch worker for task 6.0 in stage 32.0 (TID 173)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 32.0 (TID 173). 1517 bytes result sent to driver
13:25:21.052 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 32.0 (TID 176) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.052 [Executor task launch worker for task 3.0 in stage 32.0 (TID 170)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 32.0 (TID 170). 1517 bytes result sent to driver
13:25:21.053 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 32.0 (TID 171) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:21.053 [Executor task launch worker for task 1.0 in stage 32.0 (TID 168)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 32.0 (TID 168). 1517 bytes result sent to driver
13:25:21.053 [Executor task launch worker for task 2.0 in stage 32.0 (TID 169)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 32.0 (TID 169). 1517 bytes result sent to driver
13:25:21.053 [Executor task launch worker for task 9.0 in stage 32.0 (TID 176)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 32.0 (TID 176)
13:25:21.053 [Executor task launch worker for task 8.0 in stage 32.0 (TID 175)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 32.0 (TID 175). 1517 bytes result sent to driver
13:25:21.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.5 MiB)
13:25:21.054 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 32.0 (TID 174) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:21.054 [Executor task launch worker for task 0.0 in stage 32.0 (TID 167)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 32.0 (TID 167). 1520 bytes result sent to driver
13:25:21.054 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 32.0 (TID 172) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:21.055 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 32.0 (TID 173) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:21.055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:25:21.055 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 32.0 (TID 170) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:21.055 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 32.0 (TID 168) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:21.055 [Executor task launch worker for task 9.0 in stage 32.0 (TID 176)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.055 [Executor task launch worker for task 9.0 in stage 32.0 (TID 176)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.055 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 32.0 (TID 169) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:25:21.055 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 32.0 (TID 175) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:25:21.056 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 32.0 (TID 167) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:25:21.056 [Executor task launch worker for task 9.0 in stage 32.0 (TID 176)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 32.0 (TID 176). 1474 bytes result sent to driver
13:25:21.056 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 32.0 (TID 176) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:25:21.056 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 32.0, whose tasks have all completed, from pool 
13:25:21.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 32 (sortByKey at sparkStreamingSocket.java:61) finished in 0.015 s
13:25:21.057 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:21.057 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 32: Stage finished
13:25:21.057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:21.057 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 finished: sortByKey at sparkStreamingSocket.java:61, took 0.033847 s
13:25:21.057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:21.061 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:21.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 46 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 13
13:25:21.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 14 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:21.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 35 (collect at sparkStreamingSocket.java:61)
13:25:21.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 34)
13:25:21.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 34)
13:25:21.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 34 (ShuffledRDD[46] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:21.062 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:21.063 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:21.063 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.6 MiB)
13:25:21.063 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1513
13:25:21.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 34 (ShuffledRDD[46] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:21.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 10 tasks resource profile 0
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 177) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 34.0 (TID 178) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 34.0 (TID 179) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 34.0 (TID 180) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 34.0 (TID 181) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 34.0 (TID 182) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 34.0 (TID 183) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 34.0 (TID 184) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 34.0 (TID 185) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.064 [Executor task launch worker for task 3.0 in stage 34.0 (TID 180)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 34.0 (TID 180)
13:25:21.064 [Executor task launch worker for task 7.0 in stage 34.0 (TID 184)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 34.0 (TID 184)
13:25:21.064 [Executor task launch worker for task 6.0 in stage 34.0 (TID 183)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 34.0 (TID 183)
13:25:21.064 [Executor task launch worker for task 4.0 in stage 34.0 (TID 181)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 34.0 (TID 181)
13:25:21.064 [Executor task launch worker for task 5.0 in stage 34.0 (TID 182)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 34.0 (TID 182)
13:25:21.064 [Executor task launch worker for task 1.0 in stage 34.0 (TID 178)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 34.0 (TID 178)
13:25:21.064 [Executor task launch worker for task 8.0 in stage 34.0 (TID 185)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 34.0 (TID 185)
13:25:21.064 [Executor task launch worker for task 0.0 in stage 34.0 (TID 177)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 177)
13:25:21.064 [Executor task launch worker for task 2.0 in stage 34.0 (TID 179)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 34.0 (TID 179)
13:25:21.066 [Executor task launch worker for task 4.0 in stage 34.0 (TID 181)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.066 [Executor task launch worker for task 4.0 in stage 34.0 (TID 181)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:21.066 [Executor task launch worker for task 7.0 in stage 34.0 (TID 184)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.066 [Executor task launch worker for task 7.0 in stage 34.0 (TID 184)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.066 [Executor task launch worker for task 2.0 in stage 34.0 (TID 179)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.066 [Executor task launch worker for task 2.0 in stage 34.0 (TID 179)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.066 [Executor task launch worker for task 3.0 in stage 34.0 (TID 180)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.066 [Executor task launch worker for task 5.0 in stage 34.0 (TID 182)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.066 [Executor task launch worker for task 5.0 in stage 34.0 (TID 182)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.066 [Executor task launch worker for task 3.0 in stage 34.0 (TID 180)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.067 [Executor task launch worker for task 8.0 in stage 34.0 (TID 185)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.067 [Executor task launch worker for task 8.0 in stage 34.0 (TID 185)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.067 [Executor task launch worker for task 0.0 in stage 34.0 (TID 177)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 7 (903.0 B) non-empty blocks including 7 (903.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.067 [Executor task launch worker for task 0.0 in stage 34.0 (TID 177)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.067 [Executor task launch worker for task 5.0 in stage 34.0 (TID 182)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 34.0 (TID 182). 1297 bytes result sent to driver
13:25:21.067 [Executor task launch worker for task 4.0 in stage 34.0 (TID 181)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 34.0 (TID 181). 1297 bytes result sent to driver
13:25:21.067 [Executor task launch worker for task 6.0 in stage 34.0 (TID 183)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.067 [Executor task launch worker for task 6.0 in stage 34.0 (TID 183)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.067 [Executor task launch worker for task 1.0 in stage 34.0 (TID 178)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.068 [Executor task launch worker for task 1.0 in stage 34.0 (TID 178)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.068 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 34.0 (TID 186) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:21.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 34.0 (TID 182) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:21.068 [Executor task launch worker for task 7.0 in stage 34.0 (TID 184)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 34.0 (TID 184). 1340 bytes result sent to driver
13:25:21.068 [Executor task launch worker for task 9.0 in stage 34.0 (TID 186)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 34.0 (TID 186)
13:25:21.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 34.0 (TID 181) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:21.068 [Executor task launch worker for task 2.0 in stage 34.0 (TID 179)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 34.0 (TID 179). 1340 bytes result sent to driver
13:25:21.069 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 34.0 (TID 184) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:21.069 [Executor task launch worker for task 1.0 in stage 34.0 (TID 178)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 34.0 (TID 178). 1340 bytes result sent to driver
13:25:21.069 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 34.0 (TID 179) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:21.069 [Executor task launch worker for task 6.0 in stage 34.0 (TID 183)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 34.0 (TID 183). 1297 bytes result sent to driver
13:25:21.069 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 34.0 (TID 178) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:21.070 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 34.0 (TID 183) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:21.070 [Executor task launch worker for task 8.0 in stage 34.0 (TID 185)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 34.0 (TID 185). 1340 bytes result sent to driver
13:25:21.070 [Executor task launch worker for task 9.0 in stage 34.0 (TID 186)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.070 [Executor task launch worker for task 9.0 in stage 34.0 (TID 186)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.070 [Executor task launch worker for task 3.0 in stage 34.0 (TID 180)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 34.0 (TID 180). 1340 bytes result sent to driver
13:25:21.070 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 34.0 (TID 185) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:25:21.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 34.0 (TID 180) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:25:21.071 [Executor task launch worker for task 9.0 in stage 34.0 (TID 186)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 34.0 (TID 186). 1340 bytes result sent to driver
13:25:21.071 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 34.0 (TID 186) in 3 ms on 172.20.10.7 (executor driver) (9/10)
13:25:21.071 [Executor task launch worker for task 0.0 in stage 34.0 (TID 177)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 177). 1469 bytes result sent to driver
13:25:21.071 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 177) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:21.071 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool 
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 34 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.009 s
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 35)
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:21.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 35 (ShuffledRDD[49] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:21.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:21.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:21.072 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.6 MiB)
13:25:21.072 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1513
13:25:21.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 35 (ShuffledRDD[49] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:21.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 1 tasks resource profile 0
13:25:21.073 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 187) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:21.073 [Executor task launch worker for task 0.0 in stage 35.0 (TID 187)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 187)
13:25:21.074 [Executor task launch worker for task 0.0 in stage 35.0 (TID 187)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:21.074 [Executor task launch worker for task 0.0 in stage 35.0 (TID 187)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:21.074 [Executor task launch worker for task 0.0 in stage 35.0 (TID 187)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 187). 1330 bytes result sent to driver
13:25:21.074 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 187) in 1 ms on 172.20.10.7 (executor driver) (1/1)
13:25:21.074 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool 
13:25:21.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 35 (collect at sparkStreamingSocket.java:61) finished in 0.003 s
13:25:21.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:21.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 35: Stage finished
13:25:21.075 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 finished: collect at sparkStreamingSocket.java:61, took 0.014515 s
13:25:21.076 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505921000 ms.0 from job set of time 1760505921000 ms
13:25:21.076 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.076 s for time 1760505921000 ms (execution: 0.064 s)
13:25:21.076 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 39 from persistence list
13:25:21.076 [block-manager-storage-async-thread-pool-16] INFO  org.apache.spark.storage.BlockManager - Removing RDD 39
13:25:21.076 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 38 from persistence list
13:25:21.076 [block-manager-storage-async-thread-pool-14] INFO  org.apache.spark.storage.BlockManager - Removing RDD 38
13:25:21.076 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 37 from persistence list
13:25:21.076 [block-manager-storage-async-thread-pool-19] INFO  org.apache.spark.storage.BlockManager - Removing RDD 37
13:25:21.076 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 36 from persistence list
13:25:21.076 [block-manager-storage-async-thread-pool-26] INFO  org.apache.spark.storage.BlockManager - Removing RDD 36
13:25:21.076 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[36] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505921000 ms
13:25:21.077 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505914800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.077 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505915000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.077 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505915200 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.6 MiB)
13:25:21.077 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505915400 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.078 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505915600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.078 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505915800 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.6 MiB)
13:25:21.078 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505916000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.078 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505916200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.079 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505916400 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.079 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505916600 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.6 MiB)
13:25:21.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505916800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505917000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505917200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:21.080 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505915000 ms
13:25:21.080 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505915000 ms
13:25:21.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505917400 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.6 MiB)
13:25:21.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505917600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.6 MiB)
13:25:24.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505924000 ms
13:25:24.012 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505924000 ms.0 from job set of time 1760505924000 ms
13:25:24.021 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:24.022 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 52 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 14
13:25:24.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 15 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:24.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 37 (sortByKey at sparkStreamingSocket.java:61)
13:25:24.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 36)
13:25:24.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:24.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 37 (MapPartitionsRDD[55] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:24.025 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:24.026 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:24.026 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:24.026 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1513
13:25:24.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 37 (MapPartitionsRDD[55] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:24.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 37.0 with 10 tasks resource profile 0
13:25:24.028 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 37.0 (TID 188) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.028 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 37.0 (TID 189) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.028 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 37.0 (TID 190) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.028 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 37.0 (TID 191) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.028 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 37.0 (TID 192) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.029 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 37.0 (TID 193) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.029 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 37.0 (TID 194) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.029 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 37.0 (TID 195) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.029 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 37.0 (TID 196) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.029 [Executor task launch worker for task 0.0 in stage 37.0 (TID 188)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 37.0 (TID 188)
13:25:24.029 [Executor task launch worker for task 1.0 in stage 37.0 (TID 189)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 37.0 (TID 189)
13:25:24.029 [Executor task launch worker for task 2.0 in stage 37.0 (TID 190)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 37.0 (TID 190)
13:25:24.029 [Executor task launch worker for task 3.0 in stage 37.0 (TID 191)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 37.0 (TID 191)
13:25:24.029 [Executor task launch worker for task 7.0 in stage 37.0 (TID 195)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 37.0 (TID 195)
13:25:24.029 [Executor task launch worker for task 8.0 in stage 37.0 (TID 196)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 37.0 (TID 196)
13:25:24.029 [Executor task launch worker for task 5.0 in stage 37.0 (TID 193)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 37.0 (TID 193)
13:25:24.029 [Executor task launch worker for task 4.0 in stage 37.0 (TID 192)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 37.0 (TID 192)
13:25:24.029 [Executor task launch worker for task 6.0 in stage 37.0 (TID 194)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 37.0 (TID 194)
13:25:24.032 [Executor task launch worker for task 5.0 in stage 37.0 (TID 193)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 4.0 in stage 37.0 (TID 192)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 2.0 in stage 37.0 (TID 190)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 7.0 in stage 37.0 (TID 195)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 2.0 in stage 37.0 (TID 190)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 6.0 in stage 37.0 (TID 194)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 0.0 in stage 37.0 (TID 188)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 3.0 in stage 37.0 (TID 191)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 4.0 in stage 37.0 (TID 192)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 3.0 in stage 37.0 (TID 191)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 1.0 in stage 37.0 (TID 189)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.032 [Executor task launch worker for task 1.0 in stage 37.0 (TID 189)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 5.0 in stage 37.0 (TID 193)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 6.0 in stage 37.0 (TID 194)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 7.0 in stage 37.0 (TID 195)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.032 [Executor task launch worker for task 0.0 in stage 37.0 (TID 188)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.033 [Executor task launch worker for task 6.0 in stage 37.0 (TID 194)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 37.0 (TID 194). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 7.0 in stage 37.0 (TID 195)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 37.0 (TID 195). 1517 bytes result sent to driver
13:25:24.032 [Executor task launch worker for task 8.0 in stage 37.0 (TID 196)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.033 [Executor task launch worker for task 5.0 in stage 37.0 (TID 193)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 37.0 (TID 193). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 8.0 in stage 37.0 (TID 196)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.033 [Executor task launch worker for task 4.0 in stage 37.0 (TID 192)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 37.0 (TID 192). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 1.0 in stage 37.0 (TID 189)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 37.0 (TID 189). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 3.0 in stage 37.0 (TID 191)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 37.0 (TID 191). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 2.0 in stage 37.0 (TID 190)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 37.0 (TID 190). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 0.0 in stage 37.0 (TID 188)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 37.0 (TID 188). 1517 bytes result sent to driver
13:25:24.033 [Executor task launch worker for task 8.0 in stage 37.0 (TID 196)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 37.0 (TID 196). 1517 bytes result sent to driver
13:25:24.033 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 37.0 (TID 197) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.034 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 37.0 (TID 194) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:25:24.034 [Executor task launch worker for task 9.0 in stage 37.0 (TID 197)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 37.0 (TID 197)
13:25:24.034 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 37.0 (TID 195) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:25:24.034 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 37.0 (TID 193) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:24.034 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 37.0 (TID 192) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:24.035 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 37.0 (TID 189) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:25:24.035 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 37.0 (TID 190) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:25:24.035 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 37.0 (TID 191) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:24.035 [Executor task launch worker for task 9.0 in stage 37.0 (TID 197)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.035 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 37.0 (TID 188) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:25:24.035 [Executor task launch worker for task 9.0 in stage 37.0 (TID 197)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.035 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 37.0 (TID 196) in 6 ms on 172.20.10.7 (executor driver) (9/10)
13:25:24.036 [Executor task launch worker for task 9.0 in stage 37.0 (TID 197)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 37.0 (TID 197). 1517 bytes result sent to driver
13:25:24.036 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 37.0 (TID 197) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:25:24.036 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 37.0, whose tasks have all completed, from pool 
13:25:24.036 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 37 (sortByKey at sparkStreamingSocket.java:61) finished in 0.012 s
13:25:24.037 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:24.037 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 37: Stage finished
13:25:24.037 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 finished: sortByKey at sparkStreamingSocket.java:61, took 0.015839 s
13:25:24.042 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:24.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 53 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 15
13:25:24.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 16 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:24.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 40 (collect at sparkStreamingSocket.java:61)
13:25:24.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 39)
13:25:24.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 39)
13:25:24.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 39 (ShuffledRDD[53] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:24.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:24.044 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:24.044 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:24.044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1513
13:25:24.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 39 (ShuffledRDD[53] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:24.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 39.0 with 10 tasks resource profile 0
13:25:24.045 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 39.0 (TID 198) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.045 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 39.0 (TID 199) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.045 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 39.0 (TID 200) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.045 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 39.0 (TID 201) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 39.0 (TID 202) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 39.0 (TID 203) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 39.0 (TID 204) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 39.0 (TID 205) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 39.0 (TID 206) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.046 [Executor task launch worker for task 0.0 in stage 39.0 (TID 198)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 39.0 (TID 198)
13:25:24.046 [Executor task launch worker for task 4.0 in stage 39.0 (TID 202)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 39.0 (TID 202)
13:25:24.046 [Executor task launch worker for task 1.0 in stage 39.0 (TID 199)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 39.0 (TID 199)
13:25:24.046 [Executor task launch worker for task 2.0 in stage 39.0 (TID 200)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 39.0 (TID 200)
13:25:24.046 [Executor task launch worker for task 7.0 in stage 39.0 (TID 205)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 39.0 (TID 205)
13:25:24.046 [Executor task launch worker for task 8.0 in stage 39.0 (TID 206)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 39.0 (TID 206)
13:25:24.046 [Executor task launch worker for task 6.0 in stage 39.0 (TID 204)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 39.0 (TID 204)
13:25:24.046 [Executor task launch worker for task 3.0 in stage 39.0 (TID 201)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 39.0 (TID 201)
13:25:24.046 [Executor task launch worker for task 5.0 in stage 39.0 (TID 203)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 39.0 (TID 203)
13:25:24.048 [Executor task launch worker for task 6.0 in stage 39.0 (TID 204)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 8.0 in stage 39.0 (TID 206)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 6.0 in stage 39.0 (TID 204)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 8.0 in stage 39.0 (TID 206)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 1.0 in stage 39.0 (TID 199)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 1.0 in stage 39.0 (TID 199)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 0.0 in stage 39.0 (TID 198)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 0.0 in stage 39.0 (TID 198)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 7.0 in stage 39.0 (TID 205)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 5.0 in stage 39.0 (TID 203)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 3.0 in stage 39.0 (TID 201)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 2.0 in stage 39.0 (TID 200)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 4.0 in stage 39.0 (TID 202)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.048 [Executor task launch worker for task 4.0 in stage 39.0 (TID 202)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 2.0 in stage 39.0 (TID 200)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 7.0 in stage 39.0 (TID 205)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 3.0 in stage 39.0 (TID 201)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.048 [Executor task launch worker for task 5.0 in stage 39.0 (TID 203)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.049 [Executor task launch worker for task 6.0 in stage 39.0 (TID 204)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 39.0 (TID 204). 1297 bytes result sent to driver
13:25:24.049 [Executor task launch worker for task 3.0 in stage 39.0 (TID 201)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 39.0 (TID 201). 1340 bytes result sent to driver
13:25:24.049 [Executor task launch worker for task 2.0 in stage 39.0 (TID 200)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 39.0 (TID 200). 1297 bytes result sent to driver
13:25:24.049 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 39.0 (TID 207) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:24.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 39.0 (TID 204) in 3 ms on 172.20.10.7 (executor driver) (1/10)
13:25:24.049 [Executor task launch worker for task 9.0 in stage 39.0 (TID 207)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 39.0 (TID 207)
13:25:24.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 39.0 (TID 201) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:25:24.050 [Executor task launch worker for task 4.0 in stage 39.0 (TID 202)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 39.0 (TID 202). 1340 bytes result sent to driver
13:25:24.050 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 39.0 (TID 200) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:24.050 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 39.0 (TID 202) in 4 ms on 172.20.10.7 (executor driver) (4/10)
13:25:24.050 [Executor task launch worker for task 8.0 in stage 39.0 (TID 206)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 39.0 (TID 206). 1297 bytes result sent to driver
13:25:24.051 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 39.0 (TID 206) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:24.051 [Executor task launch worker for task 9.0 in stage 39.0 (TID 207)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.051 [Executor task launch worker for task 0.0 in stage 39.0 (TID 198)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 39.0 (TID 198). 1340 bytes result sent to driver
13:25:24.051 [Executor task launch worker for task 9.0 in stage 39.0 (TID 207)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.051 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 39.0 (TID 198) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:24.051 [Executor task launch worker for task 1.0 in stage 39.0 (TID 199)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 39.0 (TID 199). 1340 bytes result sent to driver
13:25:24.051 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 39.0 (TID 199) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:25:24.052 [Executor task launch worker for task 9.0 in stage 39.0 (TID 207)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 39.0 (TID 207). 1297 bytes result sent to driver
13:25:24.052 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 39.0 (TID 207) in 3 ms on 172.20.10.7 (executor driver) (8/10)
13:25:24.052 [Executor task launch worker for task 5.0 in stage 39.0 (TID 203)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 39.0 (TID 203). 1340 bytes result sent to driver
13:25:24.052 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 39.0 (TID 203) in 6 ms on 172.20.10.7 (executor driver) (9/10)
13:25:24.052 [Executor task launch worker for task 7.0 in stage 39.0 (TID 205)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 39.0 (TID 205). 1297 bytes result sent to driver
13:25:24.052 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 39.0 (TID 205) in 6 ms on 172.20.10.7 (executor driver) (10/10)
13:25:24.052 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 39.0, whose tasks have all completed, from pool 
13:25:24.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 39 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.009 s
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 40)
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 40 (ShuffledRDD[56] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:24.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:24.054 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:24.054 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:24.054 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1513
13:25:24.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 40 (ShuffledRDD[56] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:24.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 40.0 with 1 tasks resource profile 0
13:25:24.054 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 40.0 (TID 208) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:24.055 [Executor task launch worker for task 0.0 in stage 40.0 (TID 208)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 40.0 (TID 208)
13:25:24.059 [Executor task launch worker for task 0.0 in stage 40.0 (TID 208)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:24.059 [Executor task launch worker for task 0.0 in stage 40.0 (TID 208)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:24.059 [Executor task launch worker for task 0.0 in stage 40.0 (TID 208)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 40.0 (TID 208). 1228 bytes result sent to driver
13:25:24.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 40.0 (TID 208) in 5 ms on 172.20.10.7 (executor driver) (1/1)
13:25:24.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 40.0, whose tasks have all completed, from pool 
13:25:24.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 40 (collect at sparkStreamingSocket.java:61) finished in 0.007 s
13:25:24.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:24.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 40: Stage finished
13:25:24.060 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 16 finished: collect at sparkStreamingSocket.java:61, took 0.018219 s
13:25:24.060 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505924000 ms.0 from job set of time 1760505924000 ms
13:25:24.060 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.060 s for time 1760505924000 ms (execution: 0.048 s)
13:25:24.060 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 46 from persistence list
13:25:24.061 [block-manager-storage-async-thread-pool-63] INFO  org.apache.spark.storage.BlockManager - Removing RDD 46
13:25:24.061 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 45 from persistence list
13:25:24.061 [block-manager-storage-async-thread-pool-79] INFO  org.apache.spark.storage.BlockManager - Removing RDD 45
13:25:24.061 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 44 from persistence list
13:25:24.061 [block-manager-storage-async-thread-pool-74] INFO  org.apache.spark.storage.BlockManager - Removing RDD 44
13:25:24.061 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 43 from persistence list
13:25:24.062 [block-manager-storage-async-thread-pool-80] INFO  org.apache.spark.storage.BlockManager - Removing RDD 43
13:25:24.062 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[43] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505924000 ms
13:25:24.062 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505917800 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:24.062 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505918000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:24.062 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505918200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:24.062 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505918400 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.5 MiB)
13:25:24.063 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505918600 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:24.063 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505918000 ms
13:25:24.063 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505918000 ms
13:25:24.063 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505918800 on 172.20.10.7:49875 in memory (size: 10.0 B, free: 2004.5 MiB)
13:25:24.063 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505919000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:27.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505927000 ms
13:25:27.012 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505927000 ms.0 from job set of time 1760505927000 ms
13:25:27.023 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:27.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 16
13:25:27.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 17 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:27.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 42 (sortByKey at sparkStreamingSocket.java:61)
13:25:27.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 41)
13:25:27.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:27.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 42 (MapPartitionsRDD[62] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:27.027 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:27.029 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:27.029 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:27.029 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1513
13:25:27.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 42 (MapPartitionsRDD[62] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:27.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 42.0 with 10 tasks resource profile 0
13:25:27.033 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 42.0 (TID 209) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.033 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 42.0 (TID 210) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 42.0 (TID 211) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 42.0 (TID 212) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 42.0 (TID 213) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 42.0 (TID 214) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 42.0 (TID 215) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 42.0 (TID 216) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.034 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 42.0 (TID 217) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.035 [Executor task launch worker for task 2.0 in stage 42.0 (TID 211)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 42.0 (TID 211)
13:25:27.035 [Executor task launch worker for task 1.0 in stage 42.0 (TID 210)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 42.0 (TID 210)
13:25:27.035 [Executor task launch worker for task 3.0 in stage 42.0 (TID 212)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 42.0 (TID 212)
13:25:27.035 [Executor task launch worker for task 6.0 in stage 42.0 (TID 215)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 42.0 (TID 215)
13:25:27.035 [Executor task launch worker for task 4.0 in stage 42.0 (TID 213)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 42.0 (TID 213)
13:25:27.035 [Executor task launch worker for task 5.0 in stage 42.0 (TID 214)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 42.0 (TID 214)
13:25:27.035 [Executor task launch worker for task 8.0 in stage 42.0 (TID 217)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 42.0 (TID 217)
13:25:27.035 [Executor task launch worker for task 0.0 in stage 42.0 (TID 209)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 42.0 (TID 209)
13:25:27.035 [Executor task launch worker for task 7.0 in stage 42.0 (TID 216)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 42.0 (TID 216)
13:25:27.038 [Executor task launch worker for task 1.0 in stage 42.0 (TID 210)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 4.0 in stage 42.0 (TID 213)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 7.0 in stage 42.0 (TID 216)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 6.0 in stage 42.0 (TID 215)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 0.0 in stage 42.0 (TID 209)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 3.0 in stage 42.0 (TID 212)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 6.0 in stage 42.0 (TID 215)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 8.0 in stage 42.0 (TID 217)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 4.0 in stage 42.0 (TID 213)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 7.0 in stage 42.0 (TID 216)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 0.0 in stage 42.0 (TID 209)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 2.0 in stage 42.0 (TID 211)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 5.0 in stage 42.0 (TID 214)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.038 [Executor task launch worker for task 2.0 in stage 42.0 (TID 211)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 5.0 in stage 42.0 (TID 214)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 1.0 in stage 42.0 (TID 210)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 8.0 in stage 42.0 (TID 217)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.038 [Executor task launch worker for task 3.0 in stage 42.0 (TID 212)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.039 [Executor task launch worker for task 4.0 in stage 42.0 (TID 213)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 42.0 (TID 213). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 8.0 in stage 42.0 (TID 217)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 42.0 (TID 217). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 7.0 in stage 42.0 (TID 216)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 42.0 (TID 216). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 3.0 in stage 42.0 (TID 212)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 42.0 (TID 212). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 0.0 in stage 42.0 (TID 209)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 42.0 (TID 209). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 6.0 in stage 42.0 (TID 215)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 42.0 (TID 215). 1517 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 5.0 in stage 42.0 (TID 214)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 42.0 (TID 214). 1474 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 2.0 in stage 42.0 (TID 211)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 42.0 (TID 211). 1474 bytes result sent to driver
13:25:27.039 [Executor task launch worker for task 1.0 in stage 42.0 (TID 210)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 42.0 (TID 210). 1517 bytes result sent to driver
13:25:27.039 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 42.0 (TID 218) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.040 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 42.0 (TID 213) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:27.040 [Executor task launch worker for task 9.0 in stage 42.0 (TID 218)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 42.0 (TID 218)
13:25:27.040 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 42.0 (TID 217) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:25:27.040 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 42.0 (TID 216) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:27.040 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 42.0 (TID 212) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:27.041 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 42.0 (TID 209) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:25:27.041 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 42.0 (TID 215) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:25:27.041 [Executor task launch worker for task 9.0 in stage 42.0 (TID 218)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.041 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 42.0 (TID 214) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:27.041 [Executor task launch worker for task 9.0 in stage 42.0 (TID 218)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.041 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 42.0 (TID 211) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:25:27.041 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 42.0 (TID 210) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:25:27.041 [Executor task launch worker for task 9.0 in stage 42.0 (TID 218)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 42.0 (TID 218). 1431 bytes result sent to driver
13:25:27.042 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 42.0 (TID 218) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:25:27.042 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 42.0, whose tasks have all completed, from pool 
13:25:27.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 42 (sortByKey at sparkStreamingSocket.java:61) finished in 0.016 s
13:25:27.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:27.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 42: Stage finished
13:25:27.042 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 finished: sortByKey at sparkStreamingSocket.java:61, took 0.019339 s
13:25:27.047 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 60 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 17
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 18 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 45 (collect at sparkStreamingSocket.java:61)
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 44)
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 44)
13:25:27.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 44 (ShuffledRDD[60] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:27.049 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:27.049 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:27.049 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:27.050 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1513
13:25:27.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 44 (ShuffledRDD[60] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:27.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 44.0 with 10 tasks resource profile 0
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 44.0 (TID 219) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 44.0 (TID 220) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 44.0 (TID 221) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 44.0 (TID 222) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 44.0 (TID 223) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 44.0 (TID 224) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 44.0 (TID 225) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 44.0 (TID 226) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.051 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 44.0 (TID 227) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.052 [Executor task launch worker for task 1.0 in stage 44.0 (TID 220)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 44.0 (TID 220)
13:25:27.052 [Executor task launch worker for task 5.0 in stage 44.0 (TID 224)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 44.0 (TID 224)
13:25:27.052 [Executor task launch worker for task 0.0 in stage 44.0 (TID 219)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 44.0 (TID 219)
13:25:27.052 [Executor task launch worker for task 6.0 in stage 44.0 (TID 225)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 44.0 (TID 225)
13:25:27.052 [Executor task launch worker for task 4.0 in stage 44.0 (TID 223)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 44.0 (TID 223)
13:25:27.052 [Executor task launch worker for task 2.0 in stage 44.0 (TID 221)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 44.0 (TID 221)
13:25:27.052 [Executor task launch worker for task 8.0 in stage 44.0 (TID 227)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 44.0 (TID 227)
13:25:27.052 [Executor task launch worker for task 7.0 in stage 44.0 (TID 226)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 44.0 (TID 226)
13:25:27.052 [Executor task launch worker for task 3.0 in stage 44.0 (TID 222)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 44.0 (TID 222)
13:25:27.054 [Executor task launch worker for task 0.0 in stage 44.0 (TID 219)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 0.0 in stage 44.0 (TID 219)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.054 [Executor task launch worker for task 8.0 in stage 44.0 (TID 227)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 3.0 in stage 44.0 (TID 222)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 5.0 in stage 44.0 (TID 224)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 1.0 in stage 44.0 (TID 220)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 6.0 in stage 44.0 (TID 225)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 4.0 in stage 44.0 (TID 223)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 5.0 in stage 44.0 (TID 224)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.054 [Executor task launch worker for task 4.0 in stage 44.0 (TID 223)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.054 [Executor task launch worker for task 1.0 in stage 44.0 (TID 220)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.054 [Executor task launch worker for task 8.0 in stage 44.0 (TID 227)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.055 [Executor task launch worker for task 2.0 in stage 44.0 (TID 221)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.054 [Executor task launch worker for task 6.0 in stage 44.0 (TID 225)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.055 [Executor task launch worker for task 2.0 in stage 44.0 (TID 221)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.054 [Executor task launch worker for task 3.0 in stage 44.0 (TID 222)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.055 [Executor task launch worker for task 7.0 in stage 44.0 (TID 226)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.055 [Executor task launch worker for task 7.0 in stage 44.0 (TID 226)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.055 [Executor task launch worker for task 0.0 in stage 44.0 (TID 219)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 44.0 (TID 219). 1340 bytes result sent to driver
13:25:27.056 [Executor task launch worker for task 3.0 in stage 44.0 (TID 222)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 44.0 (TID 222). 1340 bytes result sent to driver
13:25:27.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 44.0 (TID 228) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:27.056 [Executor task launch worker for task 9.0 in stage 44.0 (TID 228)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 44.0 (TID 228)
13:25:27.056 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 44.0 (TID 219) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:25:27.056 [Executor task launch worker for task 2.0 in stage 44.0 (TID 221)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 44.0 (TID 221). 1340 bytes result sent to driver
13:25:27.056 [Executor task launch worker for task 6.0 in stage 44.0 (TID 225)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 44.0 (TID 225). 1340 bytes result sent to driver
13:25:27.057 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 44.0 (TID 222) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:25:27.057 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 44.0 (TID 221) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:25:27.057 [Executor task launch worker for task 8.0 in stage 44.0 (TID 227)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 44.0 (TID 227). 1340 bytes result sent to driver
13:25:27.057 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 44.0 (TID 225) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:25:27.057 [Executor task launch worker for task 9.0 in stage 44.0 (TID 228)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.057 [Executor task launch worker for task 9.0 in stage 44.0 (TID 228)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 44.0 (TID 227) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:27.058 [Executor task launch worker for task 5.0 in stage 44.0 (TID 224)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 44.0 (TID 224). 1340 bytes result sent to driver
13:25:27.058 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 44.0 (TID 224) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:25:27.058 [Executor task launch worker for task 4.0 in stage 44.0 (TID 223)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 44.0 (TID 223). 1340 bytes result sent to driver
13:25:27.058 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 44.0 (TID 223) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:27.058 [Executor task launch worker for task 1.0 in stage 44.0 (TID 220)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 44.0 (TID 220). 1340 bytes result sent to driver
13:25:27.058 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 44.0 (TID 220) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:25:27.059 [Executor task launch worker for task 9.0 in stage 44.0 (TID 228)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 44.0 (TID 228). 1297 bytes result sent to driver
13:25:27.059 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 44.0 (TID 228) in 3 ms on 172.20.10.7 (executor driver) (9/10)
13:25:27.059 [Executor task launch worker for task 7.0 in stage 44.0 (TID 226)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 44.0 (TID 226). 1340 bytes result sent to driver
13:25:27.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 44.0 (TID 226) in 8 ms on 172.20.10.7 (executor driver) (10/10)
13:25:27.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 44.0, whose tasks have all completed, from pool 
13:25:27.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 44 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.011 s
13:25:27.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:27.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:27.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 45)
13:25:27.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:27.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 45 (ShuffledRDD[63] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:27.060 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:27.061 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:27.061 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:27.061 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1513
13:25:27.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 45 (ShuffledRDD[63] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:27.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 45.0 with 1 tasks resource profile 0
13:25:27.061 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 45.0 (TID 229) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:27.062 [Executor task launch worker for task 0.0 in stage 45.0 (TID 229)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 45.0 (TID 229)
13:25:27.062 [Executor task launch worker for task 0.0 in stage 45.0 (TID 229)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:27.062 [Executor task launch worker for task 0.0 in stage 45.0 (TID 229)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:27.063 [Executor task launch worker for task 0.0 in stage 45.0 (TID 229)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 45.0 (TID 229). 1185 bytes result sent to driver
13:25:27.063 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 45.0 (TID 229) in 2 ms on 172.20.10.7 (executor driver) (1/1)
13:25:27.063 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 45.0, whose tasks have all completed, from pool 
13:25:27.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 45 (collect at sparkStreamingSocket.java:61) finished in 0.003 s
13:25:27.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:27.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 45: Stage finished
13:25:27.063 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 18 finished: collect at sparkStreamingSocket.java:61, took 0.016030 s
13:25:27.064 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505927000 ms.0 from job set of time 1760505927000 ms
13:25:27.064 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.064 s for time 1760505927000 ms (execution: 0.052 s)
13:25:27.064 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 53 from persistence list
13:25:27.064 [block-manager-storage-async-thread-pool-94] INFO  org.apache.spark.storage.BlockManager - Removing RDD 53
13:25:27.064 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 52 from persistence list
13:25:27.064 [block-manager-storage-async-thread-pool-7] INFO  org.apache.spark.storage.BlockManager - Removing RDD 52
13:25:27.064 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 51 from persistence list
13:25:27.065 [block-manager-storage-async-thread-pool-8] INFO  org.apache.spark.storage.BlockManager - Removing RDD 51
13:25:27.065 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 50 from persistence list
13:25:27.065 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 50
13:25:27.065 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[50] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505927000 ms
13:25:27.065 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505921000 ms
13:25:27.065 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505921000 ms
13:25:30.011 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505930000 ms
13:25:30.011 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505930000 ms.0 from job set of time 1760505930000 ms
13:25:30.018 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:30.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 66 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 18
13:25:30.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 19 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:30.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 47 (sortByKey at sparkStreamingSocket.java:61)
13:25:30.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 46)
13:25:30.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:30.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 47 (MapPartitionsRDD[69] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:30.022 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:30.023 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:30.023 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:30.024 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 34 from broadcast at DAGScheduler.scala:1513
13:25:30.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 47 (MapPartitionsRDD[69] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:30.024 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 47.0 with 10 tasks resource profile 0
13:25:30.025 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 47.0 (TID 230) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.025 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 47.0 (TID 231) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.025 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 47.0 (TID 232) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 47.0 (TID 233) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 47.0 (TID 234) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 47.0 (TID 235) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 47.0 (TID 236) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 47.0 (TID 237) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 47.0 (TID 238) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.026 [Executor task launch worker for task 2.0 in stage 47.0 (TID 232)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 47.0 (TID 232)
13:25:30.026 [Executor task launch worker for task 5.0 in stage 47.0 (TID 235)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 47.0 (TID 235)
13:25:30.026 [Executor task launch worker for task 7.0 in stage 47.0 (TID 237)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 47.0 (TID 237)
13:25:30.026 [Executor task launch worker for task 0.0 in stage 47.0 (TID 230)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 47.0 (TID 230)
13:25:30.026 [Executor task launch worker for task 3.0 in stage 47.0 (TID 233)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 47.0 (TID 233)
13:25:30.026 [Executor task launch worker for task 4.0 in stage 47.0 (TID 234)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 47.0 (TID 234)
13:25:30.026 [Executor task launch worker for task 1.0 in stage 47.0 (TID 231)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 47.0 (TID 231)
13:25:30.026 [Executor task launch worker for task 8.0 in stage 47.0 (TID 238)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 47.0 (TID 238)
13:25:30.026 [Executor task launch worker for task 6.0 in stage 47.0 (TID 236)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 47.0 (TID 236)
13:25:30.029 [Executor task launch worker for task 8.0 in stage 47.0 (TID 238)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 8.0 in stage 47.0 (TID 238)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 3.0 in stage 47.0 (TID 233)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 0.0 in stage 47.0 (TID 230)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 2.0 in stage 47.0 (TID 232)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 7.0 in stage 47.0 (TID 237)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 3.0 in stage 47.0 (TID 233)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 0.0 in stage 47.0 (TID 230)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 5.0 in stage 47.0 (TID 235)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 7.0 in stage 47.0 (TID 237)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 6.0 in stage 47.0 (TID 236)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 4.0 in stage 47.0 (TID 234)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 5.0 in stage 47.0 (TID 235)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 4.0 in stage 47.0 (TID 234)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 1.0 in stage 47.0 (TID 231)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.029 [Executor task launch worker for task 6.0 in stage 47.0 (TID 236)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.029 [Executor task launch worker for task 2.0 in stage 47.0 (TID 232)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.030 [Executor task launch worker for task 1.0 in stage 47.0 (TID 231)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.030 [Executor task launch worker for task 7.0 in stage 47.0 (TID 237)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 47.0 (TID 237). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 0.0 in stage 47.0 (TID 230)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 47.0 (TID 230). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 8.0 in stage 47.0 (TID 238)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 47.0 (TID 238). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 3.0 in stage 47.0 (TID 233)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 47.0 (TID 233). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 2.0 in stage 47.0 (TID 232)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 47.0 (TID 232). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 5.0 in stage 47.0 (TID 235)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 47.0 (TID 235). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 1.0 in stage 47.0 (TID 231)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 47.0 (TID 231). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 4.0 in stage 47.0 (TID 234)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 47.0 (TID 234). 1517 bytes result sent to driver
13:25:30.030 [Executor task launch worker for task 6.0 in stage 47.0 (TID 236)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 47.0 (TID 236). 1517 bytes result sent to driver
13:25:30.030 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 47.0 (TID 239) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.030 [Executor task launch worker for task 9.0 in stage 47.0 (TID 239)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 47.0 (TID 239)
13:25:30.030 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 47.0 (TID 237) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:30.031 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 47.0 (TID 230) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:25:30.031 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 47.0 (TID 238) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:25:30.031 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 47.0 (TID 233) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:30.031 [Executor task launch worker for task 9.0 in stage 47.0 (TID 239)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.031 [Executor task launch worker for task 9.0 in stage 47.0 (TID 239)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.031 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 47.0 (TID 232) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:30.032 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 47.0 (TID 235) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:25:30.032 [Executor task launch worker for task 9.0 in stage 47.0 (TID 239)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 47.0 (TID 239). 1431 bytes result sent to driver
13:25:30.032 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 47.0 (TID 231) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:30.032 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 47.0 (TID 234) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:25:30.032 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 47.0 (TID 236) in 6 ms on 172.20.10.7 (executor driver) (9/10)
13:25:30.032 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 47.0 (TID 239) in 2 ms on 172.20.10.7 (executor driver) (10/10)
13:25:30.032 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 47.0, whose tasks have all completed, from pool 
13:25:30.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 47 (sortByKey at sparkStreamingSocket.java:61) finished in 0.011 s
13:25:30.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:30.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 47: Stage finished
13:25:30.033 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 finished: sortByKey at sparkStreamingSocket.java:61, took 0.014631 s
13:25:30.037 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 67 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 19
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 20 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 50 (collect at sparkStreamingSocket.java:61)
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 49)
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 49)
13:25:30.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 49 (ShuffledRDD[67] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:30.039 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:30.039 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:30.039 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:30.039 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 35 from broadcast at DAGScheduler.scala:1513
13:25:30.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 49 (ShuffledRDD[67] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:30.040 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 49.0 with 10 tasks resource profile 0
13:25:30.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 49.0 (TID 240) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 49.0 (TID 241) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 49.0 (TID 242) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.040 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 49.0 (TID 243) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 49.0 (TID 244) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 49.0 (TID 245) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 49.0 (TID 246) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 49.0 (TID 247) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 49.0 (TID 248) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.041 [Executor task launch worker for task 1.0 in stage 49.0 (TID 241)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 49.0 (TID 241)
13:25:30.041 [Executor task launch worker for task 3.0 in stage 49.0 (TID 243)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 49.0 (TID 243)
13:25:30.041 [Executor task launch worker for task 2.0 in stage 49.0 (TID 242)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 49.0 (TID 242)
13:25:30.041 [Executor task launch worker for task 0.0 in stage 49.0 (TID 240)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 49.0 (TID 240)
13:25:30.041 [Executor task launch worker for task 4.0 in stage 49.0 (TID 244)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 49.0 (TID 244)
13:25:30.041 [Executor task launch worker for task 7.0 in stage 49.0 (TID 247)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 49.0 (TID 247)
13:25:30.041 [Executor task launch worker for task 6.0 in stage 49.0 (TID 246)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 49.0 (TID 246)
13:25:30.041 [Executor task launch worker for task 8.0 in stage 49.0 (TID 248)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 49.0 (TID 248)
13:25:30.041 [Executor task launch worker for task 5.0 in stage 49.0 (TID 245)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 49.0 (TID 245)
13:25:30.043 [Executor task launch worker for task 7.0 in stage 49.0 (TID 247)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.043 [Executor task launch worker for task 4.0 in stage 49.0 (TID 244)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.043 [Executor task launch worker for task 1.0 in stage 49.0 (TID 241)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.043 [Executor task launch worker for task 7.0 in stage 49.0 (TID 247)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 4.0 in stage 49.0 (TID 244)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 8.0 in stage 49.0 (TID 248)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.044 [Executor task launch worker for task 7.0 in stage 49.0 (TID 247)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 49.0 (TID 247). 1297 bytes result sent to driver
13:25:30.044 [Executor task launch worker for task 8.0 in stage 49.0 (TID 248)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 3.0 in stage 49.0 (TID 243)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.043 [Executor task launch worker for task 5.0 in stage 49.0 (TID 245)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.043 [Executor task launch worker for task 1.0 in stage 49.0 (TID 241)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 6.0 in stage 49.0 (TID 246)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.044 [Executor task launch worker for task 6.0 in stage 49.0 (TID 246)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.044 [Executor task launch worker for task 5.0 in stage 49.0 (TID 245)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 0.0 in stage 49.0 (TID 240)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.044 [Executor task launch worker for task 0.0 in stage 49.0 (TID 240)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:30.044 [Executor task launch worker for task 3.0 in stage 49.0 (TID 243)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.043 [Executor task launch worker for task 2.0 in stage 49.0 (TID 242)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.044 [Executor task launch worker for task 4.0 in stage 49.0 (TID 244)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 49.0 (TID 244). 1340 bytes result sent to driver
13:25:30.044 [Executor task launch worker for task 2.0 in stage 49.0 (TID 242)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:30.044 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 49.0 (TID 249) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:30.044 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 49.0 (TID 247) in 3 ms on 172.20.10.7 (executor driver) (1/10)
13:25:30.044 [Executor task launch worker for task 9.0 in stage 49.0 (TID 249)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 49.0 (TID 249)
13:25:30.044 [Executor task launch worker for task 8.0 in stage 49.0 (TID 248)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 49.0 (TID 248). 1340 bytes result sent to driver
13:25:30.045 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 49.0 (TID 244) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:25:30.045 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 49.0 (TID 248) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:25:30.045 [Executor task launch worker for task 3.0 in stage 49.0 (TID 243)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 49.0 (TID 243). 1340 bytes result sent to driver
13:25:30.045 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 49.0 (TID 243) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:30.045 [Executor task launch worker for task 2.0 in stage 49.0 (TID 242)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 49.0 (TID 242). 1340 bytes result sent to driver
13:25:30.046 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 49.0 (TID 242) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:25:30.046 [Executor task launch worker for task 9.0 in stage 49.0 (TID 249)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.046 [Executor task launch worker for task 9.0 in stage 49.0 (TID 249)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.046 [Executor task launch worker for task 5.0 in stage 49.0 (TID 245)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 49.0 (TID 245). 1340 bytes result sent to driver
13:25:30.046 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 49.0 (TID 245) in 5 ms on 172.20.10.7 (executor driver) (6/10)
13:25:30.046 [Executor task launch worker for task 0.0 in stage 49.0 (TID 240)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 49.0 (TID 240). 1340 bytes result sent to driver
13:25:30.047 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 49.0 (TID 240) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:25:30.047 [Executor task launch worker for task 6.0 in stage 49.0 (TID 246)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 49.0 (TID 246). 1340 bytes result sent to driver
13:25:30.047 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 49.0 (TID 246) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:25:30.047 [Executor task launch worker for task 1.0 in stage 49.0 (TID 241)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 49.0 (TID 241). 1340 bytes result sent to driver
13:25:30.047 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 49.0 (TID 241) in 7 ms on 172.20.10.7 (executor driver) (9/10)
13:25:30.047 [Executor task launch worker for task 9.0 in stage 49.0 (TID 249)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 49.0 (TID 249). 1297 bytes result sent to driver
13:25:30.048 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 49.0 (TID 249) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:25:30.048 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 49.0, whose tasks have all completed, from pool 
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 49 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.010 s
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 50)
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:30.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 50 (ShuffledRDD[70] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:30.049 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:30.052 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:30.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:30.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 172.20.10.7:49875 in memory (size: 3.4 KiB, free: 2004.5 MiB)
13:25:30.053 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 36 from broadcast at DAGScheduler.scala:1513
13:25:30.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 50 (ShuffledRDD[70] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:30.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 50.0 with 1 tasks resource profile 0
13:25:30.053 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 50.0 (TID 250) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:30.054 [Executor task launch worker for task 0.0 in stage 50.0 (TID 250)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 50.0 (TID 250)
13:25:30.055 [Executor task launch worker for task 0.0 in stage 50.0 (TID 250)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:30.055 [Executor task launch worker for task 0.0 in stage 50.0 (TID 250)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:30.055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:25:30.055 [Executor task launch worker for task 0.0 in stage 50.0 (TID 250)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 50.0 (TID 250). 1185 bytes result sent to driver
13:25:30.055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:25:30.056 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 50.0 (TID 250) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:25:30.056 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 50.0, whose tasks have all completed, from pool 
13:25:30.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 50 (collect at sparkStreamingSocket.java:61) finished in 0.008 s
13:25:30.056 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:30.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:30.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 50: Stage finished
13:25:30.056 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 finished: collect at sparkStreamingSocket.java:61, took 0.018716 s
13:25:30.056 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 172.20.10.7:49875 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:25:30.056 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505930000 ms.0 from job set of time 1760505930000 ms
13:25:30.056 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.056 s for time 1760505930000 ms (execution: 0.045 s)
13:25:30.057 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 60 from persistence list
13:25:30.057 [block-manager-storage-async-thread-pool-35] INFO  org.apache.spark.storage.BlockManager - Removing RDD 60
13:25:30.057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_33_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:30.057 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 59 from persistence list
13:25:30.057 [block-manager-storage-async-thread-pool-30] INFO  org.apache.spark.storage.BlockManager - Removing RDD 59
13:25:30.057 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 58 from persistence list
13:25:30.057 [block-manager-storage-async-thread-pool-43] INFO  org.apache.spark.storage.BlockManager - Removing RDD 58
13:25:30.057 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 57 from persistence list
13:25:30.057 [block-manager-storage-async-thread-pool-49] INFO  org.apache.spark.storage.BlockManager - Removing RDD 57
13:25:30.058 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[57] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505930000 ms
13:25:30.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_31_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:25:30.058 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505924000 ms
13:25:30.058 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505924000 ms
13:25:30.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:30.059 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:25:30.060 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 172.20.10.7:49875 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:25:30.060 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.20.10.7:49875 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:25:32.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505932000 stored as values in memory (estimated size 7.0 B, free 2004.5 MiB)
13:25:32.203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505932000 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:32.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:32.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505932000 replicated to only 0 peer(s) instead of 1 peers
13:25:32.205 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505932000
13:25:32.403 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505932200 stored as values in memory (estimated size 7.0 B, free 2004.5 MiB)
13:25:32.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505932200 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.6 MiB)
13:25:32.405 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:32.405 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505932200 replicated to only 0 peer(s) instead of 1 peers
13:25:32.405 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505932200
13:25:33.013 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505933000 ms
13:25:33.013 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505933000 ms.0 from job set of time 1760505933000 ms
13:25:33.025 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:33.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 73 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 20
13:25:33.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 21 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:33.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 52 (sortByKey at sparkStreamingSocket.java:61)
13:25:33.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 51)
13:25:33.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 51)
13:25:33.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 51 (MapPartitionsRDD[73] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:33.030 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:33.031 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:33.032 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.6 MiB)
13:25:33.032 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 37 from broadcast at DAGScheduler.scala:1513
13:25:33.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[73] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1))
13:25:33.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 51.0 with 2 tasks resource profile 0
13:25:33.034 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 51.0 (TID 251) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:33.034 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 51.0 (TID 252) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:33.034 [Executor task launch worker for task 1.0 in stage 51.0 (TID 252)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 51.0 (TID 252)
13:25:33.034 [Executor task launch worker for task 0.0 in stage 51.0 (TID 251)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 51.0 (TID 251)
13:25:33.037 [Executor task launch worker for task 0.0 in stage 51.0 (TID 251)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505932000 locally
13:25:33.037 [Executor task launch worker for task 1.0 in stage 51.0 (TID 252)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505932200 locally
13:25:33.040 [Executor task launch worker for task 0.0 in stage 51.0 (TID 251)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 51.0 (TID 251). 1177 bytes result sent to driver
13:25:33.040 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 51.0 (TID 251) in 6 ms on 172.20.10.7 (executor driver) (1/2)
13:25:33.041 [Executor task launch worker for task 1.0 in stage 51.0 (TID 252)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 51.0 (TID 252). 1177 bytes result sent to driver
13:25:33.041 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 51.0 (TID 252) in 7 ms on 172.20.10.7 (executor driver) (2/2)
13:25:33.041 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 51.0, whose tasks have all completed, from pool 
13:25:33.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 51 (mapToPair at sparkStreamingSocket.java:49) finished in 0.012 s
13:25:33.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:33.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:33.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 52)
13:25:33.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:33.042 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 52 (MapPartitionsRDD[76] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:33.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:33.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 2004.4 MiB)
13:25:33.044 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.20.10.7:49875 (size: 3.7 KiB, free: 2004.6 MiB)
13:25:33.044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1513
13:25:33.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 52 (MapPartitionsRDD[76] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:33.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 52.0 with 10 tasks resource profile 0
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 52.0 (TID 253) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 52.0 (TID 254) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 52.0 (TID 255) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 52.0 (TID 256) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 52.0 (TID 257) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 52.0 (TID 258) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 52.0 (TID 259) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 52.0 (TID 260) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.045 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 52.0 (TID 261) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.046 [Executor task launch worker for task 5.0 in stage 52.0 (TID 258)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 52.0 (TID 258)
13:25:33.046 [Executor task launch worker for task 0.0 in stage 52.0 (TID 253)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 52.0 (TID 253)
13:25:33.046 [Executor task launch worker for task 1.0 in stage 52.0 (TID 254)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 52.0 (TID 254)
13:25:33.046 [Executor task launch worker for task 8.0 in stage 52.0 (TID 261)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 52.0 (TID 261)
13:25:33.046 [Executor task launch worker for task 7.0 in stage 52.0 (TID 260)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 52.0 (TID 260)
13:25:33.046 [Executor task launch worker for task 4.0 in stage 52.0 (TID 257)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 52.0 (TID 257)
13:25:33.046 [Executor task launch worker for task 3.0 in stage 52.0 (TID 256)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 52.0 (TID 256)
13:25:33.046 [Executor task launch worker for task 6.0 in stage 52.0 (TID 259)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 52.0 (TID 259)
13:25:33.046 [Executor task launch worker for task 2.0 in stage 52.0 (TID 255)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 52.0 (TID 255)
13:25:33.048 [Executor task launch worker for task 1.0 in stage 52.0 (TID 254)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 3.0 in stage 52.0 (TID 256)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 4.0 in stage 52.0 (TID 257)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 5.0 in stage 52.0 (TID 258)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 4.0 in stage 52.0 (TID 257)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 8.0 in stage 52.0 (TID 261)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 1.0 in stage 52.0 (TID 254)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 3.0 in stage 52.0 (TID 256)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 6.0 in stage 52.0 (TID 259)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 7.0 in stage 52.0 (TID 260)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 8.0 in stage 52.0 (TID 261)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 7.0 in stage 52.0 (TID 260)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 0.0 in stage 52.0 (TID 253)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 2.0 in stage 52.0 (TID 255)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.048 [Executor task launch worker for task 2.0 in stage 52.0 (TID 255)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 6.0 in stage 52.0 (TID 259)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 0.0 in stage 52.0 (TID 253)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.048 [Executor task launch worker for task 5.0 in stage 52.0 (TID 258)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.049 [Executor task launch worker for task 3.0 in stage 52.0 (TID 256)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 52.0 (TID 256). 1474 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 1.0 in stage 52.0 (TID 254)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 52.0 (TID 254). 1517 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 7.0 in stage 52.0 (TID 260)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 52.0 (TID 260). 1474 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 8.0 in stage 52.0 (TID 261)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 52.0 (TID 261). 1474 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 4.0 in stage 52.0 (TID 257)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 52.0 (TID 257). 1474 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 2.0 in stage 52.0 (TID 255)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 52.0 (TID 255). 1474 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 5.0 in stage 52.0 (TID 258)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 52.0 (TID 258). 1517 bytes result sent to driver
13:25:33.049 [Executor task launch worker for task 6.0 in stage 52.0 (TID 259)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 52.0 (TID 259). 1517 bytes result sent to driver
13:25:33.049 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 52.0 (TID 262) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.049 [Executor task launch worker for task 9.0 in stage 52.0 (TID 262)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 52.0 (TID 262)
13:25:33.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 52.0 (TID 254) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:33.049 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 52.0 (TID 256) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:33.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 52.0 (TID 260) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:25:33.050 [Executor task launch worker for task 0.0 in stage 52.0 (TID 253)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 52.0 (TID 253). 1520 bytes result sent to driver
13:25:33.050 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 52.0 (TID 261) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:25:33.050 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 52.0 (TID 257) in 5 ms on 172.20.10.7 (executor driver) (5/10)
13:25:33.050 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 52.0 (TID 255) in 5 ms on 172.20.10.7 (executor driver) (6/10)
13:25:33.050 [Executor task launch worker for task 9.0 in stage 52.0 (TID 262)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 52.0 (TID 258) in 5 ms on 172.20.10.7 (executor driver) (7/10)
13:25:33.050 [Executor task launch worker for task 9.0 in stage 52.0 (TID 262)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.050 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 52.0 (TID 259) in 5 ms on 172.20.10.7 (executor driver) (8/10)
13:25:33.050 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 52.0 (TID 253) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:25:33.050 [Executor task launch worker for task 9.0 in stage 52.0 (TID 262)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 52.0 (TID 262). 1431 bytes result sent to driver
13:25:33.051 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 52.0 (TID 262) in 2 ms on 172.20.10.7 (executor driver) (10/10)
13:25:33.051 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 52.0, whose tasks have all completed, from pool 
13:25:33.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 52 (sortByKey at sparkStreamingSocket.java:61) finished in 0.009 s
13:25:33.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:33.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 52: Stage finished
13:25:33.051 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 finished: sortByKey at sparkStreamingSocket.java:61, took 0.025544 s
13:25:33.055 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 74 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 21
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 55 (collect at sparkStreamingSocket.java:61)
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 54)
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 54)
13:25:33.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 54 (ShuffledRDD[74] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:33.057 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:33.057 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:33.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:33.058 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 39 from broadcast at DAGScheduler.scala:1513
13:25:33.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 54 (ShuffledRDD[74] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:33.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 54.0 with 10 tasks resource profile 0
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 54.0 (TID 263) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 54.0 (TID 264) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 54.0 (TID 265) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 54.0 (TID 266) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 54.0 (TID 267) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 54.0 (TID 268) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 54.0 (TID 269) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 54.0 (TID 270) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 54.0 (TID 271) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.060 [Executor task launch worker for task 2.0 in stage 54.0 (TID 265)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 54.0 (TID 265)
13:25:33.060 [Executor task launch worker for task 1.0 in stage 54.0 (TID 264)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 54.0 (TID 264)
13:25:33.060 [Executor task launch worker for task 0.0 in stage 54.0 (TID 263)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 54.0 (TID 263)
13:25:33.060 [Executor task launch worker for task 3.0 in stage 54.0 (TID 266)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 54.0 (TID 266)
13:25:33.060 [Executor task launch worker for task 8.0 in stage 54.0 (TID 271)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 54.0 (TID 271)
13:25:33.060 [Executor task launch worker for task 7.0 in stage 54.0 (TID 270)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 54.0 (TID 270)
13:25:33.060 [Executor task launch worker for task 4.0 in stage 54.0 (TID 267)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 54.0 (TID 267)
13:25:33.060 [Executor task launch worker for task 6.0 in stage 54.0 (TID 269)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 54.0 (TID 269)
13:25:33.060 [Executor task launch worker for task 5.0 in stage 54.0 (TID 268)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 54.0 (TID 268)
13:25:33.061 [Executor task launch worker for task 2.0 in stage 54.0 (TID 265)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 4.0 in stage 54.0 (TID 267)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 3.0 in stage 54.0 (TID 266)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 2.0 in stage 54.0 (TID 265)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.061 [Executor task launch worker for task 1.0 in stage 54.0 (TID 264)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 7.0 in stage 54.0 (TID 270)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.062 [Executor task launch worker for task 3.0 in stage 54.0 (TID 266)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.061 [Executor task launch worker for task 0.0 in stage 54.0 (TID 263)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 6.0 in stage 54.0 (TID 269)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 5.0 in stage 54.0 (TID 268)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.061 [Executor task launch worker for task 8.0 in stage 54.0 (TID 271)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.062 [Executor task launch worker for task 6.0 in stage 54.0 (TID 269)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 4.0 in stage 54.0 (TID 267)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 8.0 in stage 54.0 (TID 271)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 0.0 in stage 54.0 (TID 263)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 1.0 in stage 54.0 (TID 264)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 5.0 in stage 54.0 (TID 268)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 7.0 in stage 54.0 (TID 270)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.062 [Executor task launch worker for task 2.0 in stage 54.0 (TID 265)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 54.0 (TID 265). 1297 bytes result sent to driver
13:25:33.063 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 54.0 (TID 272) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:33.063 [Executor task launch worker for task 7.0 in stage 54.0 (TID 270)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 54.0 (TID 270). 1297 bytes result sent to driver
13:25:33.063 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 54.0 (TID 265) in 3 ms on 172.20.10.7 (executor driver) (1/10)
13:25:33.063 [Executor task launch worker for task 9.0 in stage 54.0 (TID 272)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 54.0 (TID 272)
13:25:33.063 [Executor task launch worker for task 1.0 in stage 54.0 (TID 264)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 54.0 (TID 264). 1297 bytes result sent to driver
13:25:33.063 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 54.0 (TID 270) in 3 ms on 172.20.10.7 (executor driver) (2/10)
13:25:33.063 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 54.0 (TID 264) in 3 ms on 172.20.10.7 (executor driver) (3/10)
13:25:33.064 [Executor task launch worker for task 9.0 in stage 54.0 (TID 272)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.064 [Executor task launch worker for task 9.0 in stage 54.0 (TID 272)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.064 [Executor task launch worker for task 5.0 in stage 54.0 (TID 268)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 54.0 (TID 268). 1297 bytes result sent to driver
13:25:33.064 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 54.0 (TID 268) in 4 ms on 172.20.10.7 (executor driver) (4/10)
13:25:33.064 [Executor task launch worker for task 8.0 in stage 54.0 (TID 271)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 54.0 (TID 271). 1297 bytes result sent to driver
13:25:33.064 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 54.0 (TID 271) in 4 ms on 172.20.10.7 (executor driver) (5/10)
13:25:33.065 [Executor task launch worker for task 4.0 in stage 54.0 (TID 267)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 54.0 (TID 267). 1297 bytes result sent to driver
13:25:33.065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 54.0 (TID 267) in 5 ms on 172.20.10.7 (executor driver) (6/10)
13:25:33.065 [Executor task launch worker for task 6.0 in stage 54.0 (TID 269)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 54.0 (TID 269). 1297 bytes result sent to driver
13:25:33.065 [Executor task launch worker for task 3.0 in stage 54.0 (TID 266)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 54.0 (TID 266). 1297 bytes result sent to driver
13:25:33.065 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 54.0 (TID 269) in 5 ms on 172.20.10.7 (executor driver) (7/10)
13:25:33.065 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 54.0 (TID 266) in 5 ms on 172.20.10.7 (executor driver) (8/10)
13:25:33.066 [Executor task launch worker for task 0.0 in stage 54.0 (TID 263)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 54.0 (TID 263). 1426 bytes result sent to driver
13:25:33.066 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 54.0 (TID 263) in 7 ms on 172.20.10.7 (executor driver) (9/10)
13:25:33.067 [Executor task launch worker for task 9.0 in stage 54.0 (TID 272)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 54.0 (TID 272). 1297 bytes result sent to driver
13:25:33.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 54.0 (TID 272) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:25:33.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 54.0, whose tasks have all completed, from pool 
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 54 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.010 s
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 55)
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:33.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 55 (ShuffledRDD[77] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:33.068 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:33.068 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:33.068 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_40_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:33.068 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 40 from broadcast at DAGScheduler.scala:1513
13:25:33.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[77] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:33.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 55.0 with 1 tasks resource profile 0
13:25:33.069 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 55.0 (TID 273) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:33.069 [Executor task launch worker for task 0.0 in stage 55.0 (TID 273)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 55.0 (TID 273)
13:25:33.069 [Executor task launch worker for task 0.0 in stage 55.0 (TID 273)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:33.069 [Executor task launch worker for task 0.0 in stage 55.0 (TID 273)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:33.070 [Executor task launch worker for task 0.0 in stage 55.0 (TID 273)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 55.0 (TID 273). 1330 bytes result sent to driver
13:25:33.070 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 55.0 (TID 273) in 1 ms on 172.20.10.7 (executor driver) (1/1)
13:25:33.070 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 55.0, whose tasks have all completed, from pool 
13:25:33.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 55 (collect at sparkStreamingSocket.java:61) finished in 0.003 s
13:25:33.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:33.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 55: Stage finished
13:25:33.070 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: collect at sparkStreamingSocket.java:61, took 0.014819 s
13:25:33.071 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505933000 ms.0 from job set of time 1760505933000 ms
13:25:33.071 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.071 s for time 1760505933000 ms (execution: 0.058 s)
13:25:33.071 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 67 from persistence list
13:25:33.071 [block-manager-storage-async-thread-pool-82] INFO  org.apache.spark.storage.BlockManager - Removing RDD 67
13:25:33.071 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 66 from persistence list
13:25:33.071 [block-manager-storage-async-thread-pool-80] INFO  org.apache.spark.storage.BlockManager - Removing RDD 66
13:25:33.071 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 65 from persistence list
13:25:33.071 [block-manager-storage-async-thread-pool-83] INFO  org.apache.spark.storage.BlockManager - Removing RDD 65
13:25:33.071 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 64 from persistence list
13:25:33.071 [block-manager-storage-async-thread-pool-86] INFO  org.apache.spark.storage.BlockManager - Removing RDD 64
13:25:33.071 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[64] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505933000 ms
13:25:33.071 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505927000 ms
13:25:33.071 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505927000 ms
13:25:33.602 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505933400 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:33.603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505933400 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:33.603 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:33.604 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505933400 replicated to only 0 peer(s) instead of 1 peers
13:25:33.604 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505933400
13:25:33.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505933600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:33.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505933600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:33.803 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:33.803 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505933600 replicated to only 0 peer(s) instead of 1 peers
13:25:33.804 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505933600
13:25:34.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505934600 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:34.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505934600 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:34.804 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:34.804 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505934600 replicated to only 0 peer(s) instead of 1 peers
13:25:34.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505934600
13:25:35.003 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505934800 stored as values in memory (estimated size 7.0 B, free 2004.4 MiB)
13:25:35.004 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505934800 in memory on 172.20.10.7:49875 (size: 7.0 B, free: 2004.5 MiB)
13:25:35.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:35.005 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505934800 replicated to only 0 peer(s) instead of 1 peers
13:25:35.005 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505934800
13:25:36.012 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505936000 ms
13:25:36.013 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505936000 ms.0 from job set of time 1760505936000 ms
13:25:36.023 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:36.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 80 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 22
13:25:36.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 23 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:36.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 57 (sortByKey at sparkStreamingSocket.java:61)
13:25:36.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 56)
13:25:36.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 56)
13:25:36.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 56 (MapPartitionsRDD[80] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:25:36.028 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_41 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:25:36.030 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_41_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:25:36.030 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_41_piece0 in memory on 172.20.10.7:49875 (size: 3.4 KiB, free: 2004.5 MiB)
13:25:36.030 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 41 from broadcast at DAGScheduler.scala:1513
13:25:36.031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[80] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
13:25:36.031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 56.0 with 4 tasks resource profile 0
13:25:36.032 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 56.0 (TID 274) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:36.032 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 56.0 (TID 275) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:36.032 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 56.0 (TID 276) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:36.032 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 56.0 (TID 277) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:25:36.033 [Executor task launch worker for task 0.0 in stage 56.0 (TID 274)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 56.0 (TID 274)
13:25:36.033 [Executor task launch worker for task 2.0 in stage 56.0 (TID 276)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 56.0 (TID 276)
13:25:36.033 [Executor task launch worker for task 3.0 in stage 56.0 (TID 277)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 56.0 (TID 277)
13:25:36.033 [Executor task launch worker for task 1.0 in stage 56.0 (TID 275)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 56.0 (TID 275)
13:25:36.034 [Executor task launch worker for task 2.0 in stage 56.0 (TID 276)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505934600 locally
13:25:36.034 [Executor task launch worker for task 1.0 in stage 56.0 (TID 275)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505933600 locally
13:25:36.034 [Executor task launch worker for task 3.0 in stage 56.0 (TID 277)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505934800 locally
13:25:36.034 [Executor task launch worker for task 0.0 in stage 56.0 (TID 274)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505933400 locally
13:25:36.037 [Executor task launch worker for task 2.0 in stage 56.0 (TID 276)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 56.0 (TID 276). 1177 bytes result sent to driver
13:25:36.038 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 56.0 (TID 276) in 6 ms on 172.20.10.7 (executor driver) (1/4)
13:25:36.039 [Executor task launch worker for task 1.0 in stage 56.0 (TID 275)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 56.0 (TID 275). 1177 bytes result sent to driver
13:25:36.039 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 56.0 (TID 275) in 7 ms on 172.20.10.7 (executor driver) (2/4)
13:25:36.039 [Executor task launch worker for task 0.0 in stage 56.0 (TID 274)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 56.0 (TID 274). 1177 bytes result sent to driver
13:25:36.040 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 56.0 (TID 274) in 8 ms on 172.20.10.7 (executor driver) (3/4)
13:25:36.040 [Executor task launch worker for task 3.0 in stage 56.0 (TID 277)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 56.0 (TID 277). 1177 bytes result sent to driver
13:25:36.041 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 56.0 (TID 277) in 9 ms on 172.20.10.7 (executor driver) (4/4)
13:25:36.041 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 56.0, whose tasks have all completed, from pool 
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 56 (mapToPair at sparkStreamingSocket.java:49) finished in 0.014 s
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 57)
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:36.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 57 (MapPartitionsRDD[83] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:36.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_42 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:36.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:36.043 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_42_piece0 in memory on 172.20.10.7:49875 (size: 3.6 KiB, free: 2004.5 MiB)
13:25:36.043 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 42 from broadcast at DAGScheduler.scala:1513
13:25:36.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 57 (MapPartitionsRDD[83] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:36.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 57.0 with 10 tasks resource profile 0
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 57.0 (TID 278) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 57.0 (TID 279) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 57.0 (TID 280) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 57.0 (TID 281) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 57.0 (TID 282) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 57.0 (TID 283) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 57.0 (TID 284) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 57.0 (TID 285) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 57.0 (TID 286) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.044 [Executor task launch worker for task 0.0 in stage 57.0 (TID 278)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 57.0 (TID 278)
13:25:36.044 [Executor task launch worker for task 1.0 in stage 57.0 (TID 279)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 57.0 (TID 279)
13:25:36.044 [Executor task launch worker for task 2.0 in stage 57.0 (TID 280)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 57.0 (TID 280)
13:25:36.045 [Executor task launch worker for task 4.0 in stage 57.0 (TID 282)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 57.0 (TID 282)
13:25:36.045 [Executor task launch worker for task 6.0 in stage 57.0 (TID 284)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 57.0 (TID 284)
13:25:36.045 [Executor task launch worker for task 3.0 in stage 57.0 (TID 281)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 57.0 (TID 281)
13:25:36.045 [Executor task launch worker for task 5.0 in stage 57.0 (TID 283)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 57.0 (TID 283)
13:25:36.045 [Executor task launch worker for task 7.0 in stage 57.0 (TID 285)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 57.0 (TID 285)
13:25:36.045 [Executor task launch worker for task 8.0 in stage 57.0 (TID 286)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 57.0 (TID 286)
13:25:36.046 [Executor task launch worker for task 0.0 in stage 57.0 (TID 278)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 (516.0 B) non-empty blocks including 4 (516.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.046 [Executor task launch worker for task 0.0 in stage 57.0 (TID 278)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.046 [Executor task launch worker for task 6.0 in stage 57.0 (TID 284)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.046 [Executor task launch worker for task 8.0 in stage 57.0 (TID 286)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.046 [Executor task launch worker for task 1.0 in stage 57.0 (TID 279)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.046 [Executor task launch worker for task 8.0 in stage 57.0 (TID 286)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.046 [Executor task launch worker for task 1.0 in stage 57.0 (TID 279)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.046 [Executor task launch worker for task 2.0 in stage 57.0 (TID 280)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.046 [Executor task launch worker for task 5.0 in stage 57.0 (TID 283)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.047 [Executor task launch worker for task 7.0 in stage 57.0 (TID 285)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.047 [Executor task launch worker for task 2.0 in stage 57.0 (TID 280)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.047 [Executor task launch worker for task 7.0 in stage 57.0 (TID 285)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.047 [Executor task launch worker for task 4.0 in stage 57.0 (TID 282)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.047 [Executor task launch worker for task 4.0 in stage 57.0 (TID 282)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.047 [Executor task launch worker for task 5.0 in stage 57.0 (TID 283)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.046 [Executor task launch worker for task 3.0 in stage 57.0 (TID 281)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.047 [Executor task launch worker for task 8.0 in stage 57.0 (TID 286)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 57.0 (TID 286). 1517 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 3.0 in stage 57.0 (TID 281)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.046 [Executor task launch worker for task 6.0 in stage 57.0 (TID 284)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.047 [Executor task launch worker for task 4.0 in stage 57.0 (TID 282)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 57.0 (TID 282). 1474 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 2.0 in stage 57.0 (TID 280)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 57.0 (TID 280). 1517 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 1.0 in stage 57.0 (TID 279)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 57.0 (TID 279). 1474 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 5.0 in stage 57.0 (TID 283)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 57.0 (TID 283). 1517 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 7.0 in stage 57.0 (TID 285)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 57.0 (TID 285). 1474 bytes result sent to driver
13:25:36.047 [Executor task launch worker for task 3.0 in stage 57.0 (TID 281)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 57.0 (TID 281). 1517 bytes result sent to driver
13:25:36.047 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 57.0 (TID 287) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.047 [Executor task launch worker for task 0.0 in stage 57.0 (TID 278)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 57.0 (TID 278). 1520 bytes result sent to driver
13:25:36.048 [Executor task launch worker for task 6.0 in stage 57.0 (TID 284)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 57.0 (TID 284). 1517 bytes result sent to driver
13:25:36.048 [Executor task launch worker for task 9.0 in stage 57.0 (TID 287)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 57.0 (TID 287)
13:25:36.048 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 57.0 (TID 286) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:25:36.048 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 57.0 (TID 282) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:25:36.048 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 57.0 (TID 280) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:25:36.048 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 57.0 (TID 283) in 4 ms on 172.20.10.7 (executor driver) (4/10)
13:25:36.048 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 57.0 (TID 285) in 4 ms on 172.20.10.7 (executor driver) (5/10)
13:25:36.048 [Executor task launch worker for task 9.0 in stage 57.0 (TID 287)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.048 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 57.0 (TID 281) in 4 ms on 172.20.10.7 (executor driver) (6/10)
13:25:36.049 [Executor task launch worker for task 9.0 in stage 57.0 (TID 287)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.049 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 57.0 (TID 279) in 5 ms on 172.20.10.7 (executor driver) (7/10)
13:25:36.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 57.0 (TID 278) in 5 ms on 172.20.10.7 (executor driver) (8/10)
13:25:36.049 [Executor task launch worker for task 9.0 in stage 57.0 (TID 287)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 57.0 (TID 287). 1431 bytes result sent to driver
13:25:36.049 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 57.0 (TID 284) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:25:36.049 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 57.0 (TID 287) in 2 ms on 172.20.10.7 (executor driver) (10/10)
13:25:36.049 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 57.0, whose tasks have all completed, from pool 
13:25:36.049 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 57 (sortByKey at sparkStreamingSocket.java:61) finished in 0.007 s
13:25:36.049 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:36.049 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 57: Stage finished
13:25:36.049 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 23 finished: sortByKey at sparkStreamingSocket.java:61, took 0.026187 s
13:25:36.054 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 81 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 23
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 24 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 60 (collect at sparkStreamingSocket.java:61)
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 59)
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 59)
13:25:36.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 59 (ShuffledRDD[81] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:36.055 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:36.055 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:36.055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_43_piece0 in memory on 172.20.10.7:49875 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:36.056 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 43 from broadcast at DAGScheduler.scala:1513
13:25:36.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 59 (ShuffledRDD[81] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:36.056 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 59.0 with 10 tasks resource profile 0
13:25:36.056 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 59.0 (TID 288) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.056 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 59.0 (TID 289) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.056 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 59.0 (TID 290) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.056 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 59.0 (TID 291) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 59.0 (TID 292) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 59.0 (TID 293) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 59.0 (TID 294) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 59.0 (TID 295) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 59.0 (TID 296) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.057 [Executor task launch worker for task 0.0 in stage 59.0 (TID 288)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 59.0 (TID 288)
13:25:36.057 [Executor task launch worker for task 2.0 in stage 59.0 (TID 290)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 59.0 (TID 290)
13:25:36.057 [Executor task launch worker for task 1.0 in stage 59.0 (TID 289)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 59.0 (TID 289)
13:25:36.057 [Executor task launch worker for task 4.0 in stage 59.0 (TID 292)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 59.0 (TID 292)
13:25:36.057 [Executor task launch worker for task 5.0 in stage 59.0 (TID 293)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 59.0 (TID 293)
13:25:36.057 [Executor task launch worker for task 8.0 in stage 59.0 (TID 296)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 59.0 (TID 296)
13:25:36.057 [Executor task launch worker for task 7.0 in stage 59.0 (TID 295)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 59.0 (TID 295)
13:25:36.057 [Executor task launch worker for task 3.0 in stage 59.0 (TID 291)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 59.0 (TID 291)
13:25:36.057 [Executor task launch worker for task 6.0 in stage 59.0 (TID 294)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 59.0 (TID 294)
13:25:36.058 [Executor task launch worker for task 4.0 in stage 59.0 (TID 292)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 4.0 in stage 59.0 (TID 292)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 0.0 in stage 59.0 (TID 288)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 (516.0 B) non-empty blocks including 4 (516.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 0.0 in stage 59.0 (TID 288)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 5.0 in stage 59.0 (TID 293)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 5.0 in stage 59.0 (TID 293)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 8.0 in stage 59.0 (TID 296)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 7.0 in stage 59.0 (TID 295)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 6.0 in stage 59.0 (TID 294)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 1.0 in stage 59.0 (TID 289)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 2.0 in stage 59.0 (TID 290)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 8.0 in stage 59.0 (TID 296)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 7.0 in stage 59.0 (TID 295)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 2.0 in stage 59.0 (TID 290)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 3.0 in stage 59.0 (TID 291)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.058 [Executor task launch worker for task 6.0 in stage 59.0 (TID 294)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 1.0 in stage 59.0 (TID 289)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 3.0 in stage 59.0 (TID 291)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.058 [Executor task launch worker for task 4.0 in stage 59.0 (TID 292)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 59.0 (TID 292). 1254 bytes result sent to driver
13:25:36.059 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 59.0 (TID 297) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:36.059 [Executor task launch worker for task 5.0 in stage 59.0 (TID 293)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 59.0 (TID 293). 1297 bytes result sent to driver
13:25:36.059 [Executor task launch worker for task 9.0 in stage 59.0 (TID 297)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 59.0 (TID 297)
13:25:36.059 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 59.0 (TID 292) in 3 ms on 172.20.10.7 (executor driver) (1/10)
13:25:36.059 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 59.0 (TID 293) in 2 ms on 172.20.10.7 (executor driver) (2/10)
13:25:36.059 [Executor task launch worker for task 1.0 in stage 59.0 (TID 289)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 59.0 (TID 289). 1297 bytes result sent to driver
13:25:36.060 [Executor task launch worker for task 3.0 in stage 59.0 (TID 291)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 59.0 (TID 291). 1297 bytes result sent to driver
13:25:36.060 [Executor task launch worker for task 9.0 in stage 59.0 (TID 297)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 59.0 (TID 289) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:25:36.060 [Executor task launch worker for task 9.0 in stage 59.0 (TID 297)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.060 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 59.0 (TID 291) in 4 ms on 172.20.10.7 (executor driver) (4/10)
13:25:36.060 [Executor task launch worker for task 0.0 in stage 59.0 (TID 288)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 59.0 (TID 288). 1426 bytes result sent to driver
13:25:36.060 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 59.0 (TID 288) in 4 ms on 172.20.10.7 (executor driver) (5/10)
13:25:36.060 [Executor task launch worker for task 7.0 in stage 59.0 (TID 295)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 59.0 (TID 295). 1297 bytes result sent to driver
13:25:36.060 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 59.0 (TID 295) in 3 ms on 172.20.10.7 (executor driver) (6/10)
13:25:36.061 [Executor task launch worker for task 6.0 in stage 59.0 (TID 294)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 59.0 (TID 294). 1297 bytes result sent to driver
13:25:36.061 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 59.0 (TID 294) in 4 ms on 172.20.10.7 (executor driver) (7/10)
13:25:36.061 [Executor task launch worker for task 2.0 in stage 59.0 (TID 290)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 59.0 (TID 290). 1297 bytes result sent to driver
13:25:36.061 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 59.0 (TID 290) in 5 ms on 172.20.10.7 (executor driver) (8/10)
13:25:36.061 [Executor task launch worker for task 8.0 in stage 59.0 (TID 296)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 59.0 (TID 296). 1297 bytes result sent to driver
13:25:36.061 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 59.0 (TID 296) in 4 ms on 172.20.10.7 (executor driver) (9/10)
13:25:36.061 [Executor task launch worker for task 9.0 in stage 59.0 (TID 297)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 59.0 (TID 297). 1297 bytes result sent to driver
13:25:36.061 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 59.0 (TID 297) in 2 ms on 172.20.10.7 (executor driver) (10/10)
13:25:36.061 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 59.0, whose tasks have all completed, from pool 
13:25:36.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 59 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.006 s
13:25:36.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:36.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:36.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 60)
13:25:36.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:36.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 60 (ShuffledRDD[84] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:36.062 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:36.062 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:36.062 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_44_piece0 in memory on 172.20.10.7:49875 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:36.063 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 44 from broadcast at DAGScheduler.scala:1513
13:25:36.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 60 (ShuffledRDD[84] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:36.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 60.0 with 1 tasks resource profile 0
13:25:36.063 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 60.0 (TID 298) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:36.063 [Executor task launch worker for task 0.0 in stage 60.0 (TID 298)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 60.0 (TID 298)
13:25:36.064 [Executor task launch worker for task 0.0 in stage 60.0 (TID 298)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:36.064 [Executor task launch worker for task 0.0 in stage 60.0 (TID 298)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:36.064 [Executor task launch worker for task 0.0 in stage 60.0 (TID 298)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 60.0 (TID 298). 1330 bytes result sent to driver
13:25:36.064 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 60.0 (TID 298) in 1 ms on 172.20.10.7 (executor driver) (1/1)
13:25:36.064 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 60.0, whose tasks have all completed, from pool 
13:25:36.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 60 (collect at sparkStreamingSocket.java:61) finished in 0.002 s
13:25:36.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:36.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 60: Stage finished
13:25:36.065 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 finished: collect at sparkStreamingSocket.java:61, took 0.010798 s
13:25:36.065 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505936000 ms.0 from job set of time 1760505936000 ms
13:25:36.065 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.065 s for time 1760505936000 ms (execution: 0.052 s)
13:25:36.065 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 74 from persistence list
13:25:36.065 [block-manager-storage-async-thread-pool-98] INFO  org.apache.spark.storage.BlockManager - Removing RDD 74
13:25:36.065 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 73 from persistence list
13:25:36.065 [block-manager-storage-async-thread-pool-87] INFO  org.apache.spark.storage.BlockManager - Removing RDD 73
13:25:36.065 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 72 from persistence list
13:25:36.065 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 72
13:25:36.066 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 71 from persistence list
13:25:36.066 [block-manager-storage-async-thread-pool-6] INFO  org.apache.spark.storage.BlockManager - Removing RDD 71
13:25:36.066 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[71] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505936000 ms
13:25:36.066 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505930000 ms
13:25:36.066 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505930000 ms
13:25:36.066 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505932000 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:36.066 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505932200 on 172.20.10.7:49875 in memory (size: 7.0 B, free: 2004.5 MiB)
13:25:36.439 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:25:36.441 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:25:36.441 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:25:36.442 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:25:36.442 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Closed socket to 127.0.0.1:9999
13:25:36.442 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:25:36.442 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:25:36.443 [dispatcher-event-loop-7] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
13:25:36.443 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:25:36.444 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:25:36.442 [Socket Receiver] WARN  org.apache.spark.streaming.dstream.SocketReceiver - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:25:36.445 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:25:36.445 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
13:25:36.445 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:25:36.806 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.util.RecurringTimer - Stopped timer for BlockGenerator after time 1760505936800
13:25:36.806 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Waiting for block pushing thread to terminate
13:25:36.815 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushing out the last 0 blocks
13:25:36.815 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopped block pushing thread
13:25:36.815 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopped BlockGenerator
13:25:36.816 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver without error
13:25:36.817 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 888 bytes result sent to driver
13:25:36.817 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 34877 ms on 172.20.10.7 (executor driver) (1/1)
13:25:36.818 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
13:25:36.818 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (start at sparkStreamingSocket.java:67) finished in 35.303 s
13:25:36.818 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:36.818 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
13:25:36.819 [shutdown-hook-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - All of the receivers have deregistered successfully
13:25:36.819 [shutdown-hook-0] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker stopped
13:25:36.820 [shutdown-hook-0] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Stopping JobGenerator immediately
13:25:36.820 [shutdown-hook-0] INFO  org.apache.spark.streaming.util.RecurringTimer - Stopped timer for JobGenerator after time 1760505936000
13:25:36.821 [shutdown-hook-0] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Stopped JobGenerator
13:25:36.823 [shutdown-hook-0] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Stopped JobScheduler
13:25:36.825 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1e0895f5{/streaming,null,STOPPED,@Spark}
13:25:36.825 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@fd9ebde{/streaming/json,null,STOPPED,@Spark}
13:25:36.826 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6726cc69{/streaming/batch,null,STOPPED,@Spark}
13:25:36.826 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@33899f7a{/streaming/batch/json,null,STOPPED,@Spark}
13:25:36.827 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@644ded04{/static/streaming,null,STOPPED,@Spark}
13:25:36.827 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext stopped successfully
13:25:36.827 [shutdown-hook-0] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
13:25:36.827 [main] WARN  org.apache.spark.streaming.StreamingContext - StreamingContext has already been stopped
13:25:36.827 [main] INFO  org.apache.spark.SparkContext - SparkContext already stopped.
13:25:36.833 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector - Stopped Spark@5328a9c1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:25:36.835 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://172.20.10.7:4040
13:25:36.840 [dispatcher-event-loop-5] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
13:25:36.859 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
13:25:36.859 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
13:25:36.860 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
13:25:36.863 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
13:25:36.872 [shutdown-hook-0] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
13:25:36.872 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Shutdown hook called
13:25:36.873 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Deleting directory /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/spark-d3bc416b-f0cc-4d19-a3b2-af436372e1b3
13:25:51.026 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:25:51.048 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:25:51.076 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:25:51.076 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:25:51.076 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:25:51.076 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:25:51.084 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:25:51.088 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:25:51.089 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:25:51.105 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:25:51.106 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:25:51.106 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:25:51.106 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:25:51.106 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:25:51.197 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49891.
13:25:51.205 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:25:51.216 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:25:51.222 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:25:51.222 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:25:51.224 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:25:51.234 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-4eac6a56-2b14-4b3c-8f24-c1008b9de6fa
13:25:51.261 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:25:51.267 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:25:51.283 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5879ms to org.sparkproject.jetty.util.log.Slf4jLog
13:25:51.327 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:25:51.334 [main] INFO  org.sparkproject.jetty.server.Server - Started @5931ms
13:25:51.348 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@5328a9c1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:25:51.348 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:25:51.356 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e868ef5{/,null,AVAILABLE,@Spark}
13:25:51.382 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:25:51.384 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:25:51.390 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49892.
13:25:51.390 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49892
13:25:51.391 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:25:51.392 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49892, None)
13:25:51.394 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49892 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49892, None)
13:25:51.395 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49892, None)
13:25:51.395 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49892, None)
13:25:51.467 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e868ef5{/,null,STOPPED,@Spark}
13:25:51.468 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11bd803{/jobs,null,AVAILABLE,@Spark}
13:25:51.469 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75798d03{/jobs/json,null,AVAILABLE,@Spark}
13:25:51.469 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@442f92e6{/jobs/job,null,AVAILABLE,@Spark}
13:25:51.469 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a55f148{/jobs/job/json,null,AVAILABLE,@Spark}
13:25:51.470 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae2ed38{/stages,null,AVAILABLE,@Spark}
13:25:51.471 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/stages/json,null,AVAILABLE,@Spark}
13:25:51.471 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1431267b{/stages/stage,null,AVAILABLE,@Spark}
13:25:51.472 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/stages/stage/json,null,AVAILABLE,@Spark}
13:25:51.473 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/stages/pool,null,AVAILABLE,@Spark}
13:25:51.473 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/stages/pool/json,null,AVAILABLE,@Spark}
13:25:51.474 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/storage,null,AVAILABLE,@Spark}
13:25:51.474 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44841b43{/storage/json,null,AVAILABLE,@Spark}
13:25:51.474 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/storage/rdd,null,AVAILABLE,@Spark}
13:25:51.475 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/storage/rdd/json,null,AVAILABLE,@Spark}
13:25:51.475 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/environment,null,AVAILABLE,@Spark}
13:25:51.475 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/environment/json,null,AVAILABLE,@Spark}
13:25:51.476 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/executors,null,AVAILABLE,@Spark}
13:25:51.476 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/executors/json,null,AVAILABLE,@Spark}
13:25:51.477 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/executors/threadDump,null,AVAILABLE,@Spark}
13:25:51.477 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:25:51.482 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/static,null,AVAILABLE,@Spark}
13:25:51.482 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@167279d1{/,null,AVAILABLE,@Spark}
13:25:51.483 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d96250{/api,null,AVAILABLE,@Spark}
13:25:51.483 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d178d55{/jobs/job/kill,null,AVAILABLE,@Spark}
13:25:51.484 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a480135{/stages/stage/kill,null,AVAILABLE,@Spark}
13:25:51.486 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33364212{/metrics/json,null,AVAILABLE,@Spark}
13:25:51.642 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:25:51.643 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@523d64ee
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Slide time = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Storage level = Serialized 1x Replicated
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Checkpoint interval = null
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Remember interval = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@5ff977f3
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Slide time = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@63fb0f33
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 3000 ms
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
13:25:51.644 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 3000 ms
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@354733e6
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:25:51.645 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@ce43bb4
13:25:51.664 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760505954000
13:25:51.664 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760505954000 ms
13:25:51.665 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:25:51.666 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51288417{/streaming,null,AVAILABLE,@Spark}
13:25:51.666 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/streaming/json,null,AVAILABLE,@Spark}
13:25:51.667 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f8ae0c{/streaming/batch,null,AVAILABLE,@Spark}
13:25:51.667 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/streaming/batch/json,null,AVAILABLE,@Spark}
13:25:51.667 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290d10ef{/static/streaming,null,AVAILABLE,@Spark}
13:25:51.667 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:25:51.668 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:25:51.671 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:67) with 1 output partitions
13:25:51.672 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:67)
13:25:51.672 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:25:51.672 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:51.673 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:25:51.730 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:25:51.859 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:25:51.861 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49892 (size: 35.5 KiB, free: 2004.6 MiB)
13:25:51.862 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:25:51.868 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:25:51.868 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:25:51.892 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:25:51.899 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:25:52.025 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760505952200
13:25:52.025 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:25:52.025 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:25:52.027 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49891
13:25:52.027 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:25:52.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:25:52.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connected to 127.0.0.1:9999
13:25:52.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:25:52.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:25:54.042 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505954000 ms
13:25:54.043 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505954000 ms.0 from job set of time 1760505954000 ms
13:25:54.063 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:54.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 0
13:25:54.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:54.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (sortByKey at sparkStreamingSocket.java:61)
13:25:54.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
13:25:54.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:54.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:54.080 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:25:54.082 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.5 MiB)
13:25:54.083 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.10.7:49892 (size: 3.6 KiB, free: 2004.6 MiB)
13:25:54.083 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:25:54.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:54.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 10 tasks resource profile 0
13:25:54.085 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 1) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.085 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 2) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 3) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 4) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 5) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 6) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 7) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 8) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 9) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.087 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 1)
13:25:54.087 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 2)
13:25:54.087 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 3)
13:25:54.087 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 4)
13:25:54.088 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 5)
13:25:54.088 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 6)
13:25:54.089 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 9)
13:25:54.089 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 7)
13:25:54.089 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 8)
13:25:54.190 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.190 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.190 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.190 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.191 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.191 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.191 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.191 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.191 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.192 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.192 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.192 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:25:54.192 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.192 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:25:54.192 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.192 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.192 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:25:54.192 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
13:25:54.210 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 6). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 8). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 7). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 4). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 9). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 2). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 5). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 1). 1560 bytes result sent to driver
13:25:54.210 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 3). 1560 bytes result sent to driver
13:25:54.211 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 10) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.212 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 10)
13:25:54.213 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 6) in 127 ms on 172.20.10.7 (executor driver) (1/10)
13:25:54.214 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.214 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.215 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 4) in 129 ms on 172.20.10.7 (executor driver) (2/10)
13:25:54.215 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 10). 1517 bytes result sent to driver
13:25:54.215 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 2) in 130 ms on 172.20.10.7 (executor driver) (3/10)
13:25:54.216 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 9) in 129 ms on 172.20.10.7 (executor driver) (4/10)
13:25:54.216 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 7) in 130 ms on 172.20.10.7 (executor driver) (5/10)
13:25:54.216 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 1) in 131 ms on 172.20.10.7 (executor driver) (6/10)
13:25:54.217 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 5) in 130 ms on 172.20.10.7 (executor driver) (7/10)
13:25:54.217 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 8) in 131 ms on 172.20.10.7 (executor driver) (8/10)
13:25:54.217 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 3) in 132 ms on 172.20.10.7 (executor driver) (9/10)
13:25:54.217 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 10) in 6 ms on 172.20.10.7 (executor driver) (10/10)
13:25:54.217 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13:25:54.220 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (sortByKey at sparkStreamingSocket.java:61) finished in 0.143 s
13:25:54.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:54.222 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
13:25:54.222 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: sortByKey at sparkStreamingSocket.java:61, took 0.159250 s
13:25:54.228 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:54.229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 1
13:25:54.229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:54.229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at sparkStreamingSocket.java:61)
13:25:54.229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
13:25:54.229 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
13:25:54.230 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:54.233 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.5 KiB, free 2004.5 MiB)
13:25:54.233 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:54.234 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.10.7:49892 (size: 3.2 KiB, free: 2004.6 MiB)
13:25:54.234 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
13:25:54.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:54.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 10 tasks resource profile 0
13:25:54.236 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 11) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.236 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 12) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.236 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 13) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.236 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 14) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.236 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 15) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.237 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 16) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.237 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 4.0 (TID 17) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.237 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 4.0 (TID 18) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.237 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 4.0 (TID 19) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 11)
13:25:54.237 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 12)
13:25:54.237 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 14)
13:25:54.237 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 4.0 (TID 18)
13:25:54.237 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 4.0 (TID 17)
13:25:54.237 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 13)
13:25:54.237 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 4.0 (TID 15)
13:25:54.237 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 4.0 (TID 19)
13:25:54.237 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 4.0 (TID 16)
13:25:54.244 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.244 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.244 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.244 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.245 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.245 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:54.245 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.245 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:54.245 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.245 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.245 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.246 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:54.246 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:54.255 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 4.0 (TID 18). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 11). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 4.0 (TID 19). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 13). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 4.0 (TID 15). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 4.0 (TID 16). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 12). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 14). 1340 bytes result sent to driver
13:25:54.255 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 4.0 (TID 17). 1340 bytes result sent to driver
13:25:54.255 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 4.0 (TID 20) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:54.256 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 4.0 (TID 20)
13:25:54.257 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 11) in 22 ms on 172.20.10.7 (executor driver) (1/10)
13:25:54.257 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 4.0 (TID 18) in 20 ms on 172.20.10.7 (executor driver) (2/10)
13:25:54.257 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 15) in 21 ms on 172.20.10.7 (executor driver) (3/10)
13:25:54.257 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 4.0 (TID 19) in 20 ms on 172.20.10.7 (executor driver) (4/10)
13:25:54.258 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 13) in 22 ms on 172.20.10.7 (executor driver) (5/10)
13:25:54.258 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.259 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.260 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 16) in 23 ms on 172.20.10.7 (executor driver) (6/10)
13:25:54.260 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 4.0 (TID 20). 1340 bytes result sent to driver
13:25:54.260 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 12) in 24 ms on 172.20.10.7 (executor driver) (7/10)
13:25:54.261 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 14) in 25 ms on 172.20.10.7 (executor driver) (8/10)
13:25:54.261 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 4.0 (TID 17) in 24 ms on 172.20.10.7 (executor driver) (9/10)
13:25:54.261 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 4.0 (TID 20) in 6 ms on 172.20.10.7 (executor driver) (10/10)
13:25:54.262 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
13:25:54.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.032 s
13:25:54.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:54.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:54.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
13:25:54.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:54.264 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:54.266 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:54.267 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:54.267 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.20.10.7:49892 (size: 2.9 KiB, free: 2004.6 MiB)
13:25:54.267 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
13:25:54.267 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:54.267 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
13:25:54.268 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 21) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:54.268 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 21)
13:25:54.271 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:54.271 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:54.276 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 21). 1228 bytes result sent to driver
13:25:54.277 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 21) in 9 ms on 172.20.10.7 (executor driver) (1/1)
13:25:54.277 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
13:25:54.277 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at sparkStreamingSocket.java:61) finished in 0.012 s
13:25:54.277 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:54.277 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
13:25:54.278 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at sparkStreamingSocket.java:61, took 0.049445 s
13:25:54.279 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505954000 ms.0 from job set of time 1760505954000 ms
13:25:54.279 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.279 s for time 1760505954000 ms (execution: 0.237 s)
13:25:54.280 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:25:54.281 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:25:57.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505957000 ms
13:25:57.015 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505957000 ms.0 from job set of time 1760505957000 ms
13:25:57.026 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:25:57.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 2
13:25:57.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:25:57.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (sortByKey at sparkStreamingSocket.java:61)
13:25:57.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
13:25:57.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:25:57.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:57.034 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:25:57.036 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:25:57.037 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.20.10.7:49892 (size: 3.6 KiB, free: 2004.6 MiB)
13:25:57.037 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:25:57.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:57.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 10 tasks resource profile 0
13:25:57.039 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 22) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.040 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 23) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.040 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 24) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.040 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 25) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.040 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 26) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.040 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 27) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.041 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 28) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.041 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 29) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.041 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 7.0 (TID 30) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.041 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 22)
13:25:57.041 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 23)
13:25:57.041 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 24)
13:25:57.041 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 25)
13:25:57.042 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 7.0 (TID 26)
13:25:57.042 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 7.0 (TID 28)
13:25:57.042 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 7.0 (TID 29)
13:25:57.042 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 7.0 (TID 30)
13:25:57.042 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 7.0 (TID 27)
13:25:57.044 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.045 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.045 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.045 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.045 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.046 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.046 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.046 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.046 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.046 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.045 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.046 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.046 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.047 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 7.0 (TID 28). 1517 bytes result sent to driver
13:25:57.047 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 22). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 23). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 7.0 (TID 26). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 25). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 24). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 7.0 (TID 30). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 7.0 (TID 27). 1517 bytes result sent to driver
13:25:57.048 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 7.0 (TID 29). 1517 bytes result sent to driver
13:25:57.048 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 7.0 (TID 31) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.049 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 7.0 (TID 31)
13:25:57.049 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 7.0 (TID 28) in 9 ms on 172.20.10.7 (executor driver) (1/10)
13:25:57.049 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 22) in 10 ms on 172.20.10.7 (executor driver) (2/10)
13:25:57.050 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 23) in 11 ms on 172.20.10.7 (executor driver) (3/10)
13:25:57.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 26) in 10 ms on 172.20.10.7 (executor driver) (4/10)
13:25:57.050 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.050 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.051 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 25) in 11 ms on 172.20.10.7 (executor driver) (5/10)
13:25:57.051 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 24) in 11 ms on 172.20.10.7 (executor driver) (6/10)
13:25:57.051 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 7.0 (TID 30) in 10 ms on 172.20.10.7 (executor driver) (7/10)
13:25:57.051 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 7.0 (TID 31). 1517 bytes result sent to driver
13:25:57.052 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 27) in 11 ms on 172.20.10.7 (executor driver) (8/10)
13:25:57.052 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 7.0 (TID 29) in 11 ms on 172.20.10.7 (executor driver) (9/10)
13:25:57.052 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 7.0 (TID 31) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:25:57.052 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
13:25:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (sortByKey at sparkStreamingSocket.java:61) finished in 0.021 s
13:25:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:57.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
13:25:57.053 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: sortByKey at sparkStreamingSocket.java:61, took 0.027268 s
13:25:57.059 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:25:57.060 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 3
13:25:57.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:25:57.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (collect at sparkStreamingSocket.java:61)
13:25:57.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
13:25:57.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
13:25:57.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:25:57.063 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:25:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:25:57.065 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.20.10.7:49892 (size: 3.2 KiB, free: 2004.5 MiB)
13:25:57.065 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
13:25:57.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:25:57.065 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 10 tasks resource profile 0
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 32) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 33) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 34) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 35) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 9.0 (TID 36) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.066 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 9.0 (TID 37) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.067 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 9.0 (TID 38) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.067 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 9.0 (TID 39) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.067 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 9.0 (TID 40) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.067 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 32)
13:25:57.067 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 35)
13:25:57.067 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 9.0 (TID 36)
13:25:57.067 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 34)
13:25:57.067 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 9.0 (TID 38)
13:25:57.067 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 9.0 (TID 40)
13:25:57.067 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 9.0 (TID 39)
13:25:57.067 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 9.0 (TID 37)
13:25:57.067 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 33)
13:25:57.069 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.069 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.070 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.070 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.070 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.070 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.070 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.070 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.070 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.071 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.071 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.070 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.071 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.071 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:25:57.071 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.071 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.071 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.072 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.072 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 9.0 (TID 36). 1340 bytes result sent to driver
13:25:57.072 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 32). 1340 bytes result sent to driver
13:25:57.072 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 9.0 (TID 41) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:25:57.073 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 33). 1340 bytes result sent to driver
13:25:57.073 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 35). 1340 bytes result sent to driver
13:25:57.073 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 9.0 (TID 41)
13:25:57.073 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 9.0 (TID 37). 1340 bytes result sent to driver
13:25:57.073 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 32) in 7 ms on 172.20.10.7 (executor driver) (1/10)
13:25:57.074 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 9.0 (TID 36) in 8 ms on 172.20.10.7 (executor driver) (2/10)
13:25:57.074 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 9.0 (TID 39). 1340 bytes result sent to driver
13:25:57.074 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 35) in 8 ms on 172.20.10.7 (executor driver) (3/10)
13:25:57.074 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 33) in 8 ms on 172.20.10.7 (executor driver) (4/10)
13:25:57.074 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 34). 1340 bytes result sent to driver
13:25:57.074 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 9.0 (TID 40). 1340 bytes result sent to driver
13:25:57.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 9.0 (TID 39) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:25:57.075 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 9.0 (TID 37) in 9 ms on 172.20.10.7 (executor driver) (6/10)
13:25:57.075 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 9.0 (TID 38). 1340 bytes result sent to driver
13:25:57.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 34) in 9 ms on 172.20.10.7 (executor driver) (7/10)
13:25:57.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 9.0 (TID 40) in 9 ms on 172.20.10.7 (executor driver) (8/10)
13:25:57.076 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.076 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.076 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 9.0 (TID 38) in 9 ms on 172.20.10.7 (executor driver) (9/10)
13:25:57.077 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 9.0 (TID 41). 1340 bytes result sent to driver
13:25:57.077 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 9.0 (TID 41) in 5 ms on 172.20.10.7 (executor driver) (10/10)
13:25:57.077 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.016 s
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:25:57.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:25:57.079 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:25:57.080 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:25:57.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.20.10.7:49892 (size: 2.9 KiB, free: 2004.5 MiB)
13:25:57.080 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
13:25:57.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:25:57.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0
13:25:57.081 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 42) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:25:57.081 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 42)
13:25:57.083 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:25:57.083 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:25:57.083 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 42). 1228 bytes result sent to driver
13:25:57.084 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 42) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:25:57.084 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
13:25:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (collect at sparkStreamingSocket.java:61) finished in 0.006 s
13:25:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
13:25:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
13:25:57.084 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: collect at sparkStreamingSocket.java:61, took 0.025109 s
13:25:57.085 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505957000 ms.0 from job set of time 1760505957000 ms
13:25:57.085 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.085 s for time 1760505957000 ms (execution: 0.071 s)
13:25:57.085 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 4 from persistence list
13:25:57.091 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 3 from persistence list
13:25:57.091 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:25:57.092 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 2 from persistence list
13:25:57.092 [block-manager-storage-async-thread-pool-1] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:25:57.092 [block-manager-storage-async-thread-pool-2] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:25:57.095 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:25:57.095 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:25:57.095 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505957000 ms
13:25:57.095 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:25:57.095 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:25:57.611 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505957400 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:57.612 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505957400 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:57.619 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:57.621 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505957400 replicated to only 0 peer(s) instead of 1 peers
13:25:57.627 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505957400
13:25:57.807 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505957600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:57.808 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505957600 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:57.808 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:57.808 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505957600 replicated to only 0 peer(s) instead of 1 peers
13:25:57.809 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505957600
13:25:58.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505958000 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:58.203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505958000 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:58.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:58.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505958000 replicated to only 0 peer(s) instead of 1 peers
13:25:58.206 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505958000
13:25:58.403 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505958200 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:58.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505958200 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:58.405 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:58.405 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505958200 replicated to only 0 peer(s) instead of 1 peers
13:25:58.406 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505958200
13:25:58.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505958400 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:58.605 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505958400 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:58.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:58.606 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505958400 replicated to only 0 peer(s) instead of 1 peers
13:25:58.606 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505958400
13:25:58.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505958600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:58.805 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505958600 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:58.806 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:58.806 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505958600 replicated to only 0 peer(s) instead of 1 peers
13:25:58.807 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505958600
13:25:59.219 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505959000 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:25:59.220 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505959000 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.5 MiB)
13:25:59.221 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:59.221 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505959000 replicated to only 0 peer(s) instead of 1 peers
13:25:59.222 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505959000
13:25:59.235 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.20.10.7:49892 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:25:59.238 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.20.10.7:49892 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:59.240 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.20.10.7:49892 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:25:59.243 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.20.10.7:49892 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:25:59.245 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.20.10.7:49892 in memory (size: 3.2 KiB, free: 2004.6 MiB)
13:25:59.247 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.20.10.7:49892 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:25:59.404 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505959200 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:25:59.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505959200 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:25:59.406 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:59.406 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505959200 replicated to only 0 peer(s) instead of 1 peers
13:25:59.406 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505959200
13:25:59.602 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505959400 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:25:59.603 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505959400 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:25:59.604 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:25:59.604 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505959400 replicated to only 0 peer(s) instead of 1 peers
13:25:59.605 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505959400
13:26:00.003 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505959800 stored as values in memory (estimated size 8.0 B, free 2004.5 MiB)
13:26:00.005 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505959800 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:00.006 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:00.006 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505959800 replicated to only 0 peer(s) instead of 1 peers
13:26:00.006 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505959800
13:26:00.020 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505960000 ms
13:26:00.020 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505960000 ms.0 from job set of time 1760505960000 ms
13:26:00.030 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:26:00.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 17 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 4
13:26:00.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:26:00.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (sortByKey at sparkStreamingSocket.java:61)
13:26:00.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 11)
13:26:00.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 11)
13:26:00.035 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 11 (MapPartitionsRDD[17] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:26:00.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 6.2 KiB, free 2004.5 MiB)
13:26:00.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.5 MiB)
13:26:00.043 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.20.10.7:49892 (size: 3.4 KiB, free: 2004.6 MiB)
13:26:00.044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
13:26:00.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 9 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[17] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
13:26:00.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 9 tasks resource profile 0
13:26:00.046 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 43) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.046 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 44) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.046 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 45) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.047 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 46) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.047 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 11.0 (TID 47) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.047 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 11.0 (TID 48) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.047 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 11.0 (TID 49) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.047 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 11.0 (TID 50) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.048 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 11.0 (TID 51) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:00.048 [Executor task launch worker for task 3.0 in stage 11.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 46)
13:26:00.048 [Executor task launch worker for task 4.0 in stage 11.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 11.0 (TID 47)
13:26:00.048 [Executor task launch worker for task 2.0 in stage 11.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 45)
13:26:00.048 [Executor task launch worker for task 5.0 in stage 11.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 11.0 (TID 48)
13:26:00.048 [Executor task launch worker for task 0.0 in stage 11.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 43)
13:26:00.048 [Executor task launch worker for task 7.0 in stage 11.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 11.0 (TID 50)
13:26:00.048 [Executor task launch worker for task 6.0 in stage 11.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 11.0 (TID 49)
13:26:00.048 [Executor task launch worker for task 8.0 in stage 11.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 11.0 (TID 51)
13:26:00.048 [Executor task launch worker for task 1.0 in stage 11.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 44)
13:26:00.057 [Executor task launch worker for task 2.0 in stage 11.0 (TID 45)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505958000 locally
13:26:00.057 [Executor task launch worker for task 0.0 in stage 11.0 (TID 43)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505957400 locally
13:26:00.057 [Executor task launch worker for task 6.0 in stage 11.0 (TID 49)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505959000 locally
13:26:00.057 [Executor task launch worker for task 1.0 in stage 11.0 (TID 44)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505957600 locally
13:26:00.057 [Executor task launch worker for task 5.0 in stage 11.0 (TID 48)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505958600 locally
13:26:00.057 [Executor task launch worker for task 7.0 in stage 11.0 (TID 50)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505959200 locally
13:26:00.057 [Executor task launch worker for task 8.0 in stage 11.0 (TID 51)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505959400 locally
13:26:00.057 [Executor task launch worker for task 3.0 in stage 11.0 (TID 46)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505958200 locally
13:26:00.057 [Executor task launch worker for task 4.0 in stage 11.0 (TID 47)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505958400 locally
13:26:00.069 [Executor task launch worker for task 6.0 in stage 11.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 11.0 (TID 49). 1177 bytes result sent to driver
13:26:00.070 [Executor task launch worker for task 7.0 in stage 11.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 11.0 (TID 50). 1177 bytes result sent to driver
13:26:00.071 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 11.0 (TID 49) in 24 ms on 172.20.10.7 (executor driver) (1/9)
13:26:00.071 [Executor task launch worker for task 4.0 in stage 11.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 11.0 (TID 47). 1177 bytes result sent to driver
13:26:00.071 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 11.0 (TID 50) in 24 ms on 172.20.10.7 (executor driver) (2/9)
13:26:00.071 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 11.0 (TID 47) in 24 ms on 172.20.10.7 (executor driver) (3/9)
13:26:00.072 [Executor task launch worker for task 2.0 in stage 11.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 45). 1177 bytes result sent to driver
13:26:00.072 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 45) in 26 ms on 172.20.10.7 (executor driver) (4/9)
13:26:00.072 [Executor task launch worker for task 8.0 in stage 11.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 11.0 (TID 51). 1177 bytes result sent to driver
13:26:00.073 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 11.0 (TID 51) in 26 ms on 172.20.10.7 (executor driver) (5/9)
13:26:00.074 [Executor task launch worker for task 0.0 in stage 11.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 43). 1177 bytes result sent to driver
13:26:00.074 [Executor task launch worker for task 3.0 in stage 11.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 46). 1177 bytes result sent to driver
13:26:00.075 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 46) in 29 ms on 172.20.10.7 (executor driver) (6/9)
13:26:00.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 43) in 30 ms on 172.20.10.7 (executor driver) (7/9)
13:26:00.075 [Executor task launch worker for task 5.0 in stage 11.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 11.0 (TID 48). 1177 bytes result sent to driver
13:26:00.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 11.0 (TID 48) in 29 ms on 172.20.10.7 (executor driver) (8/9)
13:26:00.076 [Executor task launch worker for task 1.0 in stage 11.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 44). 1177 bytes result sent to driver
13:26:00.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 44) in 30 ms on 172.20.10.7 (executor driver) (9/9)
13:26:00.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 11 (mapToPair at sparkStreamingSocket.java:49) finished in 0.037 s
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 12)
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:26:00.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:26:00.079 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:26:00.080 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:26:00.080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.20.10.7:49892 (size: 3.6 KiB, free: 2004.6 MiB)
13:26:00.080 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1513
13:26:00.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:26:00.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 10 tasks resource profile 0
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 52) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 53) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 54) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 55) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 56) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.082 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 57) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.083 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 58) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.083 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 59) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.083 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 12.0 (TID 60) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.083 [Executor task launch worker for task 1.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 54)
13:26:00.083 [Executor task launch worker for task 2.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 55)
13:26:00.083 [Executor task launch worker for task 6.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 59)
13:26:00.083 [Executor task launch worker for task 3.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 56)
13:26:00.083 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 12.0 (TID 60)
13:26:00.083 [Executor task launch worker for task 4.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 57)
13:26:00.083 [Executor task launch worker for task 0.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 53)
13:26:00.083 [Executor task launch worker for task 5.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 58)
13:26:00.083 [Executor task launch worker for task 7.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 52)
13:26:00.085 [Executor task launch worker for task 3.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.085 [Executor task launch worker for task 3.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.085 [Executor task launch worker for task 6.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.085 [Executor task launch worker for task 5.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.085 [Executor task launch worker for task 1.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.085 [Executor task launch worker for task 4.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.086 [Executor task launch worker for task 0.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.086 [Executor task launch worker for task 1.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.085 [Executor task launch worker for task 5.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 0.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 3.0 in stage 12.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 56). 1517 bytes result sent to driver
13:26:00.086 [Executor task launch worker for task 4.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 2.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.086 [Executor task launch worker for task 2.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.086 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 1.0 in stage 12.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 54). 1517 bytes result sent to driver
13:26:00.087 [Executor task launch worker for task 2.0 in stage 12.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 55). 1474 bytes result sent to driver
13:26:00.085 [Executor task launch worker for task 6.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.086 [Executor task launch worker for task 0.0 in stage 12.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 53). 1517 bytes result sent to driver
13:26:00.086 [Executor task launch worker for task 5.0 in stage 12.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 58). 1517 bytes result sent to driver
13:26:00.087 [Executor task launch worker for task 4.0 in stage 12.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 57). 1517 bytes result sent to driver
13:26:00.087 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 12.0 (TID 61) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.087 [Executor task launch worker for task 6.0 in stage 12.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 59). 1517 bytes result sent to driver
13:26:00.087 [Executor task launch worker for task 8.0 in stage 12.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 12.0 (TID 60). 1517 bytes result sent to driver
13:26:00.087 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 12.0 (TID 61)
13:26:00.088 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 56) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:26:00.088 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 54) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:26:00.088 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 55) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:26:00.088 [Executor task launch worker for task 7.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 9 (1161.0 B) non-empty blocks including 9 (1161.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.088 [Executor task launch worker for task 7.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
13:26:00.088 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.088 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.089 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 53) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:26:00.089 [Executor task launch worker for task 9.0 in stage 12.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 12.0 (TID 61). 1431 bytes result sent to driver
13:26:00.089 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 58) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:26:00.089 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 57) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:26:00.089 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 59) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:26:00.090 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 12.0 (TID 60) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:26:00.090 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 12.0 (TID 61) in 3 ms on 172.20.10.7 (executor driver) (9/10)
13:26:00.096 [Executor task launch worker for task 7.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 52). 1521 bytes result sent to driver
13:26:00.097 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 52) in 15 ms on 172.20.10.7 (executor driver) (10/10)
13:26:00.097 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
13:26:00.097 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 (sortByKey at sparkStreamingSocket.java:61) finished in 0.019 s
13:26:00.097 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
13:26:00.097 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished
13:26:00.097 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: sortByKey at sparkStreamingSocket.java:61, took 0.067590 s
13:26:00.102 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 5
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (collect at sparkStreamingSocket.java:61) with 1 output partitions
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (collect at sparkStreamingSocket.java:61)
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
13:26:00.103 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:26:00.104 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:26:00.105 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:26:00.105 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.20.10.7:49892 (size: 3.2 KiB, free: 2004.6 MiB)
13:26:00.105 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1513
13:26:00.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:26:00.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 10 tasks resource profile 0
13:26:00.106 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 62) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.106 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 63) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.106 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 64) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.106 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 65) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 66) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 67) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 68) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 69) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 14.0 (TID 70) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.107 [Executor task launch worker for task 7.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 62)
13:26:00.107 [Executor task launch worker for task 3.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 66)
13:26:00.107 [Executor task launch worker for task 2.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 65)
13:26:00.107 [Executor task launch worker for task 0.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 63)
13:26:00.107 [Executor task launch worker for task 4.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 67)
13:26:00.107 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 14.0 (TID 70)
13:26:00.107 [Executor task launch worker for task 6.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 69)
13:26:00.107 [Executor task launch worker for task 1.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 64)
13:26:00.107 [Executor task launch worker for task 5.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 68)
13:26:00.110 [Executor task launch worker for task 2.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.110 [Executor task launch worker for task 2.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.110 [Executor task launch worker for task 6.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.110 [Executor task launch worker for task 1.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.110 [Executor task launch worker for task 6.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.110 [Executor task launch worker for task 1.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.110 [Executor task launch worker for task 7.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 9 (1161.0 B) non-empty blocks including 9 (1161.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.110 [Executor task launch worker for task 5.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.110 [Executor task launch worker for task 7.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:26:00.110 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.111 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.111 [Executor task launch worker for task 2.0 in stage 14.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 65). 1340 bytes result sent to driver
13:26:00.110 [Executor task launch worker for task 5.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.111 [Executor task launch worker for task 4.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.111 [Executor task launch worker for task 4.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.111 [Executor task launch worker for task 0.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.111 [Executor task launch worker for task 6.0 in stage 14.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 69). 1340 bytes result sent to driver
13:26:00.111 [Executor task launch worker for task 0.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.111 [Executor task launch worker for task 3.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.112 [Executor task launch worker for task 3.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.112 [Executor task launch worker for task 5.0 in stage 14.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 68). 1340 bytes result sent to driver
13:26:00.112 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 14.0 (TID 71) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:00.112 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 14.0 (TID 71)
13:26:00.112 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 65) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:26:00.112 [Executor task launch worker for task 1.0 in stage 14.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 64). 1340 bytes result sent to driver
13:26:00.112 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 68) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:26:00.112 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 69) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:26:00.113 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 64) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:26:00.113 [Executor task launch worker for task 3.0 in stage 14.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 66). 1340 bytes result sent to driver
13:26:00.113 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.113 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.113 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 66) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:26:00.114 [Executor task launch worker for task 4.0 in stage 14.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 67). 1340 bytes result sent to driver
13:26:00.114 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 67) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:26:00.114 [Executor task launch worker for task 0.0 in stage 14.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 63). 1340 bytes result sent to driver
13:26:00.115 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 63) in 9 ms on 172.20.10.7 (executor driver) (7/10)
13:26:00.115 [Executor task launch worker for task 8.0 in stage 14.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 14.0 (TID 70). 1340 bytes result sent to driver
13:26:00.115 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 14.0 (TID 70) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:26:00.115 [Executor task launch worker for task 9.0 in stage 14.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 14.0 (TID 71). 1297 bytes result sent to driver
13:26:00.115 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 14.0 (TID 71) in 4 ms on 172.20.10.7 (executor driver) (9/10)
13:26:00.120 [Executor task launch worker for task 7.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 62). 1469 bytes result sent to driver
13:26:00.121 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 62) in 15 ms on 172.20.10.7 (executor driver) (10/10)
13:26:00.121 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.017 s
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:26:00.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:26:00.122 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:26:00.123 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:26:00.123 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.20.10.7:49892 (size: 2.9 KiB, free: 2004.6 MiB)
13:26:00.123 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1513
13:26:00.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0))
13:26:00.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0
13:26:00.124 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 72) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:00.124 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 72)
13:26:00.125 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:00.126 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:00.126 [Executor task launch worker for task 0.0 in stage 15.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 72). 1374 bytes result sent to driver
13:26:00.127 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 72) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:26:00.127 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
13:26:00.127 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (collect at sparkStreamingSocket.java:61) finished in 0.006 s
13:26:00.127 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
13:26:00.127 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished
13:26:00.127 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: collect at sparkStreamingSocket.java:61, took 0.024761 s
13:26:00.127 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505960000 ms.0 from job set of time 1760505960000 ms
13:26:00.127 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.127 s for time 1760505960000 ms (execution: 0.107 s)
13:26:00.128 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 11 from persistence list
13:26:00.128 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 10 from persistence list
13:26:00.128 [block-manager-storage-async-thread-pool-39] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:26:00.128 [block-manager-storage-async-thread-pool-40] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:26:00.128 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 9 from persistence list
13:26:00.128 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:26:00.128 [block-manager-storage-async-thread-pool-44] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:26:00.129 [block-manager-storage-async-thread-pool-46] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:26:00.129 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505960000 ms
13:26:00.129 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505954000 ms
13:26:00.129 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505954000 ms
13:26:00.607 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505960400 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:26:00.609 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505960400 in memory on 172.20.10.7:49892 (size: 9.0 B, free: 2004.6 MiB)
13:26:00.609 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:00.610 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505960400 replicated to only 0 peer(s) instead of 1 peers
13:26:00.610 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505960400
13:26:01.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505960800 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:26:01.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505960800 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:01.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:01.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505960800 replicated to only 0 peer(s) instead of 1 peers
13:26:01.004 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505960800
13:26:01.206 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505961000 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:26:01.207 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505961000 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:01.208 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:01.208 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505961000 replicated to only 0 peer(s) instead of 1 peers
13:26:01.209 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505961000
13:26:01.403 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505961200 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:26:01.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505961200 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:01.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:01.405 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505961200 replicated to only 0 peer(s) instead of 1 peers
13:26:01.405 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505961200
13:26:01.802 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505961600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:26:01.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505961600 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:01.804 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:01.804 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505961600 replicated to only 0 peer(s) instead of 1 peers
13:26:01.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505961600
13:26:02.204 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760505962000 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:26:02.205 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760505962000 in memory on 172.20.10.7:49892 (size: 8.0 B, free: 2004.6 MiB)
13:26:02.205 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:26:02.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760505962000 replicated to only 0 peer(s) instead of 1 peers
13:26:02.206 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760505962000
13:26:03.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760505963000 ms
13:26:03.015 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760505963000 ms.0 from job set of time 1760505963000 ms
13:26:03.024 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:61
13:26:03.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 24 (mapToPair at sparkStreamingSocket.java:49) as input to shuffle 6
13:26:03.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (sortByKey at sparkStreamingSocket.java:61) with 10 output partitions
13:26:03.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (sortByKey at sparkStreamingSocket.java:61)
13:26:03.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
13:26:03.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
13:26:03.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:49), which has no missing parents
13:26:03.032 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:26:03.033 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:26:03.034 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.20.10.7:49892 (size: 3.4 KiB, free: 2004.5 MiB)
13:26:03.034 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1513
13:26:03.035 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 7 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
13:26:03.035 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 7 tasks resource profile 0
13:26:03.036 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 73) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 74) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 75) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 76) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 77) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 78) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.037 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 79) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:26:03.038 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 74)
13:26:03.038 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 73)
13:26:03.038 [Executor task launch worker for task 4.0 in stage 16.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 77)
13:26:03.038 [Executor task launch worker for task 3.0 in stage 16.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 76)
13:26:03.038 [Executor task launch worker for task 6.0 in stage 16.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 79)
13:26:03.038 [Executor task launch worker for task 2.0 in stage 16.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 75)
13:26:03.038 [Executor task launch worker for task 5.0 in stage 16.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 78)
13:26:03.042 [Executor task launch worker for task 5.0 in stage 16.0 (TID 78)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505961600 locally
13:26:03.043 [Executor task launch worker for task 4.0 in stage 16.0 (TID 77)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505961200 locally
13:26:03.043 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505959800 locally
13:26:03.043 [Executor task launch worker for task 6.0 in stage 16.0 (TID 79)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505962000 locally
13:26:03.043 [Executor task launch worker for task 2.0 in stage 16.0 (TID 75)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505960800 locally
13:26:03.044 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505960400 locally
13:26:03.044 [Executor task launch worker for task 3.0 in stage 16.0 (TID 76)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760505961000 locally
13:26:03.046 [Executor task launch worker for task 5.0 in stage 16.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 78). 1177 bytes result sent to driver
13:26:03.046 [Executor task launch worker for task 6.0 in stage 16.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 79). 1177 bytes result sent to driver
13:26:03.046 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 78) in 9 ms on 172.20.10.7 (executor driver) (1/7)
13:26:03.046 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 79) in 9 ms on 172.20.10.7 (executor driver) (2/7)
13:26:03.046 [Executor task launch worker for task 3.0 in stage 16.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 76). 1177 bytes result sent to driver
13:26:03.047 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 76) in 10 ms on 172.20.10.7 (executor driver) (3/7)
13:26:03.047 [Executor task launch worker for task 4.0 in stage 16.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 77). 1177 bytes result sent to driver
13:26:03.048 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 77) in 11 ms on 172.20.10.7 (executor driver) (4/7)
13:26:03.048 [Executor task launch worker for task 0.0 in stage 16.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 73). 1177 bytes result sent to driver
13:26:03.049 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 73) in 12 ms on 172.20.10.7 (executor driver) (5/7)
13:26:03.049 [Executor task launch worker for task 2.0 in stage 16.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 75). 1177 bytes result sent to driver
13:26:03.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 75) in 12 ms on 172.20.10.7 (executor driver) (6/7)
13:26:03.050 [Executor task launch worker for task 1.0 in stage 16.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 74). 1177 bytes result sent to driver
13:26:03.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 74) in 14 ms on 172.20.10.7 (executor driver) (7/7)
13:26:03.050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
13:26:03.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (mapToPair at sparkStreamingSocket.java:49) finished in 0.020 s
13:26:03.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:26:03.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:26:03.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
13:26:03.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:26:03.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:26:03.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:26:03.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 2004.4 MiB)
13:26:03.054 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.20.10.7:49892 (size: 3.7 KiB, free: 2004.5 MiB)
13:26:03.054 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1513
13:26:03.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:26:03.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 10 tasks resource profile 0
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 17.0 (TID 80) (172.20.10.7, executor driver, partition 4, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 17.0 (TID 81) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 82) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 17.0 (TID 83) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 17.0 (TID 84) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 17.0 (TID 85) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 17.0 (TID 86) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 17.0 (TID 87) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.055 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 17.0 (TID 88) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.056 [Executor task launch worker for task 2.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 17.0 (TID 84)
13:26:03.056 [Executor task launch worker for task 7.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 17.0 (TID 81)
13:26:03.056 [Executor task launch worker for task 4.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 17.0 (TID 80)
13:26:03.056 [Executor task launch worker for task 1.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 17.0 (TID 83)
13:26:03.056 [Executor task launch worker for task 8.0 in stage 17.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 17.0 (TID 88)
13:26:03.056 [Executor task launch worker for task 0.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 82)
13:26:03.056 [Executor task launch worker for task 6.0 in stage 17.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 17.0 (TID 87)
13:26:03.056 [Executor task launch worker for task 3.0 in stage 17.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 17.0 (TID 85)
13:26:03.056 [Executor task launch worker for task 5.0 in stage 17.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 17.0 (TID 86)
13:26:03.057 [Executor task launch worker for task 2.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.057 [Executor task launch worker for task 2.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 0.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 0.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 5.0 in stage 17.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 5.0 in stage 17.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 4.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 4.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 7.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 6 (774.0 B) non-empty blocks including 6 (774.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 7.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 8.0 in stage 17.0 (TID 88)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 3.0 in stage 17.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.058 [Executor task launch worker for task 8.0 in stage 17.0 (TID 88)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 3.0 in stage 17.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.058 [Executor task launch worker for task 5.0 in stage 17.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 17.0 (TID 86). 1517 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 1.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.059 [Executor task launch worker for task 1.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.059 [Executor task launch worker for task 3.0 in stage 17.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 17.0 (TID 85). 1517 bytes result sent to driver
13:26:03.058 [Executor task launch worker for task 0.0 in stage 17.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 82). 1474 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 2.0 in stage 17.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 17.0 (TID 84). 1474 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 8.0 in stage 17.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 17.0 (TID 88). 1517 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 1.0 in stage 17.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 17.0 (TID 83). 1474 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 6.0 in stage 17.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.059 [Executor task launch worker for task 4.0 in stage 17.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 17.0 (TID 80). 1522 bytes result sent to driver
13:26:03.059 [Executor task launch worker for task 6.0 in stage 17.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.059 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 17.0 (TID 89) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.059 [Executor task launch worker for task 9.0 in stage 17.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 17.0 (TID 89)
13:26:03.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 17.0 (TID 86) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:26:03.060 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 82) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:26:03.060 [Executor task launch worker for task 6.0 in stage 17.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 17.0 (TID 87). 1517 bytes result sent to driver
13:26:03.060 [Executor task launch worker for task 7.0 in stage 17.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 17.0 (TID 81). 1521 bytes result sent to driver
13:26:03.060 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 17.0 (TID 85) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:26:03.060 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 17.0 (TID 84) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:26:03.061 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 17.0 (TID 88) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:26:03.061 [Executor task launch worker for task 9.0 in stage 17.0 (TID 89)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.061 [Executor task launch worker for task 9.0 in stage 17.0 (TID 89)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.061 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 17.0 (TID 83) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:26:03.061 [Executor task launch worker for task 9.0 in stage 17.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 17.0 (TID 89). 1431 bytes result sent to driver
13:26:03.061 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 17.0 (TID 80) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:26:03.062 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 17.0 (TID 87) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:26:03.062 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 17.0 (TID 81) in 7 ms on 172.20.10.7 (executor driver) (9/10)
13:26:03.062 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 17.0 (TID 89) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:26:03.062 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
13:26:03.062 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (sortByKey at sparkStreamingSocket.java:61) finished in 0.010 s
13:26:03.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
13:26:03.063 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished
13:26:03.063 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: sortByKey at sparkStreamingSocket.java:61, took 0.038492 s
13:26:03.067 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:61
13:26:03.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 25 (reduceByKey at sparkStreamingSocket.java:57) as input to shuffle 7
13:26:03.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (collect at sparkStreamingSocket.java:61) with 2 output partitions
13:26:03.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (collect at sparkStreamingSocket.java:61)
13:26:03.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)
13:26:03.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 19)
13:26:03.068 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:57), which has no missing parents
13:26:03.069 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:26:03.070 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:26:03.070 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.20.10.7:49892 (size: 3.2 KiB, free: 2004.5 MiB)
13:26:03.070 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1513
13:26:03.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:57) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:26:03.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 10 tasks resource profile 0
13:26:03.072 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 19.0 (TID 90) (172.20.10.7, executor driver, partition 4, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 19.0 (TID 91) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 92) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 93) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 94) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 95) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 19.0 (TID 96) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 19.0 (TID 97) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 19.0 (TID 98) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.073 [Executor task launch worker for task 4.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 19.0 (TID 90)
13:26:03.073 [Executor task launch worker for task 1.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 93)
13:26:03.073 [Executor task launch worker for task 5.0 in stage 19.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 19.0 (TID 96)
13:26:03.073 [Executor task launch worker for task 8.0 in stage 19.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 19.0 (TID 98)
13:26:03.073 [Executor task launch worker for task 0.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 92)
13:26:03.073 [Executor task launch worker for task 7.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 19.0 (TID 91)
13:26:03.073 [Executor task launch worker for task 6.0 in stage 19.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 19.0 (TID 97)
13:26:03.073 [Executor task launch worker for task 3.0 in stage 19.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 95)
13:26:03.073 [Executor task launch worker for task 2.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 94)
13:26:03.076 [Executor task launch worker for task 8.0 in stage 19.0 (TID 98)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 8.0 in stage 19.0 (TID 98)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.076 [Executor task launch worker for task 5.0 in stage 19.0 (TID 96)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 5.0 in stage 19.0 (TID 96)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.076 [Executor task launch worker for task 2.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 4.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 4.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.076 [Executor task launch worker for task 3.0 in stage 19.0 (TID 95)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 1.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 7.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 6 (774.0 B) non-empty blocks including 6 (774.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 3.0 in stage 19.0 (TID 95)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.076 [Executor task launch worker for task 2.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.077 [Executor task launch worker for task 7.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.077 [Executor task launch worker for task 0.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.077 [Executor task launch worker for task 0.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.077 [Executor task launch worker for task 6.0 in stage 19.0 (TID 97)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.076 [Executor task launch worker for task 1.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.077 [Executor task launch worker for task 6.0 in stage 19.0 (TID 97)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.077 [Executor task launch worker for task 5.0 in stage 19.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 19.0 (TID 96). 1341 bytes result sent to driver
13:26:03.077 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 19.0 (TID 99) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:26:03.078 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 19.0 (TID 96) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:26:03.078 [Executor task launch worker for task 9.0 in stage 19.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 19.0 (TID 99)
13:26:03.078 [Executor task launch worker for task 0.0 in stage 19.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 92). 1341 bytes result sent to driver
13:26:03.078 [Executor task launch worker for task 2.0 in stage 19.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 94). 1341 bytes result sent to driver
13:26:03.078 [Executor task launch worker for task 8.0 in stage 19.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 19.0 (TID 98). 1341 bytes result sent to driver
13:26:03.079 [Executor task launch worker for task 6.0 in stage 19.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 19.0 (TID 97). 1341 bytes result sent to driver
13:26:03.079 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 92) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:26:03.079 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 19.0 (TID 98) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:26:03.079 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 94) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:26:03.079 [Executor task launch worker for task 1.0 in stage 19.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 93). 1341 bytes result sent to driver
13:26:03.079 [Executor task launch worker for task 9.0 in stage 19.0 (TID 99)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.079 [Executor task launch worker for task 9.0 in stage 19.0 (TID 99)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.080 [Executor task launch worker for task 3.0 in stage 19.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 95). 1341 bytes result sent to driver
13:26:03.080 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 19.0 (TID 97) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:26:03.080 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 93) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:26:03.080 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 95) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:26:03.080 [Executor task launch worker for task 4.0 in stage 19.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 19.0 (TID 90). 1470 bytes result sent to driver
13:26:03.081 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 19.0 (TID 90) in 9 ms on 172.20.10.7 (executor driver) (8/10)
13:26:03.081 [Executor task launch worker for task 9.0 in stage 19.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 19.0 (TID 99). 1298 bytes result sent to driver
13:26:03.081 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 19.0 (TID 99) in 4 ms on 172.20.10.7 (executor driver) (9/10)
13:26:03.081 [Executor task launch worker for task 7.0 in stage 19.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 19.0 (TID 91). 1470 bytes result sent to driver
13:26:03.082 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 19.0 (TID 91) in 10 ms on 172.20.10.7 (executor driver) (10/10)
13:26:03.082 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 19 (reduceByKey at sparkStreamingSocket.java:57) finished in 0.013 s
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 20)
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:26:03.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:61), which has no missing parents
13:26:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:26:03.084 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:26:03.084 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.20.10.7:49892 (size: 2.9 KiB, free: 2004.5 MiB)
13:26:03.085 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1513
13:26:03.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:61) (first 15 tasks are for partitions Vector(0, 1))
13:26:03.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 2 tasks resource profile 0
13:26:03.085 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 100) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.085 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 101) (172.20.10.7, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:26:03.086 [Executor task launch worker for task 0.0 in stage 20.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 100)
13:26:03.086 [Executor task launch worker for task 1.0 in stage 20.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 101)
13:26:03.087 [Executor task launch worker for task 0.0 in stage 20.0 (TID 100)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.087 [Executor task launch worker for task 0.0 in stage 20.0 (TID 100)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.087 [Executor task launch worker for task 1.0 in stage 20.0 (TID 101)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:26:03.087 [Executor task launch worker for task 1.0 in stage 20.0 (TID 101)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:26:03.087 [Executor task launch worker for task 0.0 in stage 20.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 100). 1374 bytes result sent to driver
13:26:03.088 [Executor task launch worker for task 1.0 in stage 20.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 101). 1375 bytes result sent to driver
13:26:03.088 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 100) in 3 ms on 172.20.10.7 (executor driver) (1/2)
13:26:03.088 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 101) in 3 ms on 172.20.10.7 (executor driver) (2/2)
13:26:03.088 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
13:26:03.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (collect at sparkStreamingSocket.java:61) finished in 0.005 s
13:26:03.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
13:26:03.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished
13:26:03.089 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: collect at sparkStreamingSocket.java:61, took 0.021613 s
13:26:03.089 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760505963000 ms.0 from job set of time 1760505963000 ms
13:26:03.089 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.089 s for time 1760505963000 ms (execution: 0.074 s)
13:26:03.089 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 18 from persistence list
13:26:03.089 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 17 from persistence list
13:26:03.090 [block-manager-storage-async-thread-pool-51] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:26:03.090 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 16 from persistence list
13:26:03.090 [block-manager-storage-async-thread-pool-53] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:26:03.090 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:26:03.090 [block-manager-storage-async-thread-pool-55] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:26:03.090 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760505963000 ms
13:26:03.090 [block-manager-storage-async-thread-pool-56] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:26:03.092 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505957400 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.093 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505957600 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.093 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505958000 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.093 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505958200 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.094 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505958400 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.094 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505958600 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.094 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505959000 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.094 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760505957000 ms
13:26:03.094 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760505957000 ms
13:26:03.095 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505959200 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.095 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760505959400 on 172.20.10.7:49892 in memory (size: 8.0 B, free: 2004.5 MiB)
13:26:03.799 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:26:03.801 [dispatcher-event-loop-9] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:26:03.801 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:26:03.802 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:26:03.802 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Closed socket to 127.0.0.1:9999
13:26:03.802 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:26:03.803 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:26:03.803 [dispatcher-event-loop-8] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
13:26:03.803 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:26:03.804 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:26:03.802 [Socket Receiver] WARN  org.apache.spark.streaming.dstream.SocketReceiver - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:26:03.805 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:26:03.806 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
13:26:03.806 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:27:42.331 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:27:42.356 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:27:42.388 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:27:42.388 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:27:42.388 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:27:42.388 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:27:42.397 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:27:42.402 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:27:42.402 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:27:42.426 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:27:42.426 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:27:42.426 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:27:42.426 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:27:42.426 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:27:42.531 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49904.
13:27:42.543 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:27:42.559 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:27:42.567 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:27:42.567 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:27:42.568 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:27:42.583 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-b4ca550b-dff2-4a54-82f9-623a09ea2d87
13:27:42.605 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:27:42.612 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:27:42.632 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5973ms to org.sparkproject.jetty.util.log.Slf4jLog
13:27:42.684 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:27:42.693 [main] INFO  org.sparkproject.jetty.server.Server - Started @6034ms
13:27:42.709 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@7c974942{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:27:42.709 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:27:42.719 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d4664d7{/,null,AVAILABLE,@Spark}
13:27:42.756 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:27:42.759 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:27:42.770 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49905.
13:27:42.770 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49905
13:27:42.771 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:27:42.773 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49905, None)
13:27:42.774 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49905 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49905, None)
13:27:42.775 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49905, None)
13:27:42.776 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49905, None)
13:27:42.860 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1d4664d7{/,null,STOPPED,@Spark}
13:27:42.861 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2add4d24{/jobs,null,AVAILABLE,@Spark}
13:27:42.862 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12b5454f{/jobs/json,null,AVAILABLE,@Spark}
13:27:42.862 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@342726f1{/jobs/job,null,AVAILABLE,@Spark}
13:27:42.862 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77134e08{/jobs/job/json,null,AVAILABLE,@Spark}
13:27:42.863 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67110f71{/stages,null,AVAILABLE,@Spark}
13:27:42.863 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20749d9{/stages/json,null,AVAILABLE,@Spark}
13:27:42.864 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c75db8b{/stages/stage,null,AVAILABLE,@Spark}
13:27:42.864 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cd206b5{/stages/stage/json,null,AVAILABLE,@Spark}
13:27:42.864 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a137d7a{/stages/pool,null,AVAILABLE,@Spark}
13:27:42.865 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@468be356{/stages/pool/json,null,AVAILABLE,@Spark}
13:27:42.865 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4df39a88{/storage,null,AVAILABLE,@Spark}
13:27:42.865 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bec2275{/storage/json,null,AVAILABLE,@Spark}
13:27:42.865 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60acd609{/storage/rdd,null,AVAILABLE,@Spark}
13:27:42.866 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f49e266{/storage/rdd/json,null,AVAILABLE,@Spark}
13:27:42.866 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@598260a6{/environment,null,AVAILABLE,@Spark}
13:27:42.866 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281ce6bb{/environment/json,null,AVAILABLE,@Spark}
13:27:42.867 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a1f8def{/executors,null,AVAILABLE,@Spark}
13:27:42.867 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e8c34a0{/executors/json,null,AVAILABLE,@Spark}
13:27:42.867 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7aead3af{/executors/threadDump,null,AVAILABLE,@Spark}
13:27:42.868 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55ec556{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:27:42.871 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37ce3644{/static,null,AVAILABLE,@Spark}
13:27:42.872 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1706a5c9{/,null,AVAILABLE,@Spark}
13:27:42.872 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d178d55{/api,null,AVAILABLE,@Spark}
13:27:42.873 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33db72bd{/jobs/job/kill,null,AVAILABLE,@Spark}
13:27:42.873 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31e04b13{/stages/stage/kill,null,AVAILABLE,@Spark}
13:27:42.875 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@455824ad{/metrics/json,null,AVAILABLE,@Spark}
13:27:43.034 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:27:43.035 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@4d46c6a6
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Slide time = 3000 ms
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Storage level = Serialized 1x Replicated
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Checkpoint interval = null
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Remember interval = 3000 ms
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@1d5ccb5f
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Slide time = 3000 ms
13:27:43.036 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 3000 ms
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@31ae9ff
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 3000 ms
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 3000 ms
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@50c8a2e8
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:27:43.037 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2bc03da1
13:27:43.061 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760506065000
13:27:43.061 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760506065000 ms
13:27:43.062 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:27:43.063 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60cf62ad{/streaming,null,AVAILABLE,@Spark}
13:27:43.064 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ac4ccad{/streaming/json,null,AVAILABLE,@Spark}
13:27:43.064 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323f3c96{/streaming/batch,null,AVAILABLE,@Spark}
13:27:43.065 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d92e{/streaming/batch/json,null,AVAILABLE,@Spark}
13:27:43.065 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0d349{/static/streaming,null,AVAILABLE,@Spark}
13:27:43.065 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:27:43.081 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:27:43.083 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:39) with 1 output partitions
13:27:43.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:39)
13:27:43.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:27:43.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:27:43.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:27:43.139 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:27:43.292 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:27:43.293 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49905 (size: 35.5 KiB, free: 2004.6 MiB)
13:27:43.294 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:27:43.301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:27:43.301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:27:43.320 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:27:43.326 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:27:43.443 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760506063600
13:27:43.443 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:27:43.443 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:27:43.445 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49904
13:27:43.445 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:27:43.445 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:27:43.446 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connected to 127.0.0.1:9999
13:27:43.446 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:27:43.446 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:27:45.045 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506065000 ms
13:27:45.046 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506065000 ms.0 from job set of time 1760506065000 ms
13:27:45.067 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:27:45.075 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 0
13:27:45.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:27:45.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (sortByKey at sparkStreamingSocket.java:75)
13:27:45.076 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
13:27:45.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:27:45.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:45.082 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:27:45.082 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.5 MiB)
13:27:45.083 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.10.7:49905 (size: 3.6 KiB, free: 2004.6 MiB)
13:27:45.084 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:27:45.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:45.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 10 tasks resource profile 0
13:27:45.085 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 1) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.085 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 2) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 3) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 4) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 5) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 6) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 7) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 8) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.086 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 9) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.087 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 2)
13:27:45.087 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 4)
13:27:45.087 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 1)
13:27:45.087 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 3)
13:27:45.087 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 5)
13:27:45.088 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 6)
13:27:45.088 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 7)
13:27:45.089 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 8)
13:27:45.089 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 9)
13:27:45.182 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.182 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.183 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.183 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.183 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.183 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.183 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.184 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:27:45.184 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
13:27:45.184 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
13:27:45.184 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
13:27:45.204 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 1). 1560 bytes result sent to driver
13:27:45.205 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 7). 1560 bytes result sent to driver
13:27:45.205 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 5). 1560 bytes result sent to driver
13:27:45.204 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 3). 1560 bytes result sent to driver
13:27:45.205 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 4). 1560 bytes result sent to driver
13:27:45.206 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 2). 1560 bytes result sent to driver
13:27:45.206 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 9). 1560 bytes result sent to driver
13:27:45.206 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 6). 1560 bytes result sent to driver
13:27:45.206 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 8). 1560 bytes result sent to driver
13:27:45.206 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 10) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.206 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 10)
13:27:45.208 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 5) in 122 ms on 172.20.10.7 (executor driver) (1/10)
13:27:45.209 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.209 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.209 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 3) in 124 ms on 172.20.10.7 (executor driver) (2/10)
13:27:45.209 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 1) in 124 ms on 172.20.10.7 (executor driver) (3/10)
13:27:45.209 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 4) in 123 ms on 172.20.10.7 (executor driver) (4/10)
13:27:45.210 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 7) in 124 ms on 172.20.10.7 (executor driver) (5/10)
13:27:45.210 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 10). 1517 bytes result sent to driver
13:27:45.210 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 2) in 125 ms on 172.20.10.7 (executor driver) (6/10)
13:27:45.210 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 9) in 124 ms on 172.20.10.7 (executor driver) (7/10)
13:27:45.211 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 6) in 125 ms on 172.20.10.7 (executor driver) (8/10)
13:27:45.211 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 8) in 125 ms on 172.20.10.7 (executor driver) (9/10)
13:27:45.211 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 10) in 5 ms on 172.20.10.7 (executor driver) (10/10)
13:27:45.211 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13:27:45.215 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (sortByKey at sparkStreamingSocket.java:75) finished in 0.134 s
13:27:45.216 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:45.216 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
13:27:45.216 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: sortByKey at sparkStreamingSocket.java:75, took 0.149166 s
13:27:45.222 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:27:45.222 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 1
13:27:45.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:27:45.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at sparkStreamingSocket.java:75)
13:27:45.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
13:27:45.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
13:27:45.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:27:45.226 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.5 KiB, free 2004.5 MiB)
13:27:45.226 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:27:45.227 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.10.7:49905 (size: 3.2 KiB, free: 2004.6 MiB)
13:27:45.227 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
13:27:45.227 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:45.227 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 10 tasks resource profile 0
13:27:45.228 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 11) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.228 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 12) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 13) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 14) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 15) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 16) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 4.0 (TID 17) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.229 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 4.0 (TID 18) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.230 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 4.0 (TID 19) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.230 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 13)
13:27:45.230 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 4.0 (TID 15)
13:27:45.230 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 4.0 (TID 16)
13:27:45.230 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 14)
13:27:45.230 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 4.0 (TID 19)
13:27:45.230 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 12)
13:27:45.230 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 4.0 (TID 17)
13:27:45.230 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 11)
13:27:45.230 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 4.0 (TID 18)
13:27:45.237 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.237 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.237 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.243 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 4.0 (TID 15). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 13). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 4.0 (TID 16). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 12). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 11). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 4.0 (TID 19). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 14). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 4.0 (TID 18). 1340 bytes result sent to driver
13:27:45.243 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 4.0 (TID 17). 1340 bytes result sent to driver
13:27:45.244 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 4.0 (TID 20) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:45.244 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 4.0 (TID 20)
13:27:45.244 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 15) in 15 ms on 172.20.10.7 (executor driver) (1/10)
13:27:45.245 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 13) in 16 ms on 172.20.10.7 (executor driver) (2/10)
13:27:45.245 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 16) in 16 ms on 172.20.10.7 (executor driver) (3/10)
13:27:45.245 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 12) in 17 ms on 172.20.10.7 (executor driver) (4/10)
13:27:45.246 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.246 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.246 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 11) in 18 ms on 172.20.10.7 (executor driver) (5/10)
13:27:45.246 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 4.0 (TID 19) in 17 ms on 172.20.10.7 (executor driver) (6/10)
13:27:45.247 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 14) in 17 ms on 172.20.10.7 (executor driver) (7/10)
13:27:45.247 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 4.0 (TID 20). 1340 bytes result sent to driver
13:27:45.248 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 4.0 (TID 18) in 19 ms on 172.20.10.7 (executor driver) (8/10)
13:27:45.248 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 4.0 (TID 17) in 19 ms on 172.20.10.7 (executor driver) (9/10)
13:27:45.248 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 4.0 (TID 20) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:27:45.248 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
13:27:45.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.025 s
13:27:45.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:45.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:45.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
13:27:45.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:45.250 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:45.251 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:27:45.252 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:27:45.252 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.20.10.7:49905 (size: 2.9 KiB, free: 2004.6 MiB)
13:27:45.252 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
13:27:45.253 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:27:45.253 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
13:27:45.253 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 21) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:45.253 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 21)
13:27:45.255 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:45.255 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:45.261 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 21). 1228 bytes result sent to driver
13:27:45.261 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 21) in 8 ms on 172.20.10.7 (executor driver) (1/1)
13:27:45.261 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
13:27:45.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at sparkStreamingSocket.java:75) finished in 0.011 s
13:27:45.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:45.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
13:27:45.262 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at sparkStreamingSocket.java:75, took 0.040410 s
13:27:45.263 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506065000 ms.0 from job set of time 1760506065000 ms
13:27:45.263 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.263 s for time 1760506065000 ms (execution: 0.218 s)
13:27:45.266 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:27:45.266 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:27:48.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506068000 ms
13:27:48.015 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506068000 ms.0 from job set of time 1760506068000 ms
13:27:48.026 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:27:48.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 2
13:27:48.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:27:48.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (sortByKey at sparkStreamingSocket.java:75)
13:27:48.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
13:27:48.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:27:48.029 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:48.032 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:27:48.033 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:27:48.033 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.20.10.7:49905 (size: 3.6 KiB, free: 2004.6 MiB)
13:27:48.034 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:27:48.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:48.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 10 tasks resource profile 0
13:27:48.035 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 22) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.035 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 23) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 24) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 25) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 26) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 27) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.036 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 28) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.037 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 29) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.037 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 7.0 (TID 30) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.037 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 22)
13:27:48.037 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 24)
13:27:48.037 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 25)
13:27:48.037 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 7.0 (TID 26)
13:27:48.037 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 7.0 (TID 28)
13:27:48.037 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 23)
13:27:48.037 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 7.0 (TID 27)
13:27:48.037 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 7.0 (TID 30)
13:27:48.037 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 7.0 (TID 29)
13:27:48.040 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.040 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.041 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.040 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.041 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.041 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:48.041 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.040 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.041 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:48.040 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.041 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.041 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.042 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:48.043 [Executor task launch worker for task 6.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 7.0 (TID 28). 1517 bytes result sent to driver
13:27:48.043 [Executor task launch worker for task 3.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 25). 1517 bytes result sent to driver
13:27:48.043 [Executor task launch worker for task 0.0 in stage 7.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 22). 1517 bytes result sent to driver
13:27:48.043 [Executor task launch worker for task 2.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 24). 1517 bytes result sent to driver
13:27:48.044 [Executor task launch worker for task 8.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 7.0 (TID 30). 1517 bytes result sent to driver
13:27:48.044 [Executor task launch worker for task 4.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 7.0 (TID 26). 1517 bytes result sent to driver
13:27:48.044 [Executor task launch worker for task 7.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 7.0 (TID 29). 1517 bytes result sent to driver
13:27:48.044 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 7.0 (TID 31) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.044 [Executor task launch worker for task 5.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 7.0 (TID 27). 1517 bytes result sent to driver
13:27:48.044 [Executor task launch worker for task 1.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 23). 1517 bytes result sent to driver
13:27:48.044 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 7.0 (TID 31)
13:27:48.044 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 25) in 8 ms on 172.20.10.7 (executor driver) (1/10)
13:27:48.045 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 7.0 (TID 28) in 9 ms on 172.20.10.7 (executor driver) (2/10)
13:27:48.045 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 22) in 10 ms on 172.20.10.7 (executor driver) (3/10)
13:27:48.045 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 24) in 9 ms on 172.20.10.7 (executor driver) (4/10)
13:27:48.045 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 7.0 (TID 30) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:27:48.046 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.046 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.046 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 26) in 10 ms on 172.20.10.7 (executor driver) (6/10)
13:27:48.046 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 7.0 (TID 29) in 10 ms on 172.20.10.7 (executor driver) (7/10)
13:27:48.047 [Executor task launch worker for task 9.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 7.0 (TID 31). 1474 bytes result sent to driver
13:27:48.047 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 27) in 11 ms on 172.20.10.7 (executor driver) (8/10)
13:27:48.047 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 23) in 12 ms on 172.20.10.7 (executor driver) (9/10)
13:27:48.047 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 7.0 (TID 31) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:27:48.048 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
13:27:48.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (sortByKey at sparkStreamingSocket.java:75) finished in 0.017 s
13:27:48.049 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:48.049 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
13:27:48.049 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: sortByKey at sparkStreamingSocket.java:75, took 0.022714 s
13:27:48.053 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:27:48.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 3
13:27:48.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:27:48.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (collect at sparkStreamingSocket.java:75)
13:27:48.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
13:27:48.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
13:27:48.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:27:48.057 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:27:48.058 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:27:48.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.20.10.7:49905 (size: 3.2 KiB, free: 2004.5 MiB)
13:27:48.058 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
13:27:48.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:48.059 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 10 tasks resource profile 0
13:27:48.059 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 32) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.059 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 33) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 34) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 35) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 9.0 (TID 36) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 9.0 (TID 37) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.060 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 9.0 (TID 38) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.061 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 9.0 (TID 39) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.061 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 9.0 (TID 40) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.061 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 33)
13:27:48.061 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 9.0 (TID 37)
13:27:48.061 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 35)
13:27:48.061 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 34)
13:27:48.061 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 9.0 (TID 38)
13:27:48.061 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 9.0 (TID 40)
13:27:48.061 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 9.0 (TID 36)
13:27:48.061 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 32)
13:27:48.061 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 9.0 (TID 39)
13:27:48.064 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.064 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.064 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.064 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.064 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.064 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.064 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.064 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.064 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.065 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.065 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.064 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.065 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.065 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:48.065 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.065 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.065 [Executor task launch worker for task 4.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 9.0 (TID 36). 1340 bytes result sent to driver
13:27:48.065 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.065 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.066 [Executor task launch worker for task 7.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 9.0 (TID 39). 1340 bytes result sent to driver
13:27:48.066 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 9.0 (TID 41) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:48.066 [Executor task launch worker for task 8.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 9.0 (TID 40). 1340 bytes result sent to driver
13:27:48.066 [Executor task launch worker for task 3.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 35). 1340 bytes result sent to driver
13:27:48.066 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 9.0 (TID 36) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:27:48.066 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 9.0 (TID 41)
13:27:48.067 [Executor task launch worker for task 0.0 in stage 9.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 32). 1340 bytes result sent to driver
13:27:48.067 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 9.0 (TID 39) in 7 ms on 172.20.10.7 (executor driver) (2/10)
13:27:48.068 [Executor task launch worker for task 5.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 9.0 (TID 37). 1340 bytes result sent to driver
13:27:48.068 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.068 [Executor task launch worker for task 2.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 34). 1340 bytes result sent to driver
13:27:48.068 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.068 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 35) in 8 ms on 172.20.10.7 (executor driver) (3/10)
13:27:48.068 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 9.0 (TID 40) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:27:48.068 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 32) in 9 ms on 172.20.10.7 (executor driver) (5/10)
13:27:48.069 [Executor task launch worker for task 6.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 9.0 (TID 38). 1340 bytes result sent to driver
13:27:48.069 [Executor task launch worker for task 1.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 33). 1340 bytes result sent to driver
13:27:48.069 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 34) in 10 ms on 172.20.10.7 (executor driver) (6/10)
13:27:48.070 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 9.0 (TID 38) in 10 ms on 172.20.10.7 (executor driver) (7/10)
13:27:48.070 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 9.0 (TID 37) in 10 ms on 172.20.10.7 (executor driver) (8/10)
13:27:48.070 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 33) in 11 ms on 172.20.10.7 (executor driver) (9/10)
13:27:48.070 [Executor task launch worker for task 9.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 9.0 (TID 41). 1297 bytes result sent to driver
13:27:48.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 9.0 (TID 41) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:27:48.070 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.015 s
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:48.071 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:48.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:27:48.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:27:48.073 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.20.10.7:49905 (size: 2.9 KiB, free: 2004.5 MiB)
13:27:48.073 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
13:27:48.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:27:48.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0
13:27:48.074 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 42) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:48.074 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 42)
13:27:48.075 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:48.075 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:48.076 [Executor task launch worker for task 0.0 in stage 10.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 42). 1185 bytes result sent to driver
13:27:48.076 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 42) in 2 ms on 172.20.10.7 (executor driver) (1/1)
13:27:48.076 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
13:27:48.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (collect at sparkStreamingSocket.java:75) finished in 0.006 s
13:27:48.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:48.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
13:27:48.077 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: collect at sparkStreamingSocket.java:75, took 0.024043 s
13:27:48.078 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506068000 ms.0 from job set of time 1760506068000 ms
13:27:48.078 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.078 s for time 1760506068000 ms (execution: 0.063 s)
13:27:48.078 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 4 from persistence list
13:27:48.083 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 3 from persistence list
13:27:48.083 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:27:48.084 [block-manager-storage-async-thread-pool-1] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:27:48.084 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 2 from persistence list
13:27:48.084 [block-manager-storage-async-thread-pool-2] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:27:48.084 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:27:48.085 [block-manager-storage-async-thread-pool-4] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:27:48.085 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760506068000 ms
13:27:48.085 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:27:48.085 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:27:51.014 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506071000 ms
13:27:51.015 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506071000 ms.0 from job set of time 1760506071000 ms
13:27:51.023 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:27:51.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 17 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 4
13:27:51.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:27:51.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (sortByKey at sparkStreamingSocket.java:75)
13:27:51.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 11)
13:27:51.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:27:51.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:51.030 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:27:51.031 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:27:51.032 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.20.10.7:49905 (size: 3.6 KiB, free: 2004.5 MiB)
13:27:51.033 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
13:27:51.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 12 (MapPartitionsRDD[20] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:51.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 10 tasks resource profile 0
13:27:51.034 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 43) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.034 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 44) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 45) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 46) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 47) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 48) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 49) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 50) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.035 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 12.0 (TID 51) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.036 [Executor task launch worker for task 0.0 in stage 12.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 43)
13:27:51.036 [Executor task launch worker for task 3.0 in stage 12.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 46)
13:27:51.036 [Executor task launch worker for task 1.0 in stage 12.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 44)
13:27:51.036 [Executor task launch worker for task 4.0 in stage 12.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 47)
13:27:51.036 [Executor task launch worker for task 2.0 in stage 12.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 45)
13:27:51.036 [Executor task launch worker for task 8.0 in stage 12.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 12.0 (TID 51)
13:27:51.036 [Executor task launch worker for task 7.0 in stage 12.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 50)
13:27:51.036 [Executor task launch worker for task 5.0 in stage 12.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 48)
13:27:51.036 [Executor task launch worker for task 6.0 in stage 12.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 49)
13:27:51.039 [Executor task launch worker for task 8.0 in stage 12.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.039 [Executor task launch worker for task 8.0 in stage 12.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.039 [Executor task launch worker for task 3.0 in stage 12.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.039 [Executor task launch worker for task 3.0 in stage 12.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.039 [Executor task launch worker for task 6.0 in stage 12.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.039 [Executor task launch worker for task 5.0 in stage 12.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.039 [Executor task launch worker for task 6.0 in stage 12.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.039 [Executor task launch worker for task 7.0 in stage 12.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.040 [Executor task launch worker for task 7.0 in stage 12.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.040 [Executor task launch worker for task 0.0 in stage 12.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.039 [Executor task launch worker for task 5.0 in stage 12.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.039 [Executor task launch worker for task 1.0 in stage 12.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.040 [Executor task launch worker for task 2.0 in stage 12.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.040 [Executor task launch worker for task 1.0 in stage 12.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:51.040 [Executor task launch worker for task 0.0 in stage 12.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.040 [Executor task launch worker for task 4.0 in stage 12.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.040 [Executor task launch worker for task 4.0 in stage 12.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.040 [Executor task launch worker for task 6.0 in stage 12.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 49). 1517 bytes result sent to driver
13:27:51.040 [Executor task launch worker for task 8.0 in stage 12.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 12.0 (TID 51). 1517 bytes result sent to driver
13:27:51.040 [Executor task launch worker for task 7.0 in stage 12.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 50). 1517 bytes result sent to driver
13:27:51.041 [Executor task launch worker for task 0.0 in stage 12.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 43). 1517 bytes result sent to driver
13:27:51.041 [Executor task launch worker for task 3.0 in stage 12.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 46). 1517 bytes result sent to driver
13:27:51.041 [Executor task launch worker for task 5.0 in stage 12.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 48). 1517 bytes result sent to driver
13:27:51.041 [Executor task launch worker for task 4.0 in stage 12.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 47). 1517 bytes result sent to driver
13:27:51.041 [Executor task launch worker for task 1.0 in stage 12.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 44). 1517 bytes result sent to driver
13:27:51.040 [Executor task launch worker for task 2.0 in stage 12.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.041 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 12.0 (TID 52) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.041 [Executor task launch worker for task 9.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 12.0 (TID 52)
13:27:51.041 [Executor task launch worker for task 2.0 in stage 12.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 45). 1517 bytes result sent to driver
13:27:51.042 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 49) in 7 ms on 172.20.10.7 (executor driver) (1/10)
13:27:51.042 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 12.0 (TID 51) in 7 ms on 172.20.10.7 (executor driver) (2/10)
13:27:51.042 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 50) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:27:51.043 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 43) in 9 ms on 172.20.10.7 (executor driver) (4/10)
13:27:51.043 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 46) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:27:51.043 [Executor task launch worker for task 9.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.043 [Executor task launch worker for task 9.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.043 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 48) in 8 ms on 172.20.10.7 (executor driver) (6/10)
13:27:51.043 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 47) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:27:51.044 [Executor task launch worker for task 9.0 in stage 12.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 12.0 (TID 52). 1474 bytes result sent to driver
13:27:51.044 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 44) in 10 ms on 172.20.10.7 (executor driver) (8/10)
13:27:51.044 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 45) in 10 ms on 172.20.10.7 (executor driver) (9/10)
13:27:51.044 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 12.0 (TID 52) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:27:51.044 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
13:27:51.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 (sortByKey at sparkStreamingSocket.java:75) finished in 0.017 s
13:27:51.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:51.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished
13:27:51.045 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: sortByKey at sparkStreamingSocket.java:75, took 0.022144 s
13:27:51.050 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:27:51.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 5
13:27:51.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:27:51.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (collect at sparkStreamingSocket.java:75)
13:27:51.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
13:27:51.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
13:27:51.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:27:51.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:27:51.053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:27:51.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.20.10.7:49905 (size: 3.2 KiB, free: 2004.5 MiB)
13:27:51.054 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1513
13:27:51.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 14 (ShuffledRDD[18] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:51.054 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 10 tasks resource profile 0
13:27:51.055 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 53) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.055 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 54) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.055 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 55) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.055 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 56) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 57) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 58) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 59) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 60) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 14.0 (TID 61) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.056 [Executor task launch worker for task 1.0 in stage 14.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 54)
13:27:51.056 [Executor task launch worker for task 2.0 in stage 14.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 55)
13:27:51.056 [Executor task launch worker for task 6.0 in stage 14.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 59)
13:27:51.056 [Executor task launch worker for task 5.0 in stage 14.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 58)
13:27:51.056 [Executor task launch worker for task 7.0 in stage 14.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 60)
13:27:51.056 [Executor task launch worker for task 3.0 in stage 14.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 56)
13:27:51.056 [Executor task launch worker for task 8.0 in stage 14.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 14.0 (TID 61)
13:27:51.056 [Executor task launch worker for task 0.0 in stage 14.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 53)
13:27:51.056 [Executor task launch worker for task 4.0 in stage 14.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 57)
13:27:51.059 [Executor task launch worker for task 3.0 in stage 14.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.059 [Executor task launch worker for task 8.0 in stage 14.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.059 [Executor task launch worker for task 6.0 in stage 14.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.059 [Executor task launch worker for task 6.0 in stage 14.0 (TID 59)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.059 [Executor task launch worker for task 3.0 in stage 14.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.059 [Executor task launch worker for task 8.0 in stage 14.0 (TID 61)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.059 [Executor task launch worker for task 2.0 in stage 14.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.060 [Executor task launch worker for task 7.0 in stage 14.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.060 [Executor task launch worker for task 1.0 in stage 14.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.060 [Executor task launch worker for task 7.0 in stage 14.0 (TID 60)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.060 [Executor task launch worker for task 5.0 in stage 14.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.060 [Executor task launch worker for task 1.0 in stage 14.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.060 [Executor task launch worker for task 0.0 in stage 14.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.060 [Executor task launch worker for task 2.0 in stage 14.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.060 [Executor task launch worker for task 0.0 in stage 14.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.060 [Executor task launch worker for task 6.0 in stage 14.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 59). 1340 bytes result sent to driver
13:27:51.060 [Executor task launch worker for task 5.0 in stage 14.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.061 [Executor task launch worker for task 8.0 in stage 14.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 14.0 (TID 61). 1340 bytes result sent to driver
13:27:51.061 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 14.0 (TID 62) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:51.061 [Executor task launch worker for task 4.0 in stage 14.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.061 [Executor task launch worker for task 4.0 in stage 14.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.061 [Executor task launch worker for task 9.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 14.0 (TID 62)
13:27:51.061 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 14.0 (TID 61) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:27:51.061 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 59) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:27:51.061 [Executor task launch worker for task 7.0 in stage 14.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 60). 1340 bytes result sent to driver
13:27:51.062 [Executor task launch worker for task 0.0 in stage 14.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 53). 1340 bytes result sent to driver
13:27:51.062 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 60) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:27:51.063 [Executor task launch worker for task 3.0 in stage 14.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 56). 1340 bytes result sent to driver
13:27:51.063 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 53) in 8 ms on 172.20.10.7 (executor driver) (4/10)
13:27:51.063 [Executor task launch worker for task 2.0 in stage 14.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 55). 1340 bytes result sent to driver
13:27:51.063 [Executor task launch worker for task 9.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.063 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 56) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:27:51.063 [Executor task launch worker for task 4.0 in stage 14.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 57). 1340 bytes result sent to driver
13:27:51.063 [Executor task launch worker for task 9.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.064 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 55) in 9 ms on 172.20.10.7 (executor driver) (6/10)
13:27:51.064 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 57) in 9 ms on 172.20.10.7 (executor driver) (7/10)
13:27:51.064 [Executor task launch worker for task 1.0 in stage 14.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 54). 1340 bytes result sent to driver
13:27:51.064 [Executor task launch worker for task 5.0 in stage 14.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 58). 1340 bytes result sent to driver
13:27:51.065 [Executor task launch worker for task 9.0 in stage 14.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 14.0 (TID 62). 1340 bytes result sent to driver
13:27:51.065 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 54) in 10 ms on 172.20.10.7 (executor driver) (8/10)
13:27:51.065 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 58) in 9 ms on 172.20.10.7 (executor driver) (9/10)
13:27:51.065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 14.0 (TID 62) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:27:51.065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.014 s
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:51.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:51.068 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:27:51.069 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:27:51.069 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.20.10.7:49905 (size: 2.9 KiB, free: 2004.5 MiB)
13:27:51.069 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1513
13:27:51.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[21] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:27:51.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0
13:27:51.070 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 63) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:51.070 [Executor task launch worker for task 0.0 in stage 15.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 63)
13:27:51.072 [Executor task launch worker for task 0.0 in stage 15.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:51.072 [Executor task launch worker for task 0.0 in stage 15.0 (TID 63)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:51.072 [Executor task launch worker for task 0.0 in stage 15.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 63). 1185 bytes result sent to driver
13:27:51.073 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 63) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:27:51.073 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
13:27:51.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (collect at sparkStreamingSocket.java:75) finished in 0.006 s
13:27:51.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:51.073 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished
13:27:51.074 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: collect at sparkStreamingSocket.java:75, took 0.023588 s
13:27:51.074 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506071000 ms.0 from job set of time 1760506071000 ms
13:27:51.074 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 11 from persistence list
13:27:51.075 [block-manager-storage-async-thread-pool-12] INFO  org.apache.spark.storage.BlockManager - Removing RDD 11
13:27:51.075 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 10 from persistence list
13:27:51.075 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.074 s for time 1760506071000 ms (execution: 0.060 s)
13:27:51.075 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 9 from persistence list
13:27:51.075 [block-manager-storage-async-thread-pool-14] INFO  org.apache.spark.storage.BlockManager - Removing RDD 10
13:27:51.076 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 8 from persistence list
13:27:51.076 [block-manager-storage-async-thread-pool-18] INFO  org.apache.spark.storage.BlockManager - Removing RDD 9
13:27:51.077 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[8] at socketTextStream at sparkStreamingSocket.java:31 of time 1760506071000 ms
13:27:51.077 [block-manager-storage-async-thread-pool-19] INFO  org.apache.spark.storage.BlockManager - Removing RDD 8
13:27:51.077 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760506065000 ms
13:27:51.077 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760506065000 ms
13:27:53.410 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506073200 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:53.411 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506073200 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.5 MiB)
13:27:53.417 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:53.418 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506073200 replicated to only 0 peer(s) instead of 1 peers
13:27:53.424 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506073200
13:27:53.607 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506073400 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:53.609 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506073400 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.5 MiB)
13:27:53.610 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:53.610 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506073400 replicated to only 0 peer(s) instead of 1 peers
13:27:53.611 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506073400
13:27:53.807 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506073600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:53.808 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506073600 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.5 MiB)
13:27:53.809 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:53.809 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506073600 replicated to only 0 peer(s) instead of 1 peers
13:27:53.810 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506073600
13:27:54.005 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506073800 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:54.006 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506073800 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.5 MiB)
13:27:54.006 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:54.007 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506073800 replicated to only 0 peer(s) instead of 1 peers
13:27:54.007 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506073800
13:27:54.019 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506074000 ms
13:27:54.020 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506074000 ms.0 from job set of time 1760506074000 ms
13:27:54.029 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:27:54.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 24 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 6
13:27:54.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:27:54.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (sortByKey at sparkStreamingSocket.java:75)
13:27:54.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
13:27:54.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
13:27:54.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:63), which has no missing parents
13:27:54.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:27:54.043 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:27:54.044 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.20.10.7:49905 (size: 3.4 KiB, free: 2004.5 MiB)
13:27:54.044 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1513
13:27:54.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[24] at mapToPair at sparkStreamingSocket.java:63) (first 15 tasks are for partitions Vector(0, 1, 2))
13:27:54.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 3 tasks resource profile 0
13:27:54.046 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 64) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:54.046 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 65) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:54.046 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 66) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:54.047 [Executor task launch worker for task 2.0 in stage 16.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 66)
13:27:54.047 [Executor task launch worker for task 1.0 in stage 16.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 65)
13:27:54.047 [Executor task launch worker for task 0.0 in stage 16.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 64)
13:27:54.055 [Executor task launch worker for task 0.0 in stage 16.0 (TID 64)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506073200 locally
13:27:54.055 [Executor task launch worker for task 1.0 in stage 16.0 (TID 65)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506073400 locally
13:27:54.055 [Executor task launch worker for task 2.0 in stage 16.0 (TID 66)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506073600 locally
13:27:54.066 [Executor task launch worker for task 1.0 in stage 16.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 65). 1177 bytes result sent to driver
13:27:54.066 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 65) in 20 ms on 172.20.10.7 (executor driver) (1/3)
13:27:54.066 [Executor task launch worker for task 0.0 in stage 16.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 64). 1177 bytes result sent to driver
13:27:54.067 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 64) in 22 ms on 172.20.10.7 (executor driver) (2/3)
13:27:54.068 [Executor task launch worker for task 2.0 in stage 16.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 66). 1177 bytes result sent to driver
13:27:54.068 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 66) in 22 ms on 172.20.10.7 (executor driver) (3/3)
13:27:54.068 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
13:27:54.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (mapToPair at sparkStreamingSocket.java:63) finished in 0.031 s
13:27:54.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:54.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:54.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
13:27:54.069 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:54.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:54.072 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:27:54.073 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:27:54.074 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.20.10.7:49905 (size: 3.6 KiB, free: 2004.5 MiB)
13:27:54.074 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1513
13:27:54.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 17 (MapPartitionsRDD[27] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:54.074 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 10 tasks resource profile 0
13:27:54.075 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 17.0 (TID 67) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 68) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 17.0 (TID 69) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 17.0 (TID 70) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 17.0 (TID 71) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 17.0 (TID 72) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 17.0 (TID 73) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 17.0 (TID 74) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.076 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 17.0 (TID 75) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.077 [Executor task launch worker for task 3.0 in stage 17.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 17.0 (TID 71)
13:27:54.077 [Executor task launch worker for task 4.0 in stage 17.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 17.0 (TID 72)
13:27:54.077 [Executor task launch worker for task 1.0 in stage 17.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 17.0 (TID 69)
13:27:54.077 [Executor task launch worker for task 7.0 in stage 17.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 17.0 (TID 67)
13:27:54.077 [Executor task launch worker for task 8.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 17.0 (TID 75)
13:27:54.077 [Executor task launch worker for task 6.0 in stage 17.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 17.0 (TID 74)
13:27:54.077 [Executor task launch worker for task 5.0 in stage 17.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 17.0 (TID 73)
13:27:54.077 [Executor task launch worker for task 2.0 in stage 17.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 17.0 (TID 70)
13:27:54.077 [Executor task launch worker for task 0.0 in stage 17.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 68)
13:27:54.079 [Executor task launch worker for task 3.0 in stage 17.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.079 [Executor task launch worker for task 3.0 in stage 17.0 (TID 71)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.079 [Executor task launch worker for task 3.0 in stage 17.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 17.0 (TID 71). 1517 bytes result sent to driver
13:27:54.079 [Executor task launch worker for task 8.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.079 [Executor task launch worker for task 8.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.079 [Executor task launch worker for task 4.0 in stage 17.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.080 [Executor task launch worker for task 4.0 in stage 17.0 (TID 72)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.079 [Executor task launch worker for task 6.0 in stage 17.0 (TID 74)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.080 [Executor task launch worker for task 6.0 in stage 17.0 (TID 74)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.080 [Executor task launch worker for task 0.0 in stage 17.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.080 [Executor task launch worker for task 0.0 in stage 17.0 (TID 68)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.080 [Executor task launch worker for task 1.0 in stage 17.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.080 [Executor task launch worker for task 1.0 in stage 17.0 (TID 69)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.080 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 17.0 (TID 76) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.080 [Executor task launch worker for task 2.0 in stage 17.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.081 [Executor task launch worker for task 2.0 in stage 17.0 (TID 70)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.081 [Executor task launch worker for task 5.0 in stage 17.0 (TID 73)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.081 [Executor task launch worker for task 8.0 in stage 17.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 17.0 (TID 75). 1517 bytes result sent to driver
13:27:54.081 [Executor task launch worker for task 5.0 in stage 17.0 (TID 73)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:54.081 [Executor task launch worker for task 1.0 in stage 17.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 17.0 (TID 69). 1517 bytes result sent to driver
13:27:54.081 [Executor task launch worker for task 2.0 in stage 17.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 17.0 (TID 70). 1517 bytes result sent to driver
13:27:54.081 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 17.0 (TID 71) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:27:54.081 [Executor task launch worker for task 9.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 17.0 (TID 76)
13:27:54.081 [Executor task launch worker for task 7.0 in stage 17.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 (387.0 B) non-empty blocks including 3 (387.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.082 [Executor task launch worker for task 5.0 in stage 17.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 17.0 (TID 73). 1517 bytes result sent to driver
13:27:54.082 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 17.0 (TID 75) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:27:54.082 [Executor task launch worker for task 6.0 in stage 17.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 17.0 (TID 74). 1517 bytes result sent to driver
13:27:54.081 [Executor task launch worker for task 0.0 in stage 17.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 68). 1517 bytes result sent to driver
13:27:54.082 [Executor task launch worker for task 7.0 in stage 17.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:54.082 [Executor task launch worker for task 4.0 in stage 17.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 17.0 (TID 72). 1517 bytes result sent to driver
13:27:54.083 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 17.0 (TID 69) in 6 ms on 172.20.10.7 (executor driver) (3/10)
13:27:54.083 [Executor task launch worker for task 9.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.083 [Executor task launch worker for task 9.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.083 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 17.0 (TID 73) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:27:54.083 [Executor task launch worker for task 9.0 in stage 17.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 17.0 (TID 76). 1474 bytes result sent to driver
13:27:54.084 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 17.0 (TID 70) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:27:54.084 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 17.0 (TID 74) in 8 ms on 172.20.10.7 (executor driver) (6/10)
13:27:54.088 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 17.0 (TID 72) in 8 ms on 172.20.10.7 (executor driver) (7/10)
13:27:54.091 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 68) in 15 ms on 172.20.10.7 (executor driver) (8/10)
13:27:54.091 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 17.0 (TID 76) in 11 ms on 172.20.10.7 (executor driver) (9/10)
13:27:54.098 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.20.10.7:49905 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:27:54.099 [Executor task launch worker for task 7.0 in stage 17.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 17.0 (TID 67). 1564 bytes result sent to driver
13:27:54.100 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 17.0 (TID 67) in 25 ms on 172.20.10.7 (executor driver) (10/10)
13:27:54.100 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
13:27:54.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (sortByKey at sparkStreamingSocket.java:75) finished in 0.030 s
13:27:54.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:54.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 17: Stage finished
13:27:54.101 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: sortByKey at sparkStreamingSocket.java:75, took 0.072005 s
13:27:54.102 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.20.10.7:49905 in memory (size: 3.6 KiB, free: 2004.5 MiB)
13:27:54.106 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.20.10.7:49905 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:27:54.108 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.20.10.7:49905 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:27:54.109 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.20.10.7:49905 in memory (size: 3.2 KiB, free: 2004.5 MiB)
13:27:54.110 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:27:54.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 25 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 7
13:27:54.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:27:54.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (collect at sparkStreamingSocket.java:75)
13:27:54.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)
13:27:54.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 19)
13:27:54.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:27:54.112 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.20.10.7:49905 in memory (size: 2.9 KiB, free: 2004.5 MiB)
13:27:54.115 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:27:54.116 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.20.10.7:49905 in memory (size: 3.4 KiB, free: 2004.6 MiB)
13:27:54.116 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:27:54.116 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.20.10.7:49905 (size: 3.2 KiB, free: 2004.5 MiB)
13:27:54.117 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1513
13:27:54.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 19 (ShuffledRDD[25] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:54.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 10 tasks resource profile 0
13:27:54.118 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 19.0 (TID 77) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.118 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.20.10.7:49905 in memory (size: 2.9 KiB, free: 2004.6 MiB)
13:27:54.118 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 78) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 79) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 80) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 81) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 19.0 (TID 82) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 19.0 (TID 83) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 19.0 (TID 84) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 19.0 (TID 85) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.119 [Executor task launch worker for task 1.0 in stage 19.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 79)
13:27:54.119 [Executor task launch worker for task 4.0 in stage 19.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 19.0 (TID 82)
13:27:54.119 [Executor task launch worker for task 3.0 in stage 19.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 81)
13:27:54.119 [Executor task launch worker for task 2.0 in stage 19.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 80)
13:27:54.119 [Executor task launch worker for task 8.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 19.0 (TID 85)
13:27:54.119 [Executor task launch worker for task 7.0 in stage 19.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 19.0 (TID 77)
13:27:54.119 [Executor task launch worker for task 6.0 in stage 19.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 19.0 (TID 84)
13:27:54.119 [Executor task launch worker for task 5.0 in stage 19.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 19.0 (TID 83)
13:27:54.120 [Executor task launch worker for task 0.0 in stage 19.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 78)
13:27:54.121 [Executor task launch worker for task 1.0 in stage 19.0 (TID 79)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 0.0 in stage 19.0 (TID 78)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 3.0 in stage 19.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 0.0 in stage 19.0 (TID 78)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 1.0 in stage 19.0 (TID 79)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 6.0 in stage 19.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 7.0 in stage 19.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 3 (387.0 B) non-empty blocks including 3 (387.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 3.0 in stage 19.0 (TID 81)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 7.0 in stage 19.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 6.0 in stage 19.0 (TID 84)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.20.10.7:49905 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:27:54.122 [Executor task launch worker for task 8.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 2.0 in stage 19.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 8.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 5.0 in stage 19.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.122 [Executor task launch worker for task 2.0 in stage 19.0 (TID 80)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 5.0 in stage 19.0 (TID 83)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.122 [Executor task launch worker for task 0.0 in stage 19.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 78). 1340 bytes result sent to driver
13:27:54.122 [Executor task launch worker for task 4.0 in stage 19.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.123 [Executor task launch worker for task 3.0 in stage 19.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 81). 1340 bytes result sent to driver
13:27:54.123 [Executor task launch worker for task 4.0 in stage 19.0 (TID 82)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.123 [Executor task launch worker for task 1.0 in stage 19.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 79). 1340 bytes result sent to driver
13:27:54.123 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 19.0 (TID 86) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:54.123 [Executor task launch worker for task 5.0 in stage 19.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 19.0 (TID 83). 1340 bytes result sent to driver
13:27:54.123 [Executor task launch worker for task 9.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 19.0 (TID 86)
13:27:54.123 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 78) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:27:54.124 [Executor task launch worker for task 4.0 in stage 19.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 19.0 (TID 82). 1340 bytes result sent to driver
13:27:54.124 [Executor task launch worker for task 8.0 in stage 19.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 19.0 (TID 85). 1340 bytes result sent to driver
13:27:54.124 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 79) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:27:54.124 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 81) in 5 ms on 172.20.10.7 (executor driver) (3/10)
13:27:54.124 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 19.0 (TID 83) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:27:54.125 [Executor task launch worker for task 2.0 in stage 19.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 80). 1340 bytes result sent to driver
13:27:54.125 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.20.10.7:49905 in memory (size: 3.6 KiB, free: 2004.6 MiB)
13:27:54.125 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 19.0 (TID 82) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:27:54.125 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 19.0 (TID 85) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:27:54.125 [Executor task launch worker for task 9.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.125 [Executor task launch worker for task 9.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.126 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 80) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:27:54.126 [Executor task launch worker for task 6.0 in stage 19.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 19.0 (TID 84). 1340 bytes result sent to driver
13:27:54.126 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 19.0 (TID 84) in 7 ms on 172.20.10.7 (executor driver) (8/10)
13:27:54.127 [Executor task launch worker for task 9.0 in stage 19.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 19.0 (TID 86). 1340 bytes result sent to driver
13:27:54.127 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 19.0 (TID 86) in 4 ms on 172.20.10.7 (executor driver) (9/10)
13:27:54.131 [Executor task launch worker for task 7.0 in stage 19.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 19.0 (TID 77). 1469 bytes result sent to driver
13:27:54.131 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 19.0 (TID 77) in 13 ms on 172.20.10.7 (executor driver) (10/10)
13:27:54.132 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 19 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.020 s
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 20)
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:54.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:54.133 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:27:54.134 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:27:54.134 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.20.10.7:49905 (size: 2.9 KiB, free: 2004.6 MiB)
13:27:54.134 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1513
13:27:54.134 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (ShuffledRDD[28] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:27:54.134 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0
13:27:54.135 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 87) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:54.135 [Executor task launch worker for task 0.0 in stage 20.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 87)
13:27:54.136 [Executor task launch worker for task 0.0 in stage 20.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:54.136 [Executor task launch worker for task 0.0 in stage 20.0 (TID 87)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:54.137 [Executor task launch worker for task 0.0 in stage 20.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 87). 1374 bytes result sent to driver
13:27:54.137 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 87) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:27:54.137 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
13:27:54.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (collect at sparkStreamingSocket.java:75) finished in 0.006 s
13:27:54.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:54.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished
13:27:54.138 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: collect at sparkStreamingSocket.java:75, took 0.028236 s
13:27:54.138 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506074000 ms.0 from job set of time 1760506074000 ms
13:27:54.138 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.138 s for time 1760506074000 ms (execution: 0.118 s)
13:27:54.138 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 18 from persistence list
13:27:54.139 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 17 from persistence list
13:27:54.139 [block-manager-storage-async-thread-pool-69] INFO  org.apache.spark.storage.BlockManager - Removing RDD 18
13:27:54.139 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 16 from persistence list
13:27:54.139 [block-manager-storage-async-thread-pool-70] INFO  org.apache.spark.storage.BlockManager - Removing RDD 17
13:27:54.139 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 15 from persistence list
13:27:54.139 [block-manager-storage-async-thread-pool-73] INFO  org.apache.spark.storage.BlockManager - Removing RDD 16
13:27:54.139 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[15] at socketTextStream at sparkStreamingSocket.java:31 of time 1760506074000 ms
13:27:54.139 [block-manager-storage-async-thread-pool-77] INFO  org.apache.spark.storage.BlockManager - Removing RDD 15
13:27:54.140 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760506068000 ms
13:27:54.140 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760506068000 ms
13:27:54.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506074000 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:54.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506074000 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.6 MiB)
13:27:54.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:54.204 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506074000 replicated to only 0 peer(s) instead of 1 peers
13:27:54.204 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506074000
13:27:54.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506074400 stored as values in memory (estimated size 13.0 B, free 2004.4 MiB)
13:27:54.604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506074400 in memory on 172.20.10.7:49905 (size: 13.0 B, free: 2004.6 MiB)
13:27:54.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:54.605 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506074400 replicated to only 0 peer(s) instead of 1 peers
13:27:54.605 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506074400
13:27:54.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506074600 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:27:54.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506074600 in memory on 172.20.10.7:49905 (size: 10.0 B, free: 2004.6 MiB)
13:27:54.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:54.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506074600 replicated to only 0 peer(s) instead of 1 peers
13:27:54.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506074600
13:27:55.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506074800 stored as values in memory (estimated size 10.0 B, free 2004.4 MiB)
13:27:55.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506074800 in memory on 172.20.10.7:49905 (size: 10.0 B, free: 2004.6 MiB)
13:27:55.003 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:55.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506074800 replicated to only 0 peer(s) instead of 1 peers
13:27:55.004 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506074800
13:27:55.202 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506075000 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:27:55.203 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506075000 in memory on 172.20.10.7:49905 (size: 9.0 B, free: 2004.6 MiB)
13:27:55.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:55.204 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506075000 replicated to only 0 peer(s) instead of 1 peers
13:27:55.204 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506075000
13:27:55.402 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506075200 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:55.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506075200 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.6 MiB)
13:27:55.403 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:55.404 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506075200 replicated to only 0 peer(s) instead of 1 peers
13:27:55.404 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506075200
13:27:55.605 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506075400 stored as values in memory (estimated size 15.0 B, free 2004.4 MiB)
13:27:55.606 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506075400 in memory on 172.20.10.7:49905 (size: 15.0 B, free: 2004.6 MiB)
13:27:55.606 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:55.606 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506075400 replicated to only 0 peer(s) instead of 1 peers
13:27:55.606 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506075400
13:27:55.804 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506075600 stored as values in memory (estimated size 8.0 B, free 2004.4 MiB)
13:27:55.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506075600 in memory on 172.20.10.7:49905 (size: 8.0 B, free: 2004.6 MiB)
13:27:55.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:55.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506075600 replicated to only 0 peer(s) instead of 1 peers
13:27:55.806 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506075600
13:27:56.002 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506075800 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:27:56.003 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506075800 in memory on 172.20.10.7:49905 (size: 9.0 B, free: 2004.6 MiB)
13:27:56.004 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:56.004 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506075800 replicated to only 0 peer(s) instead of 1 peers
13:27:56.004 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506075800
13:27:56.203 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506076000 stored as values in memory (estimated size 14.0 B, free 2004.4 MiB)
13:27:56.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506076000 in memory on 172.20.10.7:49905 (size: 14.0 B, free: 2004.6 MiB)
13:27:56.204 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:56.205 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506076000 replicated to only 0 peer(s) instead of 1 peers
13:27:56.205 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506076000
13:27:56.402 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506076200 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:27:56.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506076200 in memory on 172.20.10.7:49905 (size: 9.0 B, free: 2004.6 MiB)
13:27:56.404 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:56.404 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506076200 replicated to only 0 peer(s) instead of 1 peers
13:27:56.404 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506076200
13:27:56.603 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506076400 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:27:56.604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506076400 in memory on 172.20.10.7:49905 (size: 9.0 B, free: 2004.6 MiB)
13:27:56.605 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:56.605 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506076400 replicated to only 0 peer(s) instead of 1 peers
13:27:56.606 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506076400
13:27:56.803 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506076600 stored as values in memory (estimated size 14.0 B, free 2004.4 MiB)
13:27:56.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506076600 in memory on 172.20.10.7:49905 (size: 14.0 B, free: 2004.6 MiB)
13:27:56.805 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:56.805 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506076600 replicated to only 0 peer(s) instead of 1 peers
13:27:56.805 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506076600
13:27:57.006 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506076800 stored as values in memory (estimated size 9.0 B, free 2004.4 MiB)
13:27:57.008 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506076800 in memory on 172.20.10.7:49905 (size: 9.0 B, free: 2004.6 MiB)
13:27:57.008 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:27:57.009 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506076800 replicated to only 0 peer(s) instead of 1 peers
13:27:57.009 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506076800
13:27:57.017 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506077000 ms
13:27:57.018 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506077000 ms.0 from job set of time 1760506077000 ms
13:27:57.029 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:27:57.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 31 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 8
13:27:57.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:27:57.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (sortByKey at sparkStreamingSocket.java:75)
13:27:57.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 21)
13:27:57.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 21)
13:27:57.035 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 21 (MapPartitionsRDD[31] at mapToPair at sparkStreamingSocket.java:63), which has no missing parents
13:27:57.039 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:27:57.040 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:27:57.040 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.20.10.7:49905 (size: 3.4 KiB, free: 2004.6 MiB)
13:27:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1513
13:27:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 14 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[31] at mapToPair at sparkStreamingSocket.java:63) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))
13:27:57.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 14 tasks resource profile 0
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 88) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 21.0 (TID 89) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 21.0 (TID 90) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 21.0 (TID 91) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 21.0 (TID 92) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 21.0 (TID 93) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.045 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 21.0 (TID 94) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.046 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 21.0 (TID 95) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.046 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 21.0 (TID 96) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.046 [Executor task launch worker for task 1.0 in stage 21.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 21.0 (TID 89)
13:27:57.046 [Executor task launch worker for task 5.0 in stage 21.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 21.0 (TID 93)
13:27:57.046 [Executor task launch worker for task 7.0 in stage 21.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 21.0 (TID 95)
13:27:57.046 [Executor task launch worker for task 8.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 21.0 (TID 96)
13:27:57.046 [Executor task launch worker for task 6.0 in stage 21.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 21.0 (TID 94)
13:27:57.046 [Executor task launch worker for task 4.0 in stage 21.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 21.0 (TID 92)
13:27:57.046 [Executor task launch worker for task 2.0 in stage 21.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 21.0 (TID 90)
13:27:57.046 [Executor task launch worker for task 3.0 in stage 21.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 21.0 (TID 91)
13:27:57.046 [Executor task launch worker for task 0.0 in stage 21.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 88)
13:27:57.049 [Executor task launch worker for task 5.0 in stage 21.0 (TID 93)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506075000 locally
13:27:57.049 [Executor task launch worker for task 2.0 in stage 21.0 (TID 90)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506074400 locally
13:27:57.049 [Executor task launch worker for task 1.0 in stage 21.0 (TID 89)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506074000 locally
13:27:57.049 [Executor task launch worker for task 4.0 in stage 21.0 (TID 92)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506074800 locally
13:27:57.049 [Executor task launch worker for task 7.0 in stage 21.0 (TID 95)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506075400 locally
13:27:57.049 [Executor task launch worker for task 6.0 in stage 21.0 (TID 94)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506075200 locally
13:27:57.049 [Executor task launch worker for task 0.0 in stage 21.0 (TID 88)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506073800 locally
13:27:57.050 [Executor task launch worker for task 3.0 in stage 21.0 (TID 91)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506074600 locally
13:27:57.049 [Executor task launch worker for task 8.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506075600 locally
13:27:57.052 [Executor task launch worker for task 4.0 in stage 21.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 21.0 (TID 92). 1177 bytes result sent to driver
13:27:57.052 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 21.0 (TID 97) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.053 [Executor task launch worker for task 8.0 in stage 21.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 21.0 (TID 96). 1177 bytes result sent to driver
13:27:57.053 [Executor task launch worker for task 9.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 21.0 (TID 97)
13:27:57.053 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 21.0 (TID 92) in 8 ms on 172.20.10.7 (executor driver) (1/14)
13:27:57.053 [Executor task launch worker for task 0.0 in stage 21.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 88). 1177 bytes result sent to driver
13:27:57.053 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 21.0 (TID 98) (172.20.10.7, executor driver, partition 10, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.054 [Executor task launch worker for task 10.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Running task 10.0 in stage 21.0 (TID 98)
13:27:57.054 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 21.0 (TID 96) in 8 ms on 172.20.10.7 (executor driver) (2/14)
13:27:57.054 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 21.0 (TID 99) (172.20.10.7, executor driver, partition 11, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.054 [Executor task launch worker for task 9.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506075800 locally
13:27:57.054 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 88) in 10 ms on 172.20.10.7 (executor driver) (3/14)
13:27:57.054 [Executor task launch worker for task 11.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Running task 11.0 in stage 21.0 (TID 99)
13:27:57.054 [Executor task launch worker for task 3.0 in stage 21.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 21.0 (TID 91). 1177 bytes result sent to driver
13:27:57.055 [Executor task launch worker for task 10.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506076000 locally
13:27:57.055 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 21.0 (TID 100) (172.20.10.7, executor driver, partition 12, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.055 [Executor task launch worker for task 12.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Running task 12.0 in stage 21.0 (TID 100)
13:27:57.055 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 21.0 (TID 91) in 10 ms on 172.20.10.7 (executor driver) (4/14)
13:27:57.056 [Executor task launch worker for task 5.0 in stage 21.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 21.0 (TID 93). 1177 bytes result sent to driver
13:27:57.056 [Executor task launch worker for task 12.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506076400 locally
13:27:57.056 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 21.0 (TID 101) (172.20.10.7, executor driver, partition 13, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:27:57.056 [Executor task launch worker for task 11.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506076200 locally
13:27:57.056 [Executor task launch worker for task 6.0 in stage 21.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 21.0 (TID 94). 1177 bytes result sent to driver
13:27:57.056 [Executor task launch worker for task 13.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Running task 13.0 in stage 21.0 (TID 101)
13:27:57.056 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 21.0 (TID 93) in 11 ms on 172.20.10.7 (executor driver) (5/14)
13:27:57.057 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 21.0 (TID 94) in 12 ms on 172.20.10.7 (executor driver) (6/14)
13:27:57.058 [Executor task launch worker for task 7.0 in stage 21.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 21.0 (TID 95). 1177 bytes result sent to driver
13:27:57.058 [Executor task launch worker for task 13.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506076600 locally
13:27:57.058 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 21.0 (TID 95) in 12 ms on 172.20.10.7 (executor driver) (7/14)
13:27:57.058 [Executor task launch worker for task 1.0 in stage 21.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 21.0 (TID 89). 1177 bytes result sent to driver
13:27:57.059 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 21.0 (TID 89) in 14 ms on 172.20.10.7 (executor driver) (8/14)
13:27:57.059 [Executor task launch worker for task 2.0 in stage 21.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 21.0 (TID 90). 1177 bytes result sent to driver
13:27:57.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 21.0 (TID 90) in 14 ms on 172.20.10.7 (executor driver) (9/14)
13:27:57.060 [Executor task launch worker for task 11.0 in stage 21.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Finished task 11.0 in stage 21.0 (TID 99). 1177 bytes result sent to driver
13:27:57.060 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 21.0 (TID 99) in 6 ms on 172.20.10.7 (executor driver) (10/14)
13:27:57.060 [Executor task launch worker for task 12.0 in stage 21.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Finished task 12.0 in stage 21.0 (TID 100). 1134 bytes result sent to driver
13:27:57.061 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 21.0 (TID 100) in 6 ms on 172.20.10.7 (executor driver) (11/14)
13:27:57.061 [Executor task launch worker for task 10.0 in stage 21.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Finished task 10.0 in stage 21.0 (TID 98). 1134 bytes result sent to driver
13:27:57.061 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 21.0 (TID 98) in 8 ms on 172.20.10.7 (executor driver) (12/14)
13:27:57.062 [Executor task launch worker for task 9.0 in stage 21.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 21.0 (TID 97). 1134 bytes result sent to driver
13:27:57.062 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 21.0 (TID 97) in 10 ms on 172.20.10.7 (executor driver) (13/14)
13:27:57.063 [Executor task launch worker for task 13.0 in stage 21.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Finished task 13.0 in stage 21.0 (TID 101). 1134 bytes result sent to driver
13:27:57.063 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 21.0 (TID 101) in 7 ms on 172.20.10.7 (executor driver) (14/14)
13:27:57.063 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 21 (mapToPair at sparkStreamingSocket.java:63) finished in 0.026 s
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 22)
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:57.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[34] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:57.066 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:27:57.066 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:27:57.066 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.20.10.7:49905 (size: 3.6 KiB, free: 2004.5 MiB)
13:27:57.066 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1513
13:27:57.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 22 (MapPartitionsRDD[34] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:57.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 10 tasks resource profile 0
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 102) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 103) (172.20.10.7, executor driver, partition 2, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 104) (172.20.10.7, executor driver, partition 3, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 105) (172.20.10.7, executor driver, partition 5, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 106) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.068 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 22.0 (TID 107) (172.20.10.7, executor driver, partition 8, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.069 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 22.0 (TID 108) (172.20.10.7, executor driver, partition 9, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.069 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 109) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.069 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 110) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.069 [Executor task launch worker for task 0.0 in stage 22.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 102)
13:27:57.069 [Executor task launch worker for task 2.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 103)
13:27:57.069 [Executor task launch worker for task 7.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 106)
13:27:57.069 [Executor task launch worker for task 3.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 104)
13:27:57.069 [Executor task launch worker for task 9.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 22.0 (TID 108)
13:27:57.069 [Executor task launch worker for task 5.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 105)
13:27:57.069 [Executor task launch worker for task 8.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 22.0 (TID 107)
13:27:57.070 [Executor task launch worker for task 4.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 110)
13:27:57.070 [Executor task launch worker for task 1.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 109)
13:27:57.072 [Executor task launch worker for task 7.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 5 (658.0 B) non-empty blocks including 5 (658.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 7.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.072 [Executor task launch worker for task 8.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 (529.0 B) non-empty blocks including 4 (529.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 0.0 in stage 22.0 (TID 102)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 8.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.072 [Executor task launch worker for task 0.0 in stage 22.0 (TID 102)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.072 [Executor task launch worker for task 3.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 4.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 4.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.072 [Executor task launch worker for task 2.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.072 [Executor task launch worker for task 2.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:57.072 [Executor task launch worker for task 1.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.073 [Executor task launch worker for task 1.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.072 [Executor task launch worker for task 3.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.073 [Executor task launch worker for task 4.0 in stage 22.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 110). 1517 bytes result sent to driver
13:27:57.073 [Executor task launch worker for task 1.0 in stage 22.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 109). 1517 bytes result sent to driver
13:27:57.073 [Executor task launch worker for task 2.0 in stage 22.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 103). 1523 bytes result sent to driver
13:27:57.072 [Executor task launch worker for task 9.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.073 [Executor task launch worker for task 9.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:57.073 [Executor task launch worker for task 5.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.073 [Executor task launch worker for task 7.0 in stage 22.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 106). 1536 bytes result sent to driver
13:27:57.073 [Executor task launch worker for task 8.0 in stage 22.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 22.0 (TID 107). 1532 bytes result sent to driver
13:27:57.074 [Executor task launch worker for task 5.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.074 [Executor task launch worker for task 3.0 in stage 22.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 104). 1523 bytes result sent to driver
13:27:57.074 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 111) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.074 [Executor task launch worker for task 9.0 in stage 22.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 22.0 (TID 108). 1522 bytes result sent to driver
13:27:57.074 [Executor task launch worker for task 0.0 in stage 22.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 102). 1522 bytes result sent to driver
13:27:57.074 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 110) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:27:57.074 [Executor task launch worker for task 6.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 111)
13:27:57.074 [Executor task launch worker for task 5.0 in stage 22.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 105). 1528 bytes result sent to driver
13:27:57.075 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 109) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:27:57.075 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 103) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:27:57.075 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 106) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:27:57.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 22.0 (TID 107) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:27:57.075 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 104) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:27:57.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 22.0 (TID 108) in 7 ms on 172.20.10.7 (executor driver) (7/10)
13:27:57.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 102) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:27:57.076 [Executor task launch worker for task 6.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.076 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 105) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:27:57.076 [Executor task launch worker for task 6.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.076 [Executor task launch worker for task 6.0 in stage 22.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 111). 1474 bytes result sent to driver
13:27:57.077 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 111) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:27:57.077 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
13:27:57.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 (sortByKey at sparkStreamingSocket.java:75) finished in 0.012 s
13:27:57.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:57.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished
13:27:57.077 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: sortByKey at sparkStreamingSocket.java:75, took 0.047584 s
13:27:57.081 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 32 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 9
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 10 (collect at sparkStreamingSocket.java:75) with 10 output partitions
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (collect at sparkStreamingSocket.java:75)
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 24)
13:27:57.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 24 (ShuffledRDD[32] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:27:57.083 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:27:57.083 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 2004.4 MiB)
13:27:57.083 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.20.10.7:49905 (size: 3.3 KiB, free: 2004.5 MiB)
13:27:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1513
13:27:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 24 (ShuffledRDD[32] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:57.084 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 24.0 with 10 tasks resource profile 0
13:27:57.084 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 24.0 (TID 112) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 24.0 (TID 113) (172.20.10.7, executor driver, partition 2, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 24.0 (TID 114) (172.20.10.7, executor driver, partition 3, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 24.0 (TID 115) (172.20.10.7, executor driver, partition 5, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 24.0 (TID 116) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 24.0 (TID 117) (172.20.10.7, executor driver, partition 8, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 24.0 (TID 118) (172.20.10.7, executor driver, partition 9, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 24.0 (TID 119) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 24.0 (TID 120) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.085 [Executor task launch worker for task 3.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 24.0 (TID 114)
13:27:57.085 [Executor task launch worker for task 5.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 24.0 (TID 115)
13:27:57.085 [Executor task launch worker for task 0.0 in stage 24.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 24.0 (TID 112)
13:27:57.085 [Executor task launch worker for task 7.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 24.0 (TID 116)
13:27:57.085 [Executor task launch worker for task 1.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 24.0 (TID 119)
13:27:57.085 [Executor task launch worker for task 2.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 24.0 (TID 113)
13:27:57.086 [Executor task launch worker for task 4.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 24.0 (TID 120)
13:27:57.085 [Executor task launch worker for task 9.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 24.0 (TID 118)
13:27:57.085 [Executor task launch worker for task 8.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 24.0 (TID 117)
13:27:57.086 [Executor task launch worker for task 4.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.087 [Executor task launch worker for task 4.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.087 [Executor task launch worker for task 9.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.087 [Executor task launch worker for task 9.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.087 [Executor task launch worker for task 1.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.087 [Executor task launch worker for task 7.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 5 (658.0 B) non-empty blocks including 5 (658.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.087 [Executor task launch worker for task 0.0 in stage 24.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.087 [Executor task launch worker for task 1.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.087 [Executor task launch worker for task 7.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.087 [Executor task launch worker for task 0.0 in stage 24.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.088 [Executor task launch worker for task 8.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 (529.0 B) non-empty blocks including 4 (529.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.088 [Executor task launch worker for task 8.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.088 [Executor task launch worker for task 2.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.088 [Executor task launch worker for task 2.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.088 [Executor task launch worker for task 4.0 in stage 24.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 24.0 (TID 120). 1306 bytes result sent to driver
13:27:57.089 [Executor task launch worker for task 5.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.089 [Executor task launch worker for task 5.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.088 [Executor task launch worker for task 3.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.089 [Executor task launch worker for task 3.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:57.090 [Executor task launch worker for task 1.0 in stage 24.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 24.0 (TID 119). 1306 bytes result sent to driver
13:27:57.090 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 24.0 (TID 121) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:27:57.091 [Executor task launch worker for task 6.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 24.0 (TID 121)
13:27:57.091 [Executor task launch worker for task 9.0 in stage 24.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 24.0 (TID 118). 1478 bytes result sent to driver
13:27:57.091 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 24.0 (TID 119) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:27:57.091 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 24.0 (TID 120) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:27:57.092 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 24.0 (TID 118) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:27:57.092 [Executor task launch worker for task 3.0 in stage 24.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 24.0 (TID 114). 1478 bytes result sent to driver
13:27:57.092 [Executor task launch worker for task 8.0 in stage 24.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 24.0 (TID 117). 1478 bytes result sent to driver
13:27:57.092 [Executor task launch worker for task 6.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.092 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 24.0 (TID 114) in 7 ms on 172.20.10.7 (executor driver) (4/10)
13:27:57.092 [Executor task launch worker for task 6.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.093 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 24.0 (TID 117) in 8 ms on 172.20.10.7 (executor driver) (5/10)
13:27:57.093 [Executor task launch worker for task 5.0 in stage 24.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 24.0 (TID 115). 1478 bytes result sent to driver
13:27:57.094 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 24.0 (TID 115) in 9 ms on 172.20.10.7 (executor driver) (6/10)
13:27:57.094 [Executor task launch worker for task 2.0 in stage 24.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 24.0 (TID 113). 1435 bytes result sent to driver
13:27:57.094 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 24.0 (TID 113) in 10 ms on 172.20.10.7 (executor driver) (7/10)
13:27:57.095 [Executor task launch worker for task 7.0 in stage 24.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 24.0 (TID 116). 1478 bytes result sent to driver
13:27:57.095 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 24.0 (TID 116) in 10 ms on 172.20.10.7 (executor driver) (8/10)
13:27:57.095 [Executor task launch worker for task 6.0 in stage 24.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 24.0 (TID 121). 1306 bytes result sent to driver
13:27:57.095 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 24.0 (TID 121) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:27:57.096 [Executor task launch worker for task 0.0 in stage 24.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 24.0 (TID 112). 1435 bytes result sent to driver
13:27:57.096 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 24.0 (TID 112) in 12 ms on 172.20.10.7 (executor driver) (10/10)
13:27:57.096 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 24.0, whose tasks have all completed, from pool 
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 24 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.014 s
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 25)
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:27:57.096 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (ShuffledRDD[35] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:27:57.097 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:27:57.097 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:27:57.097 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.20.10.7:49905 (size: 2.9 KiB, free: 2004.5 MiB)
13:27:57.097 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1513
13:27:57.098 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 25 (ShuffledRDD[35] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:27:57.098 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 10 tasks resource profile 0
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 122) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 123) (172.20.10.7, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 124) (172.20.10.7, executor driver, partition 2, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 125) (172.20.10.7, executor driver, partition 3, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 25.0 (TID 126) (172.20.10.7, executor driver, partition 4, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.099 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 25.0 (TID 127) (172.20.10.7, executor driver, partition 5, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.100 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 25.0 (TID 128) (172.20.10.7, executor driver, partition 6, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.100 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 25.0 (TID 129) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.100 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 25.0 (TID 130) (172.20.10.7, executor driver, partition 8, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.100 [Executor task launch worker for task 0.0 in stage 25.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 122)
13:27:57.100 [Executor task launch worker for task 2.0 in stage 25.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 124)
13:27:57.100 [Executor task launch worker for task 3.0 in stage 25.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 125)
13:27:57.100 [Executor task launch worker for task 4.0 in stage 25.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 25.0 (TID 126)
13:27:57.100 [Executor task launch worker for task 1.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 123)
13:27:57.100 [Executor task launch worker for task 5.0 in stage 25.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 25.0 (TID 127)
13:27:57.100 [Executor task launch worker for task 7.0 in stage 25.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 25.0 (TID 129)
13:27:57.100 [Executor task launch worker for task 8.0 in stage 25.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 25.0 (TID 130)
13:27:57.100 [Executor task launch worker for task 6.0 in stage 25.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 25.0 (TID 128)
13:27:57.101 [Executor task launch worker for task 4.0 in stage 25.0 (TID 126)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.101 [Executor task launch worker for task 4.0 in stage 25.0 (TID 126)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 7.0 in stage 25.0 (TID 129)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 7.0 in stage 25.0 (TID 129)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 5.0 in stage 25.0 (TID 127)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 6.0 in stage 25.0 (TID 128)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 1.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 5.0 in stage 25.0 (TID 127)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 2.0 in stage 25.0 (TID 124)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 1.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 6.0 in stage 25.0 (TID 128)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 8.0 in stage 25.0 (TID 130)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 8.0 in stage 25.0 (TID 130)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 3.0 in stage 25.0 (TID 125)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (258.0 B) non-empty blocks including 2 (258.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.102 [Executor task launch worker for task 3.0 in stage 25.0 (TID 125)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.102 [Executor task launch worker for task 4.0 in stage 25.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 25.0 (TID 126). 1376 bytes result sent to driver
13:27:57.102 [Executor task launch worker for task 2.0 in stage 25.0 (TID 124)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.103 [Executor task launch worker for task 7.0 in stage 25.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 25.0 (TID 129). 1419 bytes result sent to driver
13:27:57.103 [Executor task launch worker for task 8.0 in stage 25.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 25.0 (TID 130). 1417 bytes result sent to driver
13:27:57.103 [Executor task launch worker for task 1.0 in stage 25.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 123). 1419 bytes result sent to driver
13:27:57.102 [Executor task launch worker for task 0.0 in stage 25.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.103 [Executor task launch worker for task 6.0 in stage 25.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 25.0 (TID 128). 1440 bytes result sent to driver
13:27:57.103 [Executor task launch worker for task 5.0 in stage 25.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 25.0 (TID 127). 1418 bytes result sent to driver
13:27:57.103 [Executor task launch worker for task 0.0 in stage 25.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
13:27:57.103 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 25.0 (TID 131) (172.20.10.7, executor driver, partition 9, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:27:57.103 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 25.0 (TID 126) in 4 ms on 172.20.10.7 (executor driver) (1/10)
13:27:57.104 [Executor task launch worker for task 9.0 in stage 25.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 25.0 (TID 131)
13:27:57.104 [Executor task launch worker for task 3.0 in stage 25.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 125). 1439 bytes result sent to driver
13:27:57.104 [Executor task launch worker for task 2.0 in stage 25.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 124). 1418 bytes result sent to driver
13:27:57.104 [Executor task launch worker for task 0.0 in stage 25.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 122). 1438 bytes result sent to driver
13:27:57.104 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 25.0 (TID 129) in 4 ms on 172.20.10.7 (executor driver) (2/10)
13:27:57.104 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 25.0 (TID 130) in 4 ms on 172.20.10.7 (executor driver) (3/10)
13:27:57.104 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 123) in 5 ms on 172.20.10.7 (executor driver) (4/10)
13:27:57.105 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 25.0 (TID 127) in 6 ms on 172.20.10.7 (executor driver) (5/10)
13:27:57.105 [Executor task launch worker for task 9.0 in stage 25.0 (TID 131)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:27:57.105 [Executor task launch worker for task 9.0 in stage 25.0 (TID 131)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:27:57.105 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 25.0 (TID 128) in 6 ms on 172.20.10.7 (executor driver) (6/10)
13:27:57.105 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 125) in 6 ms on 172.20.10.7 (executor driver) (7/10)
13:27:57.105 [Executor task launch worker for task 9.0 in stage 25.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 25.0 (TID 131). 1332 bytes result sent to driver
13:27:57.105 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 124) in 6 ms on 172.20.10.7 (executor driver) (8/10)
13:27:57.106 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 122) in 8 ms on 172.20.10.7 (executor driver) (9/10)
13:27:57.106 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 25.0 (TID 131) in 3 ms on 172.20.10.7 (executor driver) (10/10)
13:27:57.106 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
13:27:57.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (collect at sparkStreamingSocket.java:75) finished in 0.009 s
13:27:57.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
13:27:57.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished
13:27:57.106 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 finished: collect at sparkStreamingSocket.java:75, took 0.024894 s
13:27:57.106 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506077000 ms.0 from job set of time 1760506077000 ms
13:27:57.106 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.106 s for time 1760506077000 ms (execution: 0.088 s)
13:27:57.107 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 25 from persistence list
13:27:57.107 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 24 from persistence list
13:27:57.107 [block-manager-storage-async-thread-pool-81] INFO  org.apache.spark.storage.BlockManager - Removing RDD 25
13:27:57.107 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 23 from persistence list
13:27:57.107 [block-manager-storage-async-thread-pool-83] INFO  org.apache.spark.storage.BlockManager - Removing RDD 24
13:27:57.108 [block-manager-storage-async-thread-pool-87] INFO  org.apache.spark.storage.BlockManager - Removing RDD 23
13:27:57.108 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 22 from persistence list
13:27:57.108 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[22] at socketTextStream at sparkStreamingSocket.java:31 of time 1760506077000 ms
13:27:57.108 [block-manager-storage-async-thread-pool-90] INFO  org.apache.spark.storage.BlockManager - Removing RDD 22
13:27:57.109 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 1760506071000 ms
13:27:57.110 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 1760506071000 ms
13:27:57.110 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760506073200 on 172.20.10.7:49905 in memory (size: 8.0 B, free: 2004.5 MiB)
13:27:57.110 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760506073400 on 172.20.10.7:49905 in memory (size: 8.0 B, free: 2004.5 MiB)
13:27:57.110 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed input-0-1760506073600 on 172.20.10.7:49905 in memory (size: 8.0 B, free: 2004.5 MiB)
13:27:59.117 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:27:59.119 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:27:59.119 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:27:59.119 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:27:59.120 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Closed socket to 127.0.0.1:9999
13:27:59.120 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:27:59.120 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:27:59.121 [dispatcher-event-loop-3] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
13:27:59.120 [Socket Receiver] WARN  org.apache.spark.streaming.dstream.SocketReceiver - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:27:59.123 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:27:59.123 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:27:59.123 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
13:27:59.123 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
13:27:59.124 [dispatcher-event-loop-0] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:28:24.606 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
13:28:24.628 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13:28:24.657 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:28:24.657 [main] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
13:28:24.657 [main] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
13:28:24.657 [main] INFO  org.apache.spark.SparkContext - Submitted application: SparkStreaming
13:28:24.665 [main] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
13:28:24.670 [main] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
13:28:24.670 [main] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
13:28:24.687 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: sai
13:28:24.687 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: sai
13:28:24.687 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
13:28:24.688 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
13:28:24.688 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sai); groups with view permissions: Set(); users  with modify permissions: Set(sai); groups with modify permissions: Set()
13:28:24.777 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49917.
13:28:24.785 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
13:28:24.796 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
13:28:24.802 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
13:28:24.802 [main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
13:28:24.803 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
13:28:24.813 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/l8/0_hmw7gn2tb1m64_td5vl9p80000gn/T/blockmgr-726c785e-9e69-4b1f-918a-a51537b54332
13:28:24.835 [main] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2004.6 MiB
13:28:24.842 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
13:28:24.857 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5880ms to org.sparkproject.jetty.util.log.Slf4jLog
13:28:24.902 [main] INFO  org.sparkproject.jetty.server.Server - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_452-b09
13:28:24.909 [main] INFO  org.sparkproject.jetty.server.Server - Started @5932ms
13:28:24.923 [main] INFO  org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@7c974942{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
13:28:24.923 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
13:28:24.930 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d4664d7{/,null,AVAILABLE,@Spark}
13:28:24.955 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 172.20.10.7
13:28:24.958 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
13:28:24.964 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49918.
13:28:24.964 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 172.20.10.7:49918
13:28:24.964 [main] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
13:28:24.966 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.10.7, 49918, None)
13:28:24.967 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 172.20.10.7:49918 with 2004.6 MiB RAM, BlockManagerId(driver, 172.20.10.7, 49918, None)
13:28:24.968 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.10.7, 49918, None)
13:28:24.968 [main] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.10.7, 49918, None)
13:28:25.039 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1d4664d7{/,null,STOPPED,@Spark}
13:28:25.040 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f93e4a8{/jobs,null,AVAILABLE,@Spark}
13:28:25.040 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5445f5ba{/jobs/json,null,AVAILABLE,@Spark}
13:28:25.040 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c808207{/jobs/job,null,AVAILABLE,@Spark}
13:28:25.041 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a0cbc6f{/jobs/job/json,null,AVAILABLE,@Spark}
13:28:25.041 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89292e{/stages,null,AVAILABLE,@Spark}
13:28:25.041 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@de77232{/stages/json,null,AVAILABLE,@Spark}
13:28:25.042 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ab550d5{/stages/stage,null,AVAILABLE,@Spark}
13:28:25.043 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e85c6f{/stages/stage/json,null,AVAILABLE,@Spark}
13:28:25.043 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac0b715{/stages/pool,null,AVAILABLE,@Spark}
13:28:25.044 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9ac4cc{/stages/pool/json,null,AVAILABLE,@Spark}
13:28:25.044 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264e43c{/storage,null,AVAILABLE,@Spark}
13:28:25.044 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31da3d60{/storage/json,null,AVAILABLE,@Spark}
13:28:25.045 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65ec8b24{/storage/rdd,null,AVAILABLE,@Spark}
13:28:25.045 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f18f9d2{/storage/rdd/json,null,AVAILABLE,@Spark}
13:28:25.045 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b67519{/environment,null,AVAILABLE,@Spark}
13:28:25.046 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30d25c03{/environment/json,null,AVAILABLE,@Spark}
13:28:25.046 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@116a2108{/executors,null,AVAILABLE,@Spark}
13:28:25.047 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c1c5936{/executors/json,null,AVAILABLE,@Spark}
13:28:25.048 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8cf9b{/executors/threadDump,null,AVAILABLE,@Spark}
13:28:25.048 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b15f922{/executors/threadDump/json,null,AVAILABLE,@Spark}
13:28:25.052 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b869331{/static,null,AVAILABLE,@Spark}
13:28:25.052 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64412d34{/,null,AVAILABLE,@Spark}
13:28:25.053 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38c2c309{/api,null,AVAILABLE,@Spark}
13:28:25.053 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f92b990{/jobs/job/kill,null,AVAILABLE,@Spark}
13:28:25.054 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c1832aa{/stages/stage/kill,null,AVAILABLE,@Spark}
13:28:25.056 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7318daf8{/metrics/json,null,AVAILABLE,@Spark}
13:28:25.214 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Starting 1 receivers
13:28:25.215 [streaming-start] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker started
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Slide time = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Storage level = Serialized 1x Replicated
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Checkpoint interval = null
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Remember interval = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@41c03a4
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Slide time = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Storage level = Serialized 1x Replicated
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Checkpoint interval = null
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Remember interval = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.FlatMappedDStream - Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@42ee82
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Slide time = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@12410fbc
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 3000 ms
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
13:28:25.216 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 3000 ms
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@4e1eb037
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 3000 ms
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 3000 ms
13:28:25.217 [streaming-start] INFO  org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2afb9b73
13:28:25.237 [streaming-start] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1760506107000
13:28:25.237 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1760506107000 ms
13:28:25.238 [streaming-start] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
13:28:25.239 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0895f5{/streaming,null,AVAILABLE,@Spark}
13:28:25.239 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd9ebde{/streaming/json,null,AVAILABLE,@Spark}
13:28:25.239 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6726cc69{/streaming/batch,null,AVAILABLE,@Spark}
13:28:25.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33899f7a{/streaming/batch/json,null,AVAILABLE,@Spark}
13:28:25.240 [main] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@644ded04{/static/streaming,null,AVAILABLE,@Spark}
13:28:25.241 [main] INFO  org.apache.spark.streaming.StreamingContext - StreamingContext started
13:28:25.255 [dispatcher-event-loop-1] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Receiver 0 started
13:28:25.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (start at sparkStreamingSocket.java:39) with 1 output partitions
13:28:25.258 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (start at sparkStreamingSocket.java:39)
13:28:25.258 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
13:28:25.258 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:28:25.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
13:28:25.317 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 100.0 KiB, free 2004.5 MiB)
13:28:25.495 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 2004.5 MiB)
13:28:25.497 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.10.7:49918 (size: 35.5 KiB, free: 2004.6 MiB)
13:28:25.499 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
13:28:25.507 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
13:28:25.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
13:28:25.532 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 5450 bytes) taskResourceAssignments Map()
13:28:25.538 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
13:28:25.652 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.util.RecurringTimer - Started timer for BlockGenerator at time 1760506105800
13:28:25.652 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started BlockGenerator
13:28:25.652 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Started block pushing thread
13:28:25.654 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Registered receiver for stream 0 from 172.20.10.7:49917
13:28:25.654 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Starting receiver 0
13:28:25.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connecting to 127.0.0.1:9999
13:28:25.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Connected to 127.0.0.1:9999
13:28:25.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver 0 onStart
13:28:25.655 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Waiting for receiver to be stopped
13:28:27.051 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506107000 ms
13:28:27.053 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506107000 ms.0 from job set of time 1760506107000 ms
13:28:27.076 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:28:27.083 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 0
13:28:27.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:28:27.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (sortByKey at sparkStreamingSocket.java:75)
13:28:27.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
13:28:27.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
13:28:27.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:28:27.091 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 7.0 KiB, free 2004.5 MiB)
13:28:27.092 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.5 MiB)
13:28:27.092 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.10.7:49918 (size: 3.6 KiB, free: 2004.6 MiB)
13:28:27.094 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
13:28:27.094 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:28:27.094 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 10 tasks resource profile 0
13:28:27.095 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 1) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 2) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 3) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 4) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 5) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 6) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 7) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.096 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 8) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.097 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 9) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.097 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 1)
13:28:27.097 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 2)
13:28:27.097 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 3)
13:28:27.097 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 4)
13:28:27.097 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 5)
13:28:27.098 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 7)
13:28:27.098 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 6)
13:28:27.099 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 8)
13:28:27.099 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 9)
13:28:27.197 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.197 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.198 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.199 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
13:28:27.213 [Executor task launch worker for task 3.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 4). 1560 bytes result sent to driver
13:28:27.214 [Executor task launch worker for task 1.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 2). 1560 bytes result sent to driver
13:28:27.214 [Executor task launch worker for task 7.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 8). 1560 bytes result sent to driver
13:28:27.215 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 10) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.216 [Executor task launch worker for task 2.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 3). 1560 bytes result sent to driver
13:28:27.216 [Executor task launch worker for task 8.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 9). 1560 bytes result sent to driver
13:28:27.216 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 10)
13:28:27.216 [Executor task launch worker for task 4.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 5). 1560 bytes result sent to driver
13:28:27.216 [Executor task launch worker for task 5.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 6). 1560 bytes result sent to driver
13:28:27.216 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 1). 1560 bytes result sent to driver
13:28:27.216 [Executor task launch worker for task 6.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 7). 1560 bytes result sent to driver
13:28:27.218 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 2) in 122 ms on 172.20.10.7 (executor driver) (1/10)
13:28:27.218 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 3) in 122 ms on 172.20.10.7 (executor driver) (2/10)
13:28:27.218 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 4) in 122 ms on 172.20.10.7 (executor driver) (3/10)
13:28:27.219 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.219 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 8) in 123 ms on 172.20.10.7 (executor driver) (4/10)
13:28:27.219 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.219 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 5) in 123 ms on 172.20.10.7 (executor driver) (5/10)
13:28:27.219 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 9) in 122 ms on 172.20.10.7 (executor driver) (6/10)
13:28:27.219 [Executor task launch worker for task 9.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 10). 1517 bytes result sent to driver
13:28:27.220 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 6) in 124 ms on 172.20.10.7 (executor driver) (7/10)
13:28:27.220 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 1) in 125 ms on 172.20.10.7 (executor driver) (8/10)
13:28:27.220 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 7) in 124 ms on 172.20.10.7 (executor driver) (9/10)
13:28:27.221 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 10) in 6 ms on 172.20.10.7 (executor driver) (10/10)
13:28:27.221 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13:28:27.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (sortByKey at sparkStreamingSocket.java:75) finished in 0.134 s
13:28:27.224 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
13:28:27.224 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
13:28:27.225 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: sortByKey at sparkStreamingSocket.java:75, took 0.149156 s
13:28:27.230 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:28:27.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 1
13:28:27.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:28:27.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at sparkStreamingSocket.java:75)
13:28:27.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
13:28:27.231 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
13:28:27.232 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:28:27.234 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.5 KiB, free 2004.5 MiB)
13:28:27.234 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:28:27.235 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.10.7:49918 (size: 3.2 KiB, free: 2004.6 MiB)
13:28:27.235 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
13:28:27.236 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 4 (ShuffledRDD[4] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:28:27.236 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 10 tasks resource profile 0
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 11) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 12) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 13) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 14) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 15) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.237 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 16) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.238 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 4.0 (TID 17) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.238 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 4.0 (TID 18) (172.20.10.7, executor driver, partition 7, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.238 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 4.0 (TID 19) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.238 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 12)
13:28:27.238 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 13)
13:28:27.238 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 4.0 (TID 15)
13:28:27.238 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 4.0 (TID 18)
13:28:27.238 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 11)
13:28:27.238 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 4.0 (TID 17)
13:28:27.238 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 14)
13:28:27.238 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 4.0 (TID 19)
13:28:27.238 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 4.0 (TID 16)
13:28:27.244 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.244 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.244 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.250 [Executor task launch worker for task 7.0 in stage 4.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 4.0 (TID 18). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 3.0 in stage 4.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 14). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 5.0 in stage 4.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 4.0 (TID 16). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 2.0 in stage 4.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 13). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 8.0 in stage 4.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 4.0 (TID 19). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 4.0 in stage 4.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 4.0 (TID 15). 1340 bytes result sent to driver
13:28:27.250 [Executor task launch worker for task 1.0 in stage 4.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 12). 1340 bytes result sent to driver
13:28:27.251 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 4.0 (TID 20) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:27.251 [Executor task launch worker for task 6.0 in stage 4.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 4.0 (TID 17). 1340 bytes result sent to driver
13:28:27.251 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 4.0 (TID 20)
13:28:27.251 [Executor task launch worker for task 0.0 in stage 4.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 11). 1340 bytes result sent to driver
13:28:27.251 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 4.0 (TID 18) in 13 ms on 172.20.10.7 (executor driver) (1/10)
13:28:27.251 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 14) in 14 ms on 172.20.10.7 (executor driver) (2/10)
13:28:27.252 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 13) in 15 ms on 172.20.10.7 (executor driver) (3/10)
13:28:27.252 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 16) in 15 ms on 172.20.10.7 (executor driver) (4/10)
13:28:27.252 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 4.0 (TID 19) in 14 ms on 172.20.10.7 (executor driver) (5/10)
13:28:27.252 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.252 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.253 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 15) in 16 ms on 172.20.10.7 (executor driver) (6/10)
13:28:27.253 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 12) in 16 ms on 172.20.10.7 (executor driver) (7/10)
13:28:27.253 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 4.0 (TID 17) in 16 ms on 172.20.10.7 (executor driver) (8/10)
13:28:27.254 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 11) in 18 ms on 172.20.10.7 (executor driver) (9/10)
13:28:27.254 [Executor task launch worker for task 9.0 in stage 4.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 4.0 (TID 20). 1340 bytes result sent to driver
13:28:27.254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 4.0 (TID 20) in 4 ms on 172.20.10.7 (executor driver) (10/10)
13:28:27.255 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
13:28:27.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.022 s
13:28:27.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:28:27.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:28:27.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
13:28:27.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:28:27.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:28:27.258 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:28:27.258 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:28:27.259 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.20.10.7:49918 (size: 2.9 KiB, free: 2004.6 MiB)
13:28:27.259 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
13:28:27.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[7] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:28:27.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
13:28:27.260 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 21) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:27.260 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 21)
13:28:27.263 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:27.263 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:27.269 [Executor task launch worker for task 0.0 in stage 5.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 21). 1228 bytes result sent to driver
13:28:27.270 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 21) in 10 ms on 172.20.10.7 (executor driver) (1/1)
13:28:27.270 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
13:28:27.270 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at sparkStreamingSocket.java:75) finished in 0.013 s
13:28:27.271 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
13:28:27.271 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
13:28:27.271 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at sparkStreamingSocket.java:75, took 0.040766 s
13:28:27.272 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506107000 ms.0 from job set of time 1760506107000 ms
13:28:27.273 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.272 s for time 1760506107000 ms (execution: 0.220 s)
13:28:27.274 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:28:27.276 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:28:28.211 [Thread-15] INFO  org.apache.spark.storage.memory.MemoryStore - Block input-0-1760506108000 stored as values in memory (estimated size 36.0 B, free 2004.4 MiB)
13:28:28.212 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added input-0-1760506108000 in memory on 172.20.10.7:49918 (size: 36.0 B, free: 2004.6 MiB)
13:28:28.222 [Thread-15] WARN  org.apache.spark.storage.RandomBlockReplicationPolicy - Expecting 1 replicas with only 0 peer/s.
13:28:28.223 [Thread-15] WARN  org.apache.spark.storage.BlockManager - Block input-0-1760506108000 replicated to only 0 peer(s) instead of 1 peers
13:28:28.227 [Thread-15] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1760506108000
13:28:30.018 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1760506110000 ms
13:28:30.019 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1760506110000 ms.0 from job set of time 1760506110000 ms
13:28:30.028 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: sortByKey at sparkStreamingSocket.java:75
13:28:30.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (mapToPair at sparkStreamingSocket.java:63) as input to shuffle 2
13:28:30.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (sortByKey at sparkStreamingSocket.java:75) with 10 output partitions
13:28:30.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (sortByKey at sparkStreamingSocket.java:75)
13:28:30.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
13:28:30.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
13:28:30.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at sparkStreamingSocket.java:63), which has no missing parents
13:28:30.040 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 6.2 KiB, free 2004.4 MiB)
13:28:30.041 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 2004.4 MiB)
13:28:30.042 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.20.10.7:49918 (size: 3.4 KiB, free: 2004.6 MiB)
13:28:30.042 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
13:28:30.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[10] at mapToPair at sparkStreamingSocket.java:63) (first 15 tasks are for partitions Vector(0))
13:28:30.043 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0
13:28:30.044 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 22) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4383 bytes) taskResourceAssignments Map()
13:28:30.045 [Executor task launch worker for task 0.0 in stage 6.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 22)
13:28:30.050 [Executor task launch worker for task 0.0 in stage 6.0 (TID 22)] INFO  org.apache.spark.storage.BlockManager - Found block input-0-1760506108000 locally
13:28:30.059 [Executor task launch worker for task 0.0 in stage 6.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 22). 1177 bytes result sent to driver
13:28:30.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 22) in 16 ms on 172.20.10.7 (executor driver) (1/1)
13:28:30.060 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (mapToPair at sparkStreamingSocket.java:63) finished in 0.024 s
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:28:30.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:28:30.064 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 7.0 KiB, free 2004.4 MiB)
13:28:30.065 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 2004.4 MiB)
13:28:30.065 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.20.10.7:49918 (size: 3.6 KiB, free: 2004.5 MiB)
13:28:30.066 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
13:28:30.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[13] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:28:30.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 10 tasks resource profile 0
13:28:30.067 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 23) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.067 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 24) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.067 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 25) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.067 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 26) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 27) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 28) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 29) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 30) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 7.0 (TID 31) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.068 [Executor task launch worker for task 7.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 7.0 (TID 23)
13:28:30.068 [Executor task launch worker for task 2.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 26)
13:28:30.068 [Executor task launch worker for task 0.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 24)
13:28:30.068 [Executor task launch worker for task 3.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 27)
13:28:30.068 [Executor task launch worker for task 4.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 7.0 (TID 28)
13:28:30.068 [Executor task launch worker for task 8.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 7.0 (TID 31)
13:28:30.068 [Executor task launch worker for task 6.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 7.0 (TID 30)
13:28:30.068 [Executor task launch worker for task 5.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 7.0 (TID 29)
13:28:30.068 [Executor task launch worker for task 1.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 25)
13:28:30.071 [Executor task launch worker for task 4.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.071 [Executor task launch worker for task 4.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.071 [Executor task launch worker for task 5.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.071 [Executor task launch worker for task 6.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.071 [Executor task launch worker for task 1.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.071 [Executor task launch worker for task 1.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.071 [Executor task launch worker for task 6.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.071 [Executor task launch worker for task 5.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.071 [Executor task launch worker for task 8.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.071 [Executor task launch worker for task 0.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.072 [Executor task launch worker for task 0.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.072 [Executor task launch worker for task 8.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.072 [Executor task launch worker for task 4.0 in stage 7.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 7.0 (TID 28). 1517 bytes result sent to driver
13:28:30.072 [Executor task launch worker for task 6.0 in stage 7.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 7.0 (TID 30). 1517 bytes result sent to driver
13:28:30.072 [Executor task launch worker for task 2.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.072 [Executor task launch worker for task 2.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.072 [Executor task launch worker for task 1.0 in stage 7.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 25). 1517 bytes result sent to driver
13:28:30.073 [Executor task launch worker for task 0.0 in stage 7.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 24). 1517 bytes result sent to driver
13:28:30.072 [Executor task launch worker for task 3.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.073 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 7.0 (TID 32) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.073 [Executor task launch worker for task 3.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.073 [Executor task launch worker for task 5.0 in stage 7.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 7.0 (TID 29). 1517 bytes result sent to driver
13:28:30.073 [Executor task launch worker for task 9.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 7.0 (TID 32)
13:28:30.073 [Executor task launch worker for task 8.0 in stage 7.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 7.0 (TID 31). 1517 bytes result sent to driver
13:28:30.073 [Executor task launch worker for task 3.0 in stage 7.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 27). 1517 bytes result sent to driver
13:28:30.073 [Executor task launch worker for task 2.0 in stage 7.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 26). 1517 bytes result sent to driver
13:28:30.074 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 28) in 6 ms on 172.20.10.7 (executor driver) (1/10)
13:28:30.074 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 7.0 (TID 30) in 6 ms on 172.20.10.7 (executor driver) (2/10)
13:28:30.074 [Executor task launch worker for task 7.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.074 [Executor task launch worker for task 7.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
13:28:30.074 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 25) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:28:30.075 [Executor task launch worker for task 9.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.075 [Executor task launch worker for task 9.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.075 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 24) in 8 ms on 172.20.10.7 (executor driver) (4/10)
13:28:30.075 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 29) in 7 ms on 172.20.10.7 (executor driver) (5/10)
13:28:30.075 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 7.0 (TID 31) in 7 ms on 172.20.10.7 (executor driver) (6/10)
13:28:30.075 [Executor task launch worker for task 9.0 in stage 7.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 7.0 (TID 32). 1474 bytes result sent to driver
13:28:30.076 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 26) in 9 ms on 172.20.10.7 (executor driver) (7/10)
13:28:30.076 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 27) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:28:30.076 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 7.0 (TID 32) in 3 ms on 172.20.10.7 (executor driver) (9/10)
13:28:30.081 [Executor task launch worker for task 7.0 in stage 7.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 7.0 (TID 23). 1521 bytes result sent to driver
13:28:30.082 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 7.0 (TID 23) in 14 ms on 172.20.10.7 (executor driver) (10/10)
13:28:30.082 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
13:28:30.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (sortByKey at sparkStreamingSocket.java:75) finished in 0.019 s
13:28:30.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
13:28:30.082 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
13:28:30.082 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: sortByKey at sparkStreamingSocket.java:75, took 0.053839 s
13:28:30.088 [streaming-job-executor-0] INFO  org.apache.spark.SparkContext - Starting job: collect at sparkStreamingSocket.java:75
13:28:30.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (reduceByKey at sparkStreamingSocket.java:71) as input to shuffle 3
13:28:30.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at sparkStreamingSocket.java:75) with 1 output partitions
13:28:30.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (collect at sparkStreamingSocket.java:75)
13:28:30.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
13:28:30.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
13:28:30.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:71), which has no missing parents
13:28:30.090 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 5.5 KiB, free 2004.4 MiB)
13:28:30.091 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2004.4 MiB)
13:28:30.091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.20.10.7:49918 (size: 3.2 KiB, free: 2004.5 MiB)
13:28:30.091 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
13:28:30.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 9 (ShuffledRDD[11] at reduceByKey at sparkStreamingSocket.java:71) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
13:28:30.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 10 tasks resource profile 0
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 9.0 (TID 33) (172.20.10.7, executor driver, partition 7, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 34) (172.20.10.7, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 35) (172.20.10.7, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 36) (172.20.10.7, executor driver, partition 2, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 37) (172.20.10.7, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.093 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 9.0 (TID 38) (172.20.10.7, executor driver, partition 4, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.094 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 9.0 (TID 39) (172.20.10.7, executor driver, partition 5, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.094 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 9.0 (TID 40) (172.20.10.7, executor driver, partition 6, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.094 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 9.0 (TID 41) (172.20.10.7, executor driver, partition 8, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.094 [Executor task launch worker for task 0.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 34)
13:28:30.094 [Executor task launch worker for task 1.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 35)
13:28:30.094 [Executor task launch worker for task 2.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 36)
13:28:30.094 [Executor task launch worker for task 3.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 37)
13:28:30.094 [Executor task launch worker for task 7.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 9.0 (TID 33)
13:28:30.094 [Executor task launch worker for task 5.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 9.0 (TID 39)
13:28:30.094 [Executor task launch worker for task 4.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 9.0 (TID 38)
13:28:30.095 [Executor task launch worker for task 6.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 9.0 (TID 40)
13:28:30.095 [Executor task launch worker for task 8.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 9.0 (TID 41)
13:28:30.096 [Executor task launch worker for task 2.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.096 [Executor task launch worker for task 2.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.097 [Executor task launch worker for task 4.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 0.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 5.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 1.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 5.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.097 [Executor task launch worker for task 0.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.097 [Executor task launch worker for task 8.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 8.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.097 [Executor task launch worker for task 2.0 in stage 9.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 36). 1340 bytes result sent to driver
13:28:30.097 [Executor task launch worker for task 3.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 7.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.097 [Executor task launch worker for task 4.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.098 [Executor task launch worker for task 3.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.097 [Executor task launch worker for task 1.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.098 [Executor task launch worker for task 7.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.098 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 9.0 (TID 42) (172.20.10.7, executor driver, partition 9, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()
13:28:30.098 [Executor task launch worker for task 6.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.098 [Executor task launch worker for task 6.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.098 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 36) in 5 ms on 172.20.10.7 (executor driver) (1/10)
13:28:30.098 [Executor task launch worker for task 9.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 9.0 (TID 42)
13:28:30.099 [Executor task launch worker for task 5.0 in stage 9.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 9.0 (TID 39). 1340 bytes result sent to driver
13:28:30.099 [Executor task launch worker for task 1.0 in stage 9.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 35). 1340 bytes result sent to driver
13:28:30.099 [Executor task launch worker for task 8.0 in stage 9.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 9.0 (TID 41). 1340 bytes result sent to driver
13:28:30.099 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 9.0 (TID 39) in 5 ms on 172.20.10.7 (executor driver) (2/10)
13:28:30.100 [Executor task launch worker for task 3.0 in stage 9.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 37). 1340 bytes result sent to driver
13:28:30.100 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 35) in 7 ms on 172.20.10.7 (executor driver) (3/10)
13:28:30.100 [Executor task launch worker for task 0.0 in stage 9.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 34). 1340 bytes result sent to driver
13:28:30.100 [Executor task launch worker for task 9.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.100 [Executor task launch worker for task 9.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.100 [Executor task launch worker for task 4.0 in stage 9.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 9.0 (TID 38). 1340 bytes result sent to driver
13:28:30.100 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 9.0 (TID 41) in 6 ms on 172.20.10.7 (executor driver) (4/10)
13:28:30.101 [Executor task launch worker for task 6.0 in stage 9.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 9.0 (TID 40). 1340 bytes result sent to driver
13:28:30.102 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 34) in 9 ms on 172.20.10.7 (executor driver) (5/10)
13:28:30.102 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 37) in 9 ms on 172.20.10.7 (executor driver) (6/10)
13:28:30.102 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 9.0 (TID 38) in 9 ms on 172.20.10.7 (executor driver) (7/10)
13:28:30.102 [Executor task launch worker for task 9.0 in stage 9.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 9.0 (TID 42). 1340 bytes result sent to driver
13:28:30.102 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 9.0 (TID 40) in 8 ms on 172.20.10.7 (executor driver) (8/10)
13:28:30.103 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 9.0 (TID 42) in 5 ms on 172.20.10.7 (executor driver) (9/10)
13:28:30.106 [Executor task launch worker for task 7.0 in stage 9.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 9.0 (TID 33). 1469 bytes result sent to driver
13:28:30.106 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 9.0 (TID 33) in 13 ms on 172.20.10.7 (executor driver) (10/10)
13:28:30.106 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
13:28:30.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (reduceByKey at sparkStreamingSocket.java:71) finished in 0.016 s
13:28:30.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
13:28:30.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ResultStage 0)
13:28:30.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
13:28:30.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
13:28:30.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:75), which has no missing parents
13:28:30.108 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 5.0 KiB, free 2004.4 MiB)
13:28:30.109 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2004.4 MiB)
13:28:30.109 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.20.10.7:49918 (size: 2.9 KiB, free: 2004.5 MiB)
13:28:30.110 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
13:28:30.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[14] at sortByKey at sparkStreamingSocket.java:75) (first 15 tasks are for partitions Vector(0))
13:28:30.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0
13:28:30.110 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 43) (172.20.10.7, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
13:28:30.111 [Executor task launch worker for task 0.0 in stage 10.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 43)
13:28:30.112 [Executor task launch worker for task 0.0 in stage 10.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
13:28:30.112 [Executor task launch worker for task 0.0 in stage 10.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
13:28:30.112 [Executor task launch worker for task 0.0 in stage 10.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 43). 1374 bytes result sent to driver
13:28:30.113 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 43) in 3 ms on 172.20.10.7 (executor driver) (1/1)
13:28:30.113 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
13:28:30.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (collect at sparkStreamingSocket.java:75) finished in 0.006 s
13:28:30.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
13:28:30.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
13:28:30.114 [streaming-job-executor-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: collect at sparkStreamingSocket.java:75, took 0.025966 s
13:28:30.114 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1760506110000 ms.0 from job set of time 1760506110000 ms
13:28:30.114 [JobScheduler] INFO  org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.114 s for time 1760506110000 ms (execution: 0.095 s)
13:28:30.115 [JobGenerator] INFO  org.apache.spark.rdd.ShuffledRDD - Removing RDD 4 from persistence list
13:28:30.117 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 3 from persistence list
13:28:30.118 [JobGenerator] INFO  org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 2 from persistence list
13:28:30.118 [block-manager-storage-async-thread-pool-1] INFO  org.apache.spark.storage.BlockManager - Removing RDD 3
13:28:30.118 [block-manager-storage-async-thread-pool-0] INFO  org.apache.spark.storage.BlockManager - Removing RDD 4
13:28:30.118 [block-manager-storage-async-thread-pool-2] INFO  org.apache.spark.storage.BlockManager - Removing RDD 2
13:28:30.118 [JobGenerator] INFO  org.apache.spark.rdd.BlockRDD - Removing RDD 1 from persistence list
13:28:30.119 [block-manager-storage-async-thread-pool-3] INFO  org.apache.spark.storage.BlockManager - Removing RDD 1
13:28:30.119 [JobGenerator] INFO  org.apache.spark.streaming.dstream.SocketInputDStream - Removing blocks of RDD BlockRDD[1] at socketTextStream at sparkStreamingSocket.java:31 of time 1760506110000 ms
13:28:30.119 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
13:28:30.119 [JobGenerator] INFO  org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
13:28:31.429 [shutdown-hook-0] INFO  org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
13:28:31.431 [dispatcher-event-loop-2] INFO  org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
13:28:31.431 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
13:28:31.431 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
13:28:31.432 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.dstream.SocketReceiver - Closed socket to 127.0.0.1:9999
13:28:31.432 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
13:28:31.432 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
13:28:31.432 [dispatcher-event-loop-0] ERROR org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
13:28:31.433 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
13:28:31.433 [dispatcher-event-loop-6] INFO  org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
13:28:31.432 [Socket Receiver] WARN  org.apache.spark.streaming.dstream.SocketReceiver - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:28:31.435 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_452]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_452]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) ~[?:1.8.0_452]
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) ~[?:1.8.0_452]
	at java.io.InputStreamReader.read(InputStreamReader.java:184) ~[?:1.8.0_452]
	at java.io.BufferedReader.fill(BufferedReader.java:161) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:324) ~[?:1.8.0_452]
	at java.io.BufferedReader.readLine(BufferedReader.java:389) ~[?:1.8.0_452]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) ~[spark-core_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72) ~[spark-streaming_2.12-3.3.1.jar:3.3.1]
13:28:31.435 [receiver-supervisor-future-0] INFO  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
13:28:31.435 [receiver-supervisor-future-0] WARN  org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Receiver has been stopped
